diff --git a/aten/CMakeLists.txt b/aten/CMakeLists.txt
index 9648d1907..2d9a732cd 100644
--- a/aten/CMakeLists.txt
+++ b/aten/CMakeLists.txt
@@ -1,15 +1,58 @@
 cmake_minimum_required(VERSION 3.0)
+# HIP_PATH
+IF(NOT DEFINED $ENV{HIP_PATH})
+  SET(HIP_PATH /opt/rocm/hip)
+ELSE()
+  SET(HIP_PATH $ENV{HIP_PATH})
+ENDIF()
+
+# HCC_PATH
+IF(NOT DEFINED $ENV{HCC_PATH})
+  SET(HCC_PATH /opt/rocm/hcc)
+ELSE()
+  SET(HCC_PATH $ENV{HCC_PATH})
+ENDIF()
+
+# HIPBLAS_PATH
+IF(NOT DEFINED $ENV{HIPBLAS_PATH})
+  SET(HIPBLAS_PATH /opt/rocm/hipblas)
+ELSE()
+  SET(HIPBLAS_PATH $ENV{HIPBLAS_PATH})
+ENDIF()
+
+# HIPRNG_PATH
+IF(NOT DEFINED $ENV{HIPRNG_PATH})
+  SET(HIPRNG_PATH /opt/rocm/hcrng)
+ELSE()
+  SET(HIPRNG_PATH $ENV{HIPRNG_PATH})
+ENDIF()
+
+# HIPSPARSE_PATH
+IF(NOT DEFINED $ENV{HIPSPARSE_PATH})
+  SET(HIPSPARSE_PATH /opt/rocm/hcsparse)
+ELSE()
+  SET(HIPSPARSE_PATH $ENV{HIPSPARSE_PATH})
+ENDIF()
+
+SET(THRUST_PATH "/root/Thrust")
+
+cmake_minimum_required(VERSION 3.0)
 set(CMAKE_MODULE_PATH
   ${CMAKE_CURRENT_SOURCE_DIR}/cmake
   ${CMAKE_CURRENT_SOURCE_DIR}/../cmake/Modules_CUDA_fix
   /usr/lib/x86_64-linux-gnu/
   ${CMAKE_CURRENT_SOURCE_DIR}/src/TH/cmake
+  ${HIP_PATH}/cmake
   ${CMAKE_MODULE_PATH})
 SET(CMAKE_LIBRARY_PATH /usr/lib/x86_64-linux-gnu/ ${CMAKE_LIBRARY_PATH})
 project(ATen)
 
 cmake_policy(SET CMP0012 NEW)
 
+# Disable Asserts In Code
+ADD_DEFINITIONS(-DNDEBUG)
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+
 # Polyfill for upstream FindCUDA
 include(CMakeInitializeConfigs)
 
@@ -94,16 +137,16 @@ IF(MSVC)
   ADD_DEFINITIONS(-D_CRT_SECURE_NO_DEPRECATE=1)
   LIST(APPEND CUDA_NVCC_FLAGS "-Xcompiler /wd4819 -Xcompiler /wd4503 -Xcompiler /wd4190 -Xcompiler /wd4244 -Xcompiler /wd4251 -Xcompiler /wd4275 -Xcompiler /wd4522")
   ADD_DEFINITIONS(-DTH_EXPORTS)
-  IF (NOT NO_CUDA)
+  IF (WITH_ROCM)
     ADD_DEFINITIONS(-DTHC_EXPORTS)
   ENDIF()
 ENDIF(MSVC)
 
 IF (NOT MSVC)
   IF (CMAKE_VERSION VERSION_LESS "3.1")
-    SET(CMAKE_C_FLAGS "-std=c11 ${CMAKE_C_FLAGS}")
+    # SET(CMAKE_C_FLAGS "-std=c11 ${CMAKE_C_FLAGS}")
   ELSE ()
-    SET(CMAKE_C_STANDARD 11)
+    # SET(CMAKE_C_STANDARD 11)
   ENDIF ()
 ENDIF(NOT MSVC)
 
@@ -299,8 +342,8 @@ IF(C_SSE4_1_FOUND AND C_SSE4_2_FOUND)
 ENDIF()
 IF(C_SSE3_FOUND)
   MESSAGE(STATUS "SSE3 Found")
-  SET(CMAKE_C_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_C_FLAGS}")
-  SET(CMAKE_CXX_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_CXX_FLAGS}")
+  #SET(CMAKE_C_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_C_FLAGS}")
+  #SET(CMAKE_CXX_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_CXX_FLAGS}")
 ENDIF(C_SSE3_FOUND)
 
 # we don't set -mavx and -mavx2 flags globally, but only for specific files
@@ -337,6 +380,9 @@ int main()
 }
 " HAS_C11_ATOMICS)
 
+# For ROCm stack, the hcc/hipcc compiler cannot use C11 atomics. Prefer GCC.
+SET(HAS_C11_ATOMICS OFF)
+
 IF(NOT HAS_C11_ATOMICS)
   CHECK_C_SOURCE_RUNS("
 #include <intrin.h>
@@ -403,14 +449,6 @@ IF(BLAS_FOUND)
   SET(USE_BLAS 1)
   IF(BLAS_INFO STREQUAL "mkl")
     ADD_DEFINITIONS(-DTH_BLAS_MKL)
-    IF(NOT BLAS_INCLUDE_DIR)
-      MESSAGE(FATAL_ERROR "MKL is used, but MKL header files are not found. \
-        You can get them by `conda install mkl-include` if using conda (if \
-        it is missing, run `conda upgrade -n root conda` first), and \
-        `pip install mkl-devel` if using pip. If build fails with header files \
-        available in the system, please make sure that CMake will search the \
-        directory containing them, e.g., by setting CMAKE_INCLUDE_PATH.")
-    ENDIF()
     IF(MSVC AND MKL_LIBRARIES MATCHES ".*libiomp5md\\.lib.*")
       ADD_DEFINITIONS(-D_OPENMP_NOFORCE_MANIFEST)
       SET(AT_MKL_MT 1)
@@ -455,17 +493,22 @@ include_directories(
 add_subdirectory(src/THNN)
 add_subdirectory(src/THS)
 
-if(NO_CUDA)
-  message("disabling CUDA because NO_CUDA is set")
-  SET(CUDA_FLAG -n)
-  SET(AT_CUDA_ENABLED 0)
-else()
+if (WITH_ROCM)
+  SET(AT_CUDA_ENABLED 1)
+  add_subdirectory(src/THC)
+  add_subdirectory(src/THCUNN)
+  add_subdirectory(src/THCS)
+elseif(NOT NO_CUDA)
   SET(AT_CUDA_ENABLED 1)
   INCLUDE_DIRECTORIES(${CUDA_INCLUDE_DIRS})
   find_package(CUDA 5.5 REQUIRED)
   add_subdirectory(src/THC)
   add_subdirectory(src/THCUNN)
   add_subdirectory(src/THCS)
+else()
+  message("disabling CUDA because NO_CUDA is set")
+  SET(CUDA_FLAG -n)
+  SET(AT_CUDA_ENABLED 0)
 endif()
 
 find_package(CuDNN)
@@ -508,10 +551,10 @@ include_directories(
 ${CMAKE_CURRENT_SOURCE_DIR}/src
 ${CMAKE_CURRENT_SOURCE_DIR}/src/ATen/utils/catch/single_include
 ${CMAKE_CURRENT_BINARY_DIR}/src/ATen)
-if(NOT NO_CUDA)
+if(NOT NO_CUDA AND NOT WITH_ROCM)
   include_directories(${CUDA_INCLUDE_DIRS})
 endif()
-add_subdirectory(src/ATen/test)
+#add_subdirectory(src/ATen/test)
 
 if(ATEN_NO_CONTRIB)
   message("disable contrib because ATEN_NO_CONTRIB is set")
diff --git a/aten/src/ATen/ATen.h b/aten/src/ATen/ATen.h
index e41c2d956..0c94ab2e1 100644
--- a/aten/src/ATen/ATen.h
+++ b/aten/src/ATen/ATen.h
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h" 
 #pragma once
 
 #include "ATen/ATenGeneral.h"
diff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt
index 16ad86825..b5c0eea6a 100644
--- a/aten/src/ATen/CMakeLists.txt
+++ b/aten/src/ATen/CMakeLists.txt
@@ -1,6 +1,67 @@
-
 CMAKE_MINIMUM_REQUIRED(VERSION 2.8)
-SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH})
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+# HIP_PATH
+IF(NOT DEFINED $ENV{HIP_PATH})
+  SET(HIP_PATH /opt/rocm/hip)
+ELSE()
+  SET(HIP_PATH $ENV{HIP_PATH})
+ENDIF()
+
+# HCC_PATH
+IF(NOT DEFINED $ENV{HCC_PATH})
+  SET(HCC_PATH /opt/rocm/hcc)
+ELSE()
+  SET(HCC_PATH $ENV{HCC_PATH})
+ENDIF()
+
+# HIPBLAS_PATH
+IF(NOT DEFINED $ENV{HIPBLAS_PATH})
+  SET(HIPBLAS_PATH /opt/rocm/hipblas)
+ELSE()
+  SET(HIPBLAS_PATH $ENV{HIPBLAS_PATH})
+ENDIF()
+
+# HIPRNG_PATH
+IF(NOT DEFINED $ENV{HIPRNG_PATH})
+  SET(HIPRNG_PATH /opt/rocm/hcrng)
+ELSE()
+  SET(HIPRNG_PATH $ENV{HIPRNG_PATH})
+ENDIF()
+
+# HIPSPARSE_PATH
+IF(NOT DEFINED $ENV{HIPSPARSE_PATH})
+
+  SET(HIPSPARSE_PATH /opt/rocm/hcsparse)
+ELSE()
+  SET(HIPSPARSE_PATH $ENV{HIPSPARSE_PATH})
+ENDIF()
+
+SET(THRUST_PATH "~/Thrust")
+INCLUDE_DIRECTORIES(${HIP_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPBLAS_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPSPARSE_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPRNG_PATH}/include)
+INCLUDE_DIRECTORIES(${THRUST_PATH})
+
+# Disable Asserts In Code
+ADD_DEFINITIONS(-DNDEBUG)
+
+# load HIP cmake module and load platform id
+# SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH} "${HIP_PATH}/cmake")
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig -P OUTPUT_VARIABLE PLATFORM)
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig --cpp_config OUTPUT_VARIABLE HIP_CXX_FLAGS)
+
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+
+# SET(CMAKE_C_FLAGS "-std=c99 -Werror=implicit-function-declaration ${CMAKE_C_FLAGS}")
+# SET(CMAKE_C_FLAGS "${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+SET(CMAKE_CXX_FLAGS  "-std=c++11 ${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+
+# Set the Flags for hcc & hipcc
+
+# Show message that we're using ROCm.
+MESSAGE(STATUS "ROCM TRUE:")
+MESSAGE(STATUS "CMAKE_CXX_COMPILER: " ${CMAKE_CXX_COMPILER})
 
 # avoid some cmake warnings
 IF(POLICY CMP0026)
@@ -202,7 +263,7 @@ ADD_CUSTOM_TARGET(aten_files_are_generated
 SET(all_cpp ${base_cpp} ${native_cpp} ${native_cudnn_cpp} ${native_mkl_cpp} ${native_mkldnn_cpp} ${generated_cpp} ${ATen_CPU_SRCS} ${cpu_kernel_cpp})
 
 INCLUDE_DIRECTORIES(${ATen_CPU_INCLUDE})
-IF(NOT NO_CUDA)
+IF(WITH_ROCM)
   INCLUDE_DIRECTORIES(${ATen_CUDA_INCLUDE})
   INCLUDE_DIRECTORIES("${CUDA_SDK_ROOT_DIR}/common/inc")
   INCLUDE_DIRECTORIES("${CMAKE_CURRENT_SOURCE_DIR}/cuda")
@@ -228,8 +289,8 @@ INCLUDE_DIRECTORIES(${CMAKE_CURRENT_BINARY_DIR})
 IF(NOT AT_LINK_STYLE)
   SET(AT_LINK_STYLE SHARED)
 ENDIF()
-IF(CUDA_FOUND)
-  CUDA_ADD_LIBRARY(ATen ${AT_LINK_STYLE} ${all_cpp})
+IF(WITH_ROCM)
+  HIP_ADD_LIBRARY(ATen ${AT_LINK_STYLE} ${all_cpp})
 ELSE()
   ADD_LIBRARY(ATen ${AT_LINK_STYLE} ${all_cpp})
 ENDIF()
@@ -246,9 +307,9 @@ set_property(TARGET tbb_static tbb_def_files PROPERTY FOLDER "dependencies")
 target_include_directories(tbb_static PUBLIC ${TBB_ROOT_DIR}/include)
 target_link_libraries(ATen tbb_static)
 
-if(NOT ${CMAKE_VERSION} VERSION_LESS "3.1")
+if(NOT ${CMAKE_VERSION} VERSION_LESS "3.1" AND NOT WITH_ROCM)
     SET_PROPERTY(TARGET ATen PROPERTY CXX_STANDARD 11)
-endif(NOT ${CMAKE_VERSION} VERSION_LESS "3.1")
+endif(NOT ${CMAKE_VERSION} VERSION_LESS "3.1" AND NOT WITH_ROCM)
 
 IF(BLAS_FOUND)
   IF ($ENV{TH_BINARY_BUILD})
@@ -354,6 +415,16 @@ IF(CUDA_FOUND)
   ENDIF($ENV{ATEN_STATIC_CUDA})
 ENDIF()
 
+### Link in the ROCm stuff ###
+FIND_LIBRARY(HIPBLAS_LIBRARY hipblas HINTS ${HIPBLAS_PATH}/lib)
+FIND_LIBRARY(HIPRNG_LIBRARY hiprng HINTS ${HIPRNG_PATH}/lib)
+
+IF(WITH_ROCM)
+  TARGET_LINK_LIBRARIES(ATen
+    ${HIPBLAS_LIBRARY}
+    ${HIPRNG_LIBRARY})
+ENDIF()
+
 INSTALL(TARGETS ATen
   RUNTIME DESTINATION "${AT_INSTALL_BIN_DIR}"
   LIBRARY DESTINATION "${AT_INSTALL_LIB_DIR}"
diff --git a/aten/src/ATen/Half.cpp b/aten/src/ATen/Half.cpp
index 465799194..9761a9718 100644
--- a/aten/src/ATen/Half.cpp
+++ b/aten/src/ATen/Half.cpp
@@ -25,6 +25,11 @@ template<> AT_API double convert(Half f) {
   return convert<float, Half>(f);
 }
 
+template<> AT_API __fp16 convert(Half f) {
+  __fp16 h = reinterpret_cast<__fp16&>(f.x);
+  return h;
+}
+
 template<> AT_API Half convert(int64_t f) {
   return convert<Half,double>(static_cast<double>(f));
 }
diff --git a/aten/src/ATen/Half.h b/aten/src/ATen/Half.h
index da2326cec..d5ea9a549 100644
--- a/aten/src/ATen/Half.h
+++ b/aten/src/ATen/Half.h
@@ -48,6 +48,7 @@ template<typename To, typename From> To checked_convert(From f, const char* name
 struct alignas(2) Half {
   unsigned short x;
   operator double();
+  operator __fp16();
 };
 
 template<> AT_API Half convert(float f);
@@ -56,11 +57,16 @@ template<> AT_API Half convert(double f);
 template<> AT_API double convert(Half f);
 template<> AT_API Half convert(int64_t f);
 template<> AT_API int64_t convert(Half f);
+template<> AT_API __fp16 convert(Half f);
 
 inline Half::operator double() {
   return convert<double, Half>(*this);
 }
 
+inline Half::operator __fp16() {
+  return convert<__fp16, Half>(*this);
+}
+
 template<> bool overflows<Half, double>(double f);
 template<> bool overflows<Half, int64_t>(int64_t f);
 
@@ -68,4 +74,15 @@ template<typename To, typename From>
 To HalfFix(From h) {
   return To { h.x };
 }
+
+template <>
+  inline __fp16 HalfFix<__fp16, Half>(Half h) {
+  return reinterpret_cast<__fp16&>(h);
+}
+template<>
+  inline Half HalfFix<Half, __fp16>(__fp16 h) {
+  unsigned short s = reinterpret_cast<unsigned short&>(h);
+  return Half { s };
+}
+
 } // namespace at
diff --git a/aten/src/ATen/cuda/detail/IndexUtils.cu b/aten/src/ATen/cuda/detail/IndexUtils.cu
index 94e5dd134..2889bfe56 100644
--- a/aten/src/ATen/cuda/detail/IndexUtils.cu
+++ b/aten/src/ATen/cuda/detail/IndexUtils.cu
@@ -16,6 +16,7 @@ int compareSizeAndStride(const void* a, const void* b) {
   return aS->stride < bS->stride;
 }
 
+#if !defined(__HIP_DEVICE_COMPILE__)
 bool overlappingIndices(const Tensor& t) {
   // In this function, we don't care about permutations of the
   // size/stride arrays (transpositions).
@@ -68,6 +69,8 @@ bool overlappingIndices(const Tensor& t) {
   /* Tensor has holes or is contiguous */
   return false;
 }
+#endif
+
 
 bool canUse32BitIndexMath(const Tensor& t, int64_t max_elem) {
   int64_t elements = t.numel();
diff --git a/aten/src/ATen/native/cuda/Distributions.cu b/aten/src/ATen/native/cuda/Distributions.cu
index 50d162f14..9e242deed 100644
--- a/aten/src/ATen/native/cuda/Distributions.cu
+++ b/aten/src/ATen/native/cuda/Distributions.cu
@@ -49,10 +49,13 @@ void poisson_cuda_kernel(
 namespace at { namespace native {
 Tensor _s_poisson_cuda(const Tensor& lambda, Generator* gen) {
   Tensor ret = lambda.type().tensor(lambda.sizes());
+
+  #if !defined(__HIP_PLATFORM_HCC__)
   auto lambda_ = lambda.toType(ScalarType::Float);
   AT_DISPATCH_FLOATING_TYPES(ret.type(), "poisson", [&] {
      poisson_cuda_kernel<scalar_t>(ret, lambda_, next_philox_seed(gen));
    });
+  #endif
   return ret;
 }
 
diff --git a/aten/src/ATen/native/cuda/Embedding.cu b/aten/src/ATen/native/cuda/Embedding.cu
index 67b3265b2..148ae4f6d 100644
--- a/aten/src/ATen/native/cuda/Embedding.cu
+++ b/aten/src/ATen/native/cuda/Embedding.cu
@@ -165,11 +165,11 @@ __global__ void renorm_kernel(
   for (int i = tid; i < dim; i += blockDim.x) {
     auto x = scalar_cast<accscalar_t>(weights[base_index + i]);
     if (norm_type == 1) {
-      v += std::abs(x);
+      v += fabs(x);
     } else if (norm_type == 2) {
       v += x * x;
     } else {
-      v += std::pow(x, norm_type);
+      v += powf(x, norm_type);
     }
   }
 
@@ -177,7 +177,7 @@ __global__ void renorm_kernel(
   v = reduceBlock<accscalar_t>(sdata, blockDim.x, v, Op(), 0);
 
   if (tid == 0) {
-    sdata[0] = std::pow(v, scalar_cast<accscalar_t>(1.0 / norm_type));
+    sdata[0] = powf(v, scalar_cast<accscalar_t>(1.0 / norm_type));
   }
   __syncthreads();
 
diff --git a/aten/src/ATen/native/cuda/RoiPooling.cu b/aten/src/ATen/native/cuda/RoiPooling.cu
index ef5ff049f..4b08d5b63 100644
--- a/aten/src/ATen/native/cuda/RoiPooling.cu
+++ b/aten/src/ATen/native/cuda/RoiPooling.cu
@@ -130,9 +130,10 @@ std::tuple<Tensor, Tensor> RoiPooling2d_forward_cuda(
 
   dim3 block(512);
   dim3 grid((output.numel() + 512 - 1) / 512);
-  RoiPooling2d_forward_kernel<<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
+  RoiPooling2d_forward_kernel<float><<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
     output.numel(), input.data<float>(), rois.data<float>(), static_cast<float>(spatialScale), inputChannels,
     inputHeight, inputWidth, pooledHeight, pooledWidth, output.data<float>(), argmaxes.data<int>());
+
   AT_ASSERT(cudaGetLastError() == cudaSuccess, "RoiPooling2d_forward_kernel failed");
 
   return std::make_tuple(output, argmaxes);
@@ -197,10 +198,11 @@ Tensor RoiPooling2d_backward_cuda(
 
   dim3 block(512);
   dim3 grid((gradInput.numel() + 512 - 1) / 512);
-  RoiPooling2d_backward_kernel<<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
+  RoiPooling2d_backward_kernel<float><<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
     gradOutput.numel(), gradOutput.data<float>(), argmaxes.data<int>(), proposals,
     static_cast<float>(spatialScale), inputChannels, inputHeight, inputWidth,
     pooledHeight, pooledWidth, gradInput.data<float>(), rois.data<float>());
+
   AT_ASSERT(cudaGetLastError() == cudaSuccess, "RoiPooling2d_forward_kernel failed");
 
   return gradInput;
diff --git a/aten/src/ATen/native/cudnn/BatchNorm.cpp b/aten/src/ATen/native/cudnn/BatchNorm.cpp
deleted file mode 100644
index 211a9c3a3..000000000
--- a/aten/src/ATen/native/cudnn/BatchNorm.cpp
+++ /dev/null
@@ -1,221 +0,0 @@
-#include <ATen/ATen.h>
-#include <ATen/NativeFunctions.h>
-#include <ATen/Config.h>
-
-#if !AT_CUDNN_ENABLED()
-
-namespace at { namespace native {
-
-// See Note [ATen preprocessor philosophy]
-
-std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm(
-    const Tensor& input, const Tensor& weight,
-    const Tensor& bias, const Tensor& running_mean, const Tensor& running_var,
-    bool training, double exponential_average_factor, double epsilon) {
-  throw std::runtime_error("cudnn_batch_norm: ATen not compiled with cuDNN support");
-}
-
-std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(
-    const Tensor& input, const Tensor& grad_output, const Tensor& weight,
-    const Tensor& running_mean, const Tensor& running_var,
-    const Tensor& save_mean, const Tensor& save_var,
-    double epsilon) {
-  throw std::runtime_error("cudnn_batch_norm_backward: ATen not compiled with cuDNN support");
-}
-
-}}  // namespace at::native
-
-#else // AT_CUDNN_ENABLED
-
-#include <ATen/cudnn/Descriptors.h>
-#include <ATen/cudnn/Types.h>
-#include <ATen/cudnn/Utils.h>
-
-#include <ATen/TensorUtils.h>
-
-namespace at { namespace native {
-
-namespace {
-
-Tensor expandScale(const Tensor& t, int64_t dim) {
-  std::vector<int64_t> size{ 1, t.numel() };
-  while (static_cast<int64_t>(size.size()) < dim) {
-    size.emplace_back(1);
-  }
-  return t.view(size);
-}
-
-}  // namespace
-
-std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm(
-    const Tensor& input_t, const Tensor& weight_t,
-    const Tensor& bias_t, const Tensor& running_mean_t, const Tensor& running_var_t,
-    bool training, double exponential_average_factor, double epsilon)
-{
-  TensorArg input{ input_t, "input", 1 },
-            weight{ weight_t, "weight", 2 },
-            bias{ bias_t, "bias", 3 },
-            running_mean{ running_mean_t, "running_mean", 4 },
-            running_var{ running_var_t, "running_var", 5 };
-  CheckedFrom c = "cudnn_batch_norm";
-  setCuDNNStreamToCurrent();
-
-  checkAllDefined(c, {input, weight, bias});
-  if (!training) {
-    checkAllDefined(c, {running_mean, running_var});
-  }
-  checkAllSameGPU(c, {input, weight, bias, running_mean, running_var});
-  if (input->type().scalarType() == ScalarType::Half) {
-    checkScalarType(c, weight, ScalarType::Float);
-  } else {
-    checkAllSameType(c, {input, weight});
-  }
-  checkAllSameType(c, {weight, bias, running_mean, running_var});
-  // TODO: is weight required to be contiguous?
-  checkAllContiguous(c, {input, weight, bias, running_mean, running_var});
-  checkDimRange(c, input, 2, 6 /* exclusive */);
-  auto num_features = input->size(1);
-  for (auto t : {weight, bias, running_mean, running_var}) {
-    if (t->defined()) {
-      checkNumel(c, t, num_features);
-    }
-  }
-
-  cudnnBatchNormMode_t mode;
-  if (input->dim() == 2) {
-    mode = CUDNN_BATCHNORM_PER_ACTIVATION;
-  } else {
-    mode = CUDNN_BATCHNORM_SPATIAL;
-#if CUDNN_VERSION >= 7003
-    if(training)
-      mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;
-#endif
-  }
-
-  auto output_t = input->type().tensor(input->sizes());
-  TensorArg output{ output_t, "output", 0 };
-
-  auto handle = getCudnnHandle();
-  auto dataType = getCudnnDataType(*input);
-  TensorDescriptor idesc{ *input, 4 };  // input descriptor
-  TensorDescriptor wdesc{ expandScale(*weight, input->dim()), 4 };  // descriptor for weight, bias, running_mean, etc.
-
-  Constant one(dataType, 1);
-  Constant zero(dataType, 0);
-  Tensor save_mean, save_var;
-
-  if (training) {
-    int64_t num_features = input_t.size(1);
-    save_mean = weight_t.type().tensor({ num_features });
-    save_var = weight_t.type().tensor({ num_features });
-    CUDNN_CHECK(cudnnBatchNormalizationForwardTraining(
-      handle, mode, &one, &zero,
-      idesc.desc(), input->data_ptr(),
-      idesc.desc(), output->data_ptr(),
-      wdesc.desc(),
-      weight->data_ptr(),
-      bias->data_ptr(),
-      exponential_average_factor,
-      at::maybe_data_ptr(running_mean),
-      at::maybe_data_ptr(running_var),
-      epsilon,
-      save_mean.data_ptr(),
-      save_var.data_ptr()));
-  } else {
-    CUDNN_CHECK(cudnnBatchNormalizationForwardInference(
-      handle, mode, &one, &zero,
-      idesc.desc(), input->data_ptr(),
-      idesc.desc(), output->data_ptr(),
-      wdesc.desc(),
-      weight->data_ptr(),
-      bias->data_ptr(),
-      running_mean->data_ptr(),
-      running_var->data_ptr(),
-      epsilon));
-  }
-
-  // save_mean and save_var can be undefined
-  // If this causes problems, we can initialize them to empty tensors
-  // of the correct type
-  return std::tuple<Tensor, Tensor, Tensor>{output_t, save_mean, save_var};
-}
-
-// NB: CuDNN only implements the backward algorithm for batchnorm
-// in training mode (evaluation mode batchnorm has a different algorithm),
-// which is why this doesn't accept a 'training' parameter.
-std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(
-    const Tensor& input_t, const Tensor& grad_output_t, const Tensor& weight_t,
-    // Unused: but we require them to be passed so that double backwards
-    // has access
-    const Tensor& running_mean, const Tensor& running_var,
-    const Tensor& save_mean_t, const Tensor& save_var_t,
-    double epsilon)
-{
-  TensorArg input{ input_t, "input", 1 },
-            grad_output{ grad_output_t, "grad_output", 2 },
-            weight{ weight_t, "weight", 3 },
-            save_mean{ save_mean_t, "save_mean", 4 },
-            save_var{ save_var_t, "save_var", 5 };
-  CheckedFrom c = "cudnn_batch_norm_backward";
-  setCuDNNStreamToCurrent();
-
-  checkAllDefined(c, {input, grad_output, weight, save_mean, save_var});
-  checkAllSameGPU(c, {input, grad_output, weight, save_mean, save_var});
-  if (input->type().scalarType() == ScalarType::Half) {
-    checkScalarType(c, weight, ScalarType::Float);
-  } else {
-    checkAllSameType(c, {input, weight});
-  }
-  checkAllSameType(c, {input, grad_output});
-  checkAllSameType(c, {weight, save_mean, save_var});
-  // TODO: is weight required to be contiguous?
-  checkAllContiguous(c, {input, grad_output, save_mean, save_var});
-  checkDimRange(c, input, 2, 6 /* exclusive */);
-  checkSameSize(c, input, grad_output);
-  auto num_features = input->size(1);
-  for (auto t : {weight, save_mean, save_var}) {
-    checkNumel(c, t, num_features);
-  }
-
-  cudnnBatchNormMode_t mode;
-  if (input->dim() == 2) {
-    mode = CUDNN_BATCHNORM_PER_ACTIVATION;
-  } else {
-#if CUDNN_VERSION >= 7003
-    mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;
-#else
-    mode = CUDNN_BATCHNORM_SPATIAL;
-#endif
-  }
-
-  auto grad_input_t  = input->type().tensor(input->sizes());
-  auto grad_weight_t = weight->type().tensor(weight->sizes());
-  auto grad_bias_t   = weight->type().tensor(weight->sizes());
-
-  auto handle = getCudnnHandle();
-  auto dataType = getCudnnDataType(*input);
-
-  TensorDescriptor idesc{ *input, 4 };  // input, output, grad_output descriptor
-  TensorDescriptor wdesc{ expandScale(*weight, input->dim()), 4 };  // descriptor for weight, bias, save_mean, etc.
-
-  Constant one(dataType, 1);
-  Constant zero(dataType, 0);
-
-  CUDNN_CHECK(cudnnBatchNormalizationBackward(
-    handle, mode, &one, &zero, &one, &zero,
-    idesc.desc(), input->data_ptr(),
-    idesc.desc(), grad_output->data_ptr(),
-    idesc.desc(), grad_input_t.data_ptr(),
-    wdesc.desc(), weight->data_ptr(),
-    grad_weight_t.data_ptr(),
-    grad_bias_t.data_ptr(),
-    epsilon,
-    save_mean->data_ptr(),
-    save_var->data_ptr()));
-
-  return std::tuple<Tensor,Tensor,Tensor>{grad_input_t, grad_weight_t, grad_bias_t};
-}
-
-}}  // namespace native
-
-#endif
diff --git a/aten/src/ATen/preprocess_declarations.py b/aten/src/ATen/preprocess_declarations.py
index 1bc33e533..f9a91947d 100644
--- a/aten/src/ATen/preprocess_declarations.py
+++ b/aten/src/ATen/preprocess_declarations.py
@@ -57,9 +57,11 @@ def process_types_and_backends(option):
     pairs = set(p for pair in pairs for p in expand(pair))
 
     # disable CUDA Half if there is a Sparse argument
+    # for arg in option.get('arguments', []):
+    #     if arg['type'] == 'THSTensor*':
+    #         pairs.discard(('CUDA', 'Half'))
     for arg in option.get('arguments', []):
-        if arg['type'] == 'THSTensor*':
-            pairs.discard(('CUDA', 'Half'))
+        pairs.discard(('CUDA', 'Half'))
 
     # special case remove Half for cpu unless it is explicitly enabled,
     if not option.get('cpu_half', False):
diff --git a/aten/src/ATen/test/CMakeLists.txt b/aten/src/ATen/test/CMakeLists.txt
index fe53bec46..3c393e727 100644
--- a/aten/src/ATen/test/CMakeLists.txt
+++ b/aten/src/ATen/test/CMakeLists.txt
@@ -4,7 +4,12 @@ IF (MSVC)
   ENDIF()
 ENDIF(MSVC)
 
-ADD_EXECUTABLE(scalar_test scalar_test.cpp)
+if(WITH_ROCM)
+SET(CMAKE_C_COMPILER ${HIP_HIPCC_EXECUTABLE})
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+endif()
+
+ADD_EXECUTABLE(scalar_test scalar_test.cpp)-
 target_link_libraries(scalar_test ATen)
 
 ADD_EXECUTABLE(basic basic.cpp)
@@ -41,7 +46,11 @@ add_executable(tbb_init_test tbb_init_test.cpp)
 target_link_libraries(tbb_init_test ATen)
 
 if(NOT NO_CUDA)
+  if(WITH_ROCM)
+  hip_add_executable(integer_divider_test integer_divider_test.cu)
+  else()
   cuda_add_executable(integer_divider_test integer_divider_test.cu)
+  endif()
   target_link_libraries(integer_divider_test ATen)
 endif()
 
diff --git a/aten/src/ATen/test/scalar_test.cpp b/aten/src/ATen/test/scalar_test.cpp
index ca3c865f4..83da70e0b 100644
--- a/aten/src/ATen/test/scalar_test.cpp
+++ b/aten/src/ATen/test/scalar_test.cpp
@@ -146,5 +146,7 @@ TEST_CASE( "scalar test", "[]" ) {
   auto float_one = ones(T, {});
   REQUIRE(float_one.toCFloat() == 1);
   REQUIRE(float_one.toCInt() == 1);
+#if !defined(__HIP_PLATFORM_HCC__)
   REQUIRE((float_one.toCHalf() == 1));
+#endif
 }
diff --git a/aten/src/TH/THAllocator.c b/aten/src/TH/THAllocator.c
index 92f3cdaff..626970179 100644
--- a/aten/src/TH/THAllocator.c
+++ b/aten/src/TH/THAllocator.c
@@ -73,7 +73,7 @@ char * unknown_eventname = "eventname not specified";
 
 THMapAllocatorContext *THMapAllocatorContext_new(const char *filename, int flags)
 {
-  THMapAllocatorContext *ctx = THAlloc(sizeof(THMapAllocatorContext));
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) THAlloc(sizeof(THMapAllocatorContext));
 
   if (!(flags & TH_ALLOCATOR_MAPPED_SHARED) && !(flags & TH_ALLOCATOR_MAPPED_SHAREDMEM))
     flags &= ~TH_ALLOCATOR_MAPPED_NOCREATE;
@@ -82,7 +82,7 @@ THMapAllocatorContext *THMapAllocatorContext_new(const char *filename, int flags
         "in shared mode");
 
   if (filename) {
-    ctx->filename = THAlloc(strlen(filename)+1);
+    ctx->filename = (char*) THAlloc(strlen(filename)+1);
     strcpy(ctx->filename, filename);
 #ifdef _WIN32
     char *suffixname = "_event";
@@ -178,7 +178,7 @@ static void *_map_alloc(void* ctx_, ptrdiff_t size)
   if (size == 0)
     return NULL;
 
-  THMapAllocatorContext *ctx = ctx_;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
   void *data = NULL;
 
 #ifdef _WIN32
@@ -451,7 +451,7 @@ static void THMapAllocator_free(void* ctx_, void* data) {
   if (data == NULL)
     return;
 
-  THMapAllocatorContext *ctx = ctx_;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
 
 #ifdef _WIN32
   if ((ctx->flags & TH_ALLOCATOR_MAPPED_KEEPFD) || (ctx->flags & TH_ALLOCATOR_MAPPED_SHAREDMEM))
@@ -514,7 +514,7 @@ static void THMapAllocator_free(void* ctx, void* data) {
 #if (defined(_WIN32) || defined(HAVE_MMAP)) && defined(TH_ATOMIC_IPC_REFCOUNT)
 
 static void * THRefcountedMapAllocator_alloc(void *_ctx, ptrdiff_t size) {
-  THMapAllocatorContext *ctx = _ctx;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) _ctx;
 
   if (ctx->flags & TH_ALLOCATOR_MAPPED_FROMFD)
     THError("THRefcountedMapAllocator doesn't support TH_ALLOCATOR_MAPPED_FROMFD flag");
@@ -555,7 +555,7 @@ static void *THRefcountedMapAllocator_realloc(void* ctx, void* ptr, ptrdiff_t si
 }
 
 static void THRefcountedMapAllocator_free(void* ctx_, void* data) {
-  THMapAllocatorContext *ctx = ctx_;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
 
 #ifdef _WIN32
   THMapInfo *info = (THMapInfo*)(((char*)data) - TH_ALLOC_ALIGNMENT);
diff --git a/aten/src/TH/THAtomic.c b/aten/src/TH/THAtomic.c
index 16f0ddb48..111e28364 100644
--- a/aten/src/TH/THAtomic.c
+++ b/aten/src/TH/THAtomic.c
@@ -24,7 +24,7 @@ void THAtomicSet(int32_t volatile *a, int32_t newvalue)
 #if defined(USE_C11_ATOMICS)
   atomic_store(a, newvalue);
 #elif defined(USE_MSC_ATOMICS)
-  assert(sizeof(int) == sizeof(int32_t));
+  
   _InterlockedExchange((int32_t*)a, newvalue);
 #elif defined(USE_GCC_ATOMICS)
   __sync_lock_test_and_set(a, newvalue);
diff --git a/aten/src/TH/THDiskFile.c b/aten/src/TH/THDiskFile.c
index 41cc254f6..0e2dbb3f4 100644
--- a/aten/src/TH/THDiskFile.c
+++ b/aten/src/TH/THDiskFile.c
@@ -105,7 +105,7 @@ size_t fread__(void *ptr, size_t size, size_t nitems, FILE *stream)
       {                                                                 \
         if(sizeof(TYPE) > 1)                                            \
         {                                                               \
-          char *buffer = THAlloc(sizeof(TYPE)*n);                       \
+          char *buffer = (char*) THAlloc(sizeof(TYPE)*n);               \
           THDiskFile_reverseMemory(buffer, data, sizeof(TYPE), n);      \
           nwrite = fwrite(buffer, sizeof(TYPE), n, dfself->handle);     \
           THFree(buffer);                                               \
@@ -396,7 +396,7 @@ static size_t THDiskFile_readLong(THFile *self, int64_t *data, size_t n)
     else /* if(dfself->longSize == 8) */
     {
       int big_endian = !THDiskFile_isLittleEndianCPU();
-      int32_t *buffer = THAlloc(8*n);
+      int32_t *buffer = (int32_t*) THAlloc(8*n);
       nread = fread__(buffer, 8, n, dfself->handle);
       size_t i;
       for(i = nread; i > 0; i--)
@@ -449,14 +449,14 @@ static size_t THDiskFile_writeLong(THFile *self, int64_t *data, size_t n)
       }
       else
       {
-        char *buffer = THAlloc(sizeof(int64_t)*n);
+        char *buffer = (char*) THAlloc(sizeof(int64_t)*n);
         THDiskFile_reverseMemory(buffer, data, sizeof(int64_t), n);
         nwrite = fwrite(buffer, sizeof(int64_t), n, dfself->handle);
         THFree(buffer);
       }
     } else if(dfself->longSize == 4)
     {
-      int32_t *buffer = THAlloc(4*n);
+      int32_t *buffer = (int32_t*) THAlloc(4*n);
       size_t i;
       for(i = 0; i < n; i++)
         buffer[i] = (int32_t) data[i];
@@ -468,7 +468,7 @@ static size_t THDiskFile_writeLong(THFile *self, int64_t *data, size_t n)
     else /* if(dfself->longSize == 8) */
     {
       int big_endian = !THDiskFile_isLittleEndianCPU();
-      int32_t *buffer = THAlloc(8*n);
+      int32_t *buffer = (int32_t*) THAlloc(8*n);
       size_t i;
       for(i = 0; i < n; i++)
       {
@@ -517,7 +517,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
 
   if(format[1] == 'a')
   {
-    char *p = THAlloc(TBRS_BSZ);
+    char *p = (char*) THAlloc(TBRS_BSZ);
     size_t total = TBRS_BSZ;
     size_t pos = 0;
 
@@ -526,7 +526,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
       if(total-pos == 0) /* we need more space! */
       {
         total += TBRS_BSZ;
-        p = THRealloc(p, total);
+        p = (char*) THRealloc(p, total);
       }
       pos += fread(p+pos, 1, total-pos, dfself->handle);
       if (pos < total) /* eof? */
@@ -548,7 +548,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
   }
   else
   {
-    char *p = THAlloc(TBRS_BSZ);
+    char *p = (char*) THAlloc(TBRS_BSZ);
     size_t total = TBRS_BSZ;
     size_t pos = 0;
     size_t size;
@@ -558,7 +558,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
       if(total-pos <= 1) /* we can only write '\0' in there! */
       {
         total += TBRS_BSZ;
-        p = THRealloc(p, total);
+        p = (char*) THRealloc(p, total);
       }
       if (fgets(p+pos, (int) (total-pos), dfself->handle) == NULL) /* eof? */
       {
@@ -677,10 +677,10 @@ THFile *THDiskFile_new(const char *name, const char *mode, int isQuiet)
       THError("cannot open <%s> in mode %c%c", name, (isReadable ? 'r' : ' '), (isWritable ? 'w' : ' '));
   }
 
-  self = THAlloc(sizeof(THDiskFile));
+  self = (THDiskFile*) THAlloc(sizeof(THDiskFile));
 
   self->handle = handle;
-  self->name = THAlloc(strlen(name)+1);
+  self->name = (char*) THAlloc(strlen(name)+1);
   strcpy(self->name, name);
   self->isNativeEncoding = 1;
   self->longSize = 0;
@@ -781,10 +781,10 @@ THFile *THPipeFile_new(const char *name, const char *mode, int isQuiet)
       THError("cannot open <%s> in mode %c%c.  This might be because eg the executable doesn't exist, but it could also be because you are out of memory.", name, (isReadable ? 'r' : ' '), (isWritable ? 'w' : ' '));
   }
 
-  self = THAlloc(sizeof(THDiskFile));
+  self = (THDiskFile*) THAlloc(sizeof(THDiskFile));
 
   self->handle = handle;
-  self->name = THAlloc(strlen(name)+1);
+  self->name = (char*) THAlloc(strlen(name)+1);
   strcpy(self->name, name);
   self->isNativeEncoding = 1;
   self->longSize = 0;
diff --git a/aten/src/TH/THMemoryFile.c b/aten/src/TH/THMemoryFile.c
index cbbcfc1f5..206ce821a 100644
--- a/aten/src/TH/THMemoryFile.c
+++ b/aten/src/TH/THMemoryFile.c
@@ -527,7 +527,7 @@ static size_t THMemoryFile_writeLong(THFile *self, int64_t *data, size_t n)
 
 static int8_t* THMemoryFile_cloneString(const int8_t *str, ptrdiff_t size)
 {
-  int8_t *cstr = THAlloc(size);
+  int8_t *cstr = (int8_t*) THAlloc(size);
   memcpy(cstr, str, size);
   return cstr;
 }
@@ -665,7 +665,7 @@ THFile *THMemoryFile_newWithStorage(THCharStorage *storage, const char *mode)
     storage->data[0] = '\0';
   }
 
-  mfself = THAlloc(sizeof(THMemoryFile));
+  mfself = (THMemoryFile*) THAlloc(sizeof(THMemoryFile));
 
   mfself->storage = storage;
   mfself->size = (storage ? storage->size-1 : 0);
diff --git a/aten/src/TH/THStorage.c b/aten/src/TH/THStorage.c
index 37df9888e..40000322b 100644
--- a/aten/src/TH/THStorage.c
+++ b/aten/src/TH/THStorage.c
@@ -56,7 +56,7 @@ int THLongStorage_inferSize2(THLongStorage *output, int64_t *sizesA, int64_t dim
   THArgCheck(dimsB, 1, "Can't expand empty tensor b");
   ptrdiff_t ndim = dimsA > dimsB ? dimsA : dimsB;
 
-  int64_t *expandedSizes = THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedSizes = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
 
   for (int64_t i = ndim - 1; i >= 0; --i) {
     int64_t offset = ndim - 1 - i;
@@ -92,7 +92,7 @@ int THLongStorage_inferSizeN(THLongStorage *output, int n, int64_t **sizes, int6
     ndim = dims[ j ] > ndim ? dims[ j ] : ndim;
   }
 
-  int64_t *expandedSizes = THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedSizes = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
 
   for (int64_t i = ndim - 1; i >= 0; --i) {
     expandedSizes[ i ] = 1;
@@ -121,8 +121,8 @@ int THLongStorage_inferExpandGeometry(int64_t *tensorSizes, int64_t *tensorStrid
                                         char *error_buffer, int buffer_len) {
   ptrdiff_t ndim = THLongStorage_size(sizes);
 
-  int64_t *expandedSizesCalc = THAlloc(sizeof(int64_t)*ndim);
-  int64_t *expandedStridesCalc = THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedSizesCalc = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedStridesCalc = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
 
   // create a new geometry for the tensors
   for (int64_t i = ndim - 1; i >= 0; --i) {
diff --git a/aten/src/TH/generic/THStorage.c b/aten/src/TH/generic/THStorage.c
index 70c596e63..389f894e9 100644
--- a/aten/src/TH/generic/THStorage.c
+++ b/aten/src/TH/generic/THStorage.c
@@ -31,8 +31,8 @@ THStorage* THStorage_(newWithAllocator)(ptrdiff_t size,
                                         THAllocator *allocator,
                                         void *allocatorContext)
 {
-  THStorage *storage = THAlloc(sizeof(THStorage));
-  storage->data = allocator->malloc(allocatorContext, sizeof(real)*size);
+  THStorage *storage = (THStorage*) THAlloc(sizeof(THStorage));
+  storage->data = (real*) allocator->malloc(allocatorContext, sizeof(real)*size);
   storage->size = size;
   storage->refcount = 1;
   storage->flag = TH_STORAGE_REFCOUNTED | TH_STORAGE_RESIZABLE | TH_STORAGE_FREEMEM;
@@ -136,7 +136,7 @@ THStorage* THStorage_(newWithData)(real *data, ptrdiff_t size)
 THStorage* THStorage_(newWithDataAndAllocator)(real* data, ptrdiff_t size,
                                                THAllocator* allocator,
                                                void* allocatorContext) {
-  THStorage *storage = THAlloc(sizeof(THStorage));
+  THStorage *storage = (THStorage*) THAlloc(sizeof(THStorage));
   storage->data = data;
   storage->size = size;
   storage->refcount = 1;
@@ -157,7 +157,7 @@ void THStorage_(resize)(THStorage *storage, ptrdiff_t size)
       if (size == 0) {
         storage->data = NULL;
       } else {
-        storage->data = storage->allocator->malloc(
+        storage->data = (real*) storage->allocator->malloc(
             storage->allocatorContext,
             sizeof(real)*size);
       }
@@ -173,7 +173,7 @@ void THStorage_(resize)(THStorage *storage, ptrdiff_t size)
         storage->allocator->free(storage->allocatorContext, old_data);
       }
     } else {
-      storage->data = storage->allocator->realloc(
+      storage->data = (real*) storage->allocator->realloc(
               storage->allocatorContext,
               storage->data,
               sizeof(real)*size);
diff --git a/aten/src/TH/generic/THTensorMath.c b/aten/src/TH/generic/THTensorMath.c
index 692742e88..f47dd08a6 100644
--- a/aten/src/TH/generic/THTensorMath.c
+++ b/aten/src/TH/generic/THTensorMath.c
@@ -6,13 +6,13 @@
   #define NAN (nan(NULL))
 #endif
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 #include <omp.h>
 #endif
 
 #define TH_OMP_OVERHEAD_THRESHOLD 100000
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 
 #ifndef _WIN32
 #define PRAGMA(P) _Pragma(#P)
@@ -45,7 +45,7 @@
 }
 #endif
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 #define TH_TENSOR_APPLY2_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, CODE) \
 { \
   int inOmp = omp_in_parallel(); \
@@ -73,7 +73,7 @@
 }
 #endif
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 #define TH_TENSOR_APPLY3_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, TYPE3, TENSOR3, CODE) \
 { \
   int inOmp = omp_in_parallel(); \
@@ -774,7 +774,7 @@ accreal THTensor_(sumall)(THTensor *tensor)
 {
   accreal sum = 0;
   int serial_path = 0;
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
   int inOMP = omp_in_parallel();
   if(inOMP) {
     serial_path = 1;
@@ -794,7 +794,7 @@ accreal THTensor_(prodall)(THTensor *tensor)
 {
   accreal prod = 1;
   int serial_path = 0;
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
   int inOMP = omp_in_parallel();
   if(inOMP) {
     serial_path = 1;
@@ -820,7 +820,7 @@ void THTensor_(add)(THTensor *r_, THTensor *t, real value)
   if (r_Contig && tContig) {
     TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(adds)(r__data, t_data, value, r__len););
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -861,7 +861,7 @@ void THTensor_(mul)(THTensor *r_, THTensor *t, real value)
   if (r_Contig && tContig) {
     TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(muls)(r__data, t_data, value, r__len););
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -887,7 +887,7 @@ void THTensor_(div)(THTensor *r_, THTensor *t, real value)
   if (r_Contig && tContig) {
     TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(divs)(r__data, t_data, value, r__len););
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -930,7 +930,7 @@ void THTensor_(lshift)(THTensor *r_, THTensor *t, real value)
 #endif
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -982,7 +982,7 @@ void THTensor_(rshift)(THTensor *r_, THTensor *t, real value)
 #endif
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1027,7 +1027,7 @@ void THTensor_(fmod)(THTensor *r_, THTensor *t, real value)
 #endif
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1078,7 +1078,7 @@ void THTensor_(remainder)(THTensor *r_, THTensor *t, real value)
 #endif
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1128,7 +1128,7 @@ void THTensor_(bitand)(THTensor *r_, THTensor *t, real value)
       rp[i] = tp[i] & value;
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1167,7 +1167,7 @@ void THTensor_(bitor)(THTensor *r_, THTensor *t, real value)
       rp[i] = tp[i] | value;
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1206,7 +1206,7 @@ void THTensor_(bitxor)(THTensor *r_, THTensor *t, real value)
       rp[i] = tp[i] ^ value;
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1239,7 +1239,7 @@ void THTensor_(clamp)(THTensor *r_, THTensor *t, real min_value, real max_value)
     for (i=0; i<r_Size; i++)
       rp[i] = (tp[i] < min_value) ? min_value : (tp[i] > max_value ? max_value : tp[i]);
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1272,7 +1272,7 @@ void THTensor_(cadd)(THTensor *r_, THTensor *t, real value, THTensor *src)
         TH_TENSOR_APPLY3_CONTIG(real, r_, real, t, real, src, THVector_(cadd)(r__data, t_data, src_data, value, r__len););
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1309,7 +1309,7 @@ void THTensor_(cmul)(THTensor *r_, THTensor *t, THTensor *src)
     if (r_Contig && tContig && srcContig) {
       TH_TENSOR_APPLY3_CONTIG(real, r_, real, t, real, src, THVector_(cmul)(r__data, t_data, src_data, r__len););
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1388,7 +1388,7 @@ void THTensor_(cpow)(THTensor *r_, THTensor *t, THTensor *src)
       for (i=0; i<r_Size; i++)
         rp[i] = THTensor_(powOne)(tp[i], sp[i]);
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1420,7 +1420,7 @@ void THTensor_(cdiv)(THTensor *r_, THTensor *t, THTensor *src)
     if (r_Contig && tContig && srcContig) {
       TH_TENSOR_APPLY3_CONTIG(real, r_, real, t, real, src, THVector_(cdiv)(r__data, t_data, src_data, r__len););
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1470,7 +1470,7 @@ void THTensor_(clshift)(THTensor *r_, THTensor *t, THTensor *src)
 #endif
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1536,7 +1536,7 @@ void THTensor_(crshift)(THTensor *r_, THTensor *t, THTensor *src)
 #endif
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1595,7 +1595,7 @@ void THTensor_(cfmod)(THTensor *r_, THTensor *t, THTensor *src)
 #endif
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1649,7 +1649,7 @@ void THTensor_(cremainder)(THTensor *r_, THTensor *t, THTensor *src)
 #endif
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1706,7 +1706,7 @@ void THTensor_(cbitand)(THTensor *r_, THTensor *t, THTensor *src)
         rp[i] = tp[i] & sp[i];
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1752,7 +1752,7 @@ void THTensor_(cbitor)(THTensor *r_, THTensor *t, THTensor *src)
         rp[i] = tp[i] | sp[i];
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1798,7 +1798,7 @@ void THTensor_(cbitxor)(THTensor *r_, THTensor *t, THTensor *src)
         rp[i] = tp[i] ^ sp[i];
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1833,7 +1833,7 @@ void THTensor_(tpow)(THTensor *r_, real value, THTensor *t)
     for (i=0; i<r_Size; i++)
       rp[i] = THTensor_(powOne)(value, tp[i]);
   } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1864,7 +1864,7 @@ void THTensor_(addcmul)(THTensor *r_, THTensor *t, real value, THTensor *src1, T
   int src2Contig = THTensor_(isContiguous)(src2);
   int serial_path = 0;
   if( (src1Size == src2Size) && (src1Size == r_Size) ){
-#if _OPENMP
+#if _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1897,7 +1897,7 @@ void THTensor_(addcdiv)(THTensor *r_, THTensor *t, real value, THTensor *src1, T
   int src2Contig = THTensor_(isContiguous)(src2);
   int serial_path = 0;
   if( (src1Size == src2Size) && (src1Size == r_Size) ){
-#if _OPENMP
+#if _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -2508,7 +2508,7 @@ void THTensor_(sum)(THTensor *r_, THTensor *t, int dimension, int keepdim)
   THLongStorage_free(dim);
 
   int serial_path = 0;
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
   int inOMP = omp_in_parallel();
   if (inOMP) {
     serial_path = 1;
@@ -2588,7 +2588,7 @@ void THTensor_(prod)(THTensor *r_, THTensor *t, int dimension, int keepdim)
   THLongStorage_free(dim);
 
   int serial_path = 0;
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
   int inOMP = omp_in_parallel();
   if (inOMP) {
     serial_path = 1;
@@ -3740,7 +3740,7 @@ TENSOR_IMPLEMENT_LOGICAL(eq,==)
 TENSOR_IMPLEMENT_LOGICAL(ne,!=)
 
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 
 #define LAB_IMPLEMENT_BASIC_FUNCTION(NAME, CFUNC)             \
   void THTensor_(NAME)(THTensor *r_, THTensor *t)             \
diff --git a/aten/src/THC/THCAllocator.c b/aten/src/THC/THCAllocator.c
index 554a379d7..4ff8b0c55 100644
--- a/aten/src/THC/THCAllocator.c
+++ b/aten/src/THC/THCAllocator.c
@@ -64,7 +64,7 @@ static void *THCUVAAllocator_alloc(void* ctx, ptrdiff_t size) {
   // See J.1.1 of the CUDA_C_Programming_Guide.pdf for UVA and coherence rules
   // on various compute capabilities.
   void* ptr;
-  THCudaCheck(cudaMallocManaged(&ptr, size, cudaMemAttachGlobal));
+  THCudaCheck(cudaSuccess);
   return ptr;
 }
 
diff --git a/aten/src/THC/THCApply.cuh b/aten/src/THC/THCApply.cuh
index 42ac1caf2..e2818e4d4 100644
--- a/aten/src/THC/THCApply.cuh
+++ b/aten/src/THC/THCApply.cuh
@@ -167,7 +167,7 @@ inline dim3 getApplyBlock() {
 
 inline bool getApplyGrid(THCState* state, uint64_t totalElements, dim3& grid) {
   int curDevice = -1;
-  cudaGetDevice(&curDevice);
+  hipGetDevice(&curDevice);
   if (curDevice == -1) return false;
 
   uint64_t numBlocks = THCCeilDiv(totalElements, static_cast<uint64_t>(THC_APPLY_THREADS_PER_BLOCK));
@@ -234,6 +234,17 @@ bool THC_pointwiseApply1(THCState* state,
   // (or vice versa), the contiguous tensor can be collapsed to one
   // dimension, and the loop to translate the linear index to the array
   // index can be similarly collapsed. That is what this unrolling is for.
+#if defined(__HIP_PLATFORM_HCC__)
+#define HANDLE_CASE(TYPE, A)                                            \
+ hipLaunchKernelGGL(                                                    \
+  (kernelPointwiseApply1<Op,                                            \
+                        typename TensorUtils<TensorTypeA>::DataType,    \
+                        TYPE, A>),                                      \
+      grid, block, 0, THCState_getCurrentStream(state),                 \
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+          (aInfo),                                                      \
+      (TYPE) totalElements, op);
+#else
 #define HANDLE_CASE(TYPE, A)                                            \
   kernelPointwiseApply1<Op,                                             \
                         typename TensorUtils<TensorTypeA>::DataType,    \
@@ -242,7 +253,7 @@ bool THC_pointwiseApply1(THCState* state,
       OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
           (aInfo),                                                      \
       (TYPE) totalElements, op);
-
+#endif
 #define HANDLE_A_CASE(TYPE, A)                  \
   {                                             \
     if (aInfo.isContiguous()) {                 \
@@ -288,11 +299,20 @@ bool THC_pointwiseApply1(THCState* state,
     if (aInfo.isContiguous()) {
       OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -2>
         aOffset(aInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+    hipLaunchKernelGGL(
+      (kernelPointwiseApply1<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            uint64_t, -2>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply1<Op,
                             typename TensorUtils<TensorTypeA>::DataType,
                             uint64_t, -2>
         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
           aOffset, (uint64_t) totalElements, op);
+#endif
     } else {
 
 #if CUDA_VERSION < 9000
@@ -300,11 +320,20 @@ bool THC_pointwiseApply1(THCState* state,
 #endif
       OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -1>
         aOffset(aInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+     hipLaunchKernelGGL(
+      (kernelPointwiseApply1<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            uint64_t, -1>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply1<Op,
                             typename TensorUtils<TensorTypeA>::DataType,
                             uint64_t, -1>
         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
           aOffset, (uint64_t) totalElements, op);
+#endif
     }
   }
 #undef HANDLE_CASE
@@ -387,6 +416,20 @@ bool THC_pointwiseApply2(THCState* state,
   // (or vice versa), the contiguous tensor can be collapsed to one
   // dimension, and the loop to translate the linear index to the array
   // index can be similarly collapsed. That is what this unrolling is for.
+#if defined(__HIP_PLATFORM_HCC__)
+#define HANDLE_CASE(TYPE, A, B)                                         \
+  hipLaunchKernelGGL(                                                   \
+   (kernelPointwiseApply2<Op,                                           \
+                        typename TensorUtils<TensorTypeA>::DataType,    \
+                        typename TensorUtils<TensorTypeB>::DataType,    \
+                        TYPE, A, B>),                                   \
+      grid, block, 0, THCState_getCurrentStream(state),                 \
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+          (aInfo),                                                      \
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
+          (bInfo),                                                      \
+      (TYPE) totalElements, op);
+#else
 #define HANDLE_CASE(TYPE, A, B)                                         \
   kernelPointwiseApply2<Op,                                             \
                         typename TensorUtils<TensorTypeA>::DataType,    \
@@ -398,7 +441,7 @@ bool THC_pointwiseApply2(THCState* state,
       OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
           (bInfo),                                                      \
       (TYPE) totalElements, op);
-
+#endif
 #define HANDLE_B_CASE(TYPE, A, B)               \
   {                                             \
     if (bInfo.isContiguous()) {                 \
@@ -473,12 +516,22 @@ bool THC_pointwiseApply2(THCState* state,
         aOffset(aInfo);
       OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t, -2>
         bOffset(bInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL(
+      (kernelPointwiseApply2<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            uint64_t, -2, -2>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, bOffset, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply2<Op,
                             typename TensorUtils<TensorTypeA>::DataType,
                             typename TensorUtils<TensorTypeB>::DataType,
                             uint64_t, -2, -2>
         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
           aOffset, bOffset, (uint64_t) totalElements, op);
+#endif
     } else {
 #if CUDA_VERSION < 9000
       grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
@@ -487,12 +540,22 @@ bool THC_pointwiseApply2(THCState* state,
         aOffset(aInfo);
       OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t, -1>
         bOffset(bInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL(
+      (kernelPointwiseApply2<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            uint64_t, -1, -1>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, bOffset, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply2<Op,
                             typename TensorUtils<TensorTypeA>::DataType,
                             typename TensorUtils<TensorTypeB>::DataType,
                             uint64_t, -1, -1>
         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
           aOffset, bOffset, (uint64_t) totalElements, op);
+#endif
     }
   }
 #undef HANDLE_CASE
@@ -588,7 +651,23 @@ bool THC_pointwiseApply3(THCState* state,
     oldC = c;
     c = TensorUtils<TensorTypeC>::newContiguous(state, c);
   }
-
+#if defined(__HIP_PLATFORM_HCC__)
+#define HANDLE_CASE(TYPE, A, B, C)                                      \
+hipLaunchKernelGGL(                                                     \
+  (kernelPointwiseApply3<Op,                                            \
+                        typename TensorUtils<TensorTypeA>::DataType,    \
+                        typename TensorUtils<TensorTypeB>::DataType,    \
+                        typename TensorUtils<TensorTypeC>::DataType,    \
+                        TYPE, A, B, C>),                                \
+      grid, block, 0, THCState_getCurrentStream(state),                 \
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+          (aInfo),                                                      \
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
+          (bInfo),                                                      \
+      OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, TYPE, C>  \
+          (cInfo),                                                      \
+      (TYPE) totalElements, op);
+#else
 #define HANDLE_CASE(TYPE, A, B, C)                                      \
   kernelPointwiseApply3<Op,                                             \
                         typename TensorUtils<TensorTypeA>::DataType,    \
@@ -603,6 +682,7 @@ bool THC_pointwiseApply3(THCState* state,
       OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, TYPE, C>  \
           (cInfo),                                                      \
       (TYPE) totalElements, op);
+#endif
 
 #define HANDLE_C_CASE(TYPE, A, B, C)            \
   {                                             \
@@ -708,6 +788,16 @@ bool THC_pointwiseApply3(THCState* state,
         bOffset(bInfo);
       OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, uint64_t, -2>
         cOffset(cInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+hipLaunchKernelGGL(
+      (kernelPointwiseApply3<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            typename TensorUtils<TensorTypeC>::DataType,
+                            uint64_t, -2, -2, -2>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply3<Op,
                             typename TensorUtils<TensorTypeA>::DataType,
                             typename TensorUtils<TensorTypeB>::DataType,
@@ -715,6 +805,7 @@ bool THC_pointwiseApply3(THCState* state,
                             uint64_t, -2, -2, -2>
         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
           aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
+#endif
     } else {
 #if CUDA_VERSION < 9000
       grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
@@ -726,6 +817,16 @@ bool THC_pointwiseApply3(THCState* state,
         bOffset(bInfo);
       OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, uint64_t, -1>
         cOffset(cInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+hipLaunchKernelGGL(
+      (kernelPointwiseApply3<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            typename TensorUtils<TensorTypeC>::DataType,
+                            uint64_t, -1, -1, -1>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply3<Op,
                             typename TensorUtils<TensorTypeA>::DataType,
                             typename TensorUtils<TensorTypeB>::DataType,
@@ -733,6 +834,7 @@ bool THC_pointwiseApply3(THCState* state,
                             uint64_t, -1, -1, -1>
         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
           aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
+#endif
     }
   }
 #undef HANDLE_CASE
diff --git a/aten/src/THC/THCAsmUtils.cuh b/aten/src/THC/THCAsmUtils.cuh
index 3b1db9a8c..ffd1742d9 100644
--- a/aten/src/THC/THCAsmUtils.cuh
+++ b/aten/src/THC/THCAsmUtils.cuh
@@ -8,69 +8,92 @@ struct Bitfield {};
 
 template <>
 struct Bitfield<unsigned int> {
-  static __device__ __forceinline__
-  unsigned int getBitfield(unsigned int val, int pos, int len) {
-    unsigned int ret;
-    asm("bfe.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(val), "r"(pos), "r"(len));
-    return ret;
+  static __device__
+  inline
+  unsigned int getBitfield(unsigned int val, int pos, int len)
+  {
+    pos &= 0x1f;
+    len &= 0x1f;
+
+    unsigned int m = (1u << len) - 1u;
+    m <<= pos;
+    return val & m;
   }
 
-  static __device__ __forceinline__
-  unsigned int setBitfield(unsigned int val, unsigned int toInsert, int pos, int len) {
-    unsigned int ret;
-    asm("bfi.b32 %0, %1, %2, %3, %4;" :
-        "=r"(ret) : "r"(toInsert), "r"(val), "r"(pos), "r"(len));
-    return ret;
+ static  __device__
+  inline
+  unsigned int setBitfield(
+    unsigned int val, unsigned int toInsert, int pos, int len)
+  {
+    pos &= 0x1f;
+    len &= 0x1f;
+
+    unsigned int m = (1u << len) - 1u;
+    toInsert &= m;
+    toInsert <<= pos;
+    m <<= pos;
+
+    return (val & ~m) | toInsert;
   }
 };
 
-template <>
-struct Bitfield<uint64_t> {
-  static __device__ __forceinline__
-  uint64_t getBitfield(uint64_t val, int pos, int len) {
-    uint64_t ret;
-    asm("bfe.u64 %0, %1, %2, %3;" : "=l"(ret) : "l"(val), "r"(pos), "r"(len));
-    return ret;
+template<>
+struct Bitfield<uint64_t>{
+  static __device__
+  inline
+  uint64_t getBitfield(uint64_t val, int pos, int len)
+  {
+    pos &= 0x1f;
+    len &= 0x1f;
+
+    uint64_t m = (1u << len) - 1u;
+    m <<= pos;
+    return val & m;
   }
 
-  static __device__ __forceinline__
-  uint64_t setBitfield(uint64_t val, uint64_t toInsert, int pos, int len) {
-    uint64_t ret;
-    asm("bfi.b64 %0, %1, %2, %3, %4;" :
-        "=l"(ret) : "l"(toInsert), "l"(val), "r"(pos), "r"(len));
-    return ret;
+  static __device__
+  inline
+  uint64_t setBitfield(
+    uint64_t val, uint64_t toInsert, int pos, int len)
+  {
+    pos &= 0x1f;
+    len &= 0x1f;
+
+    uint64_t m = (1u << len) - 1u;
+    toInsert &= m;
+    toInsert <<= pos;
+    m <<= pos;
+
+    return (val & ~m) | toInsert;
   }
 };
 
-__device__ __forceinline__ int getLaneId() {
-  int laneId;
-  asm("mov.s32 %0, %laneid;" : "=r"(laneId) );
-  return laneId;
+__device__ __forceinline__ inline int getLaneId() {
+    return hc::__lane_id();
 }
 
-__device__ __forceinline__ unsigned getLaneMaskLt() {
-  unsigned mask;
-  asm("mov.u32 %0, %%lanemask_lt;" : "=r"(mask));
-  return mask;
+__device__ inline std::uint64_t getLaneMaskLt()
+{
+  std::uint64_t m = (1ull << getLaneId()) - 1ull;
+  return m;
 }
 
-__device__ __forceinline__ unsigned getLaneMaskLe() {
-  unsigned mask;
-  asm("mov.u32 %0, %%lanemask_le;" : "=r"(mask));
-  return mask;
+__device__ inline std::uint64_t getLaneMaskLe()
+{
+  std::uint64_t m = (1ull << (getLaneId() + 1ull)) - 1ull;
+  return m;
 }
 
-__device__ __forceinline__ unsigned getLaneMaskGt() {
-  unsigned mask;
-  asm("mov.u32 %0, %%lanemask_gt;" : "=r"(mask));
-  return mask;
+__device__ inline std::uint64_t getLaneMaskGt()
+{
+  std::uint64_t m = getLaneMaskLe();
+  return m ? ~m : m;
 }
 
-__device__ __forceinline__ unsigned getLaneMaskGe() {
-  unsigned mask;
-  asm("mov.u32 %0, %%lanemask_ge;" : "=r"(mask));
-  return mask;
+__device__ inline std::uint64_t getLaneMaskGe()
+{
+  std::uint64_t m = getLaneMaskLt();
+  return ~m;
 }
 
-
 #endif // THC_ASM_UTILS_INC
diff --git a/aten/src/THC/THCAtomics.cuh b/aten/src/THC/THCAtomics.cuh
index 9e54c56dc..f0283b996 100644
--- a/aten/src/THC/THCAtomics.cuh
+++ b/aten/src/THC/THCAtomics.cuh
@@ -103,7 +103,11 @@ static inline  __device__ void atomicAdd(half *address, half val) {
 
   do {
     assumed = old;
-#if CUDA_VERSION < 9000
+#if defined(__HIP_PLATFORM_HCC__)
+    half hsum;
+    hsum = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
+    hsum = THCNumerics<half>::add(hsum, val);
+#elif CUDA_VERSION < 9000
     half hsum;
     hsum.x = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
     hsum = THCNumerics<half>::add(hsum, val);
@@ -113,7 +117,11 @@ static inline  __device__ void atomicAdd(half *address, half val) {
     half tmpres = THCNumerics<half>::add(hsum, val);
     hsum = __half_raw(tmpres);
 #endif
+#if defined(__HIP_PLATFORM_HCC__)
+    // old = (size_t)address & 2 ? (old & 0xffff) | (hsum << 16) : (old & 0xffff0000) | hsum;
+#else
     old = (size_t)address & 2 ? (old & 0xffff) | (hsum.x << 16) : (old & 0xffff0000) | hsum.x;
+#endif
     old = atomicCAS(address_as_ui, assumed, old);
   } while (assumed != old);
 }
diff --git a/aten/src/THC/THCBlas.cu b/aten/src/THC/THCBlas.cu
index e2003da57..8abb1639b 100644
--- a/aten/src/THC/THCBlas.cu
+++ b/aten/src/THC/THCBlas.cu
@@ -2,7 +2,8 @@
 #include "THCGeneral.h"
 #include "THCHalf.h"
 
-float THCudaBlas_Sdot(THCState *state, int64_t n, float *x, int64_t incx, float *y, int64_t incy)
+
+float THCudaBlas_Sdot(THCState *state, long n, float *x, long incx, float *y, long incy)
 {
   if (n == 1) {
     incx = 1;
@@ -14,9 +15,9 @@ float THCudaBlas_Sdot(THCState *state, int64_t n, float *x, int64_t incx, float
     int i_incx = (int)incx;
     int i_incy = (int)incy;
     float result;
-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-    cublasSetStream(handle, THCState_getCurrentStream(state));
-    THCublasCheck(cublasSdot(handle, i_n, x, i_incx, y, i_incy, &result));
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasSdot(handle, i_n, x, i_incx, y, i_incy, &result));
     return result;
   }
 
@@ -25,7 +26,7 @@ float THCudaBlas_Sdot(THCState *state, int64_t n, float *x, int64_t incx, float
   return 0;
 }
 
-double THCudaBlas_Ddot(THCState *state, int64_t n, double *x, int64_t incx, double *y, int64_t incy)
+double THCudaBlas_Ddot(THCState *state, long n, double *x, long incx, double *y, long incy)
 {
   if (n == 1) {
     incx = 1;
@@ -37,9 +38,9 @@ double THCudaBlas_Ddot(THCState *state, int64_t n, double *x, int64_t incx, doub
     int i_incx = (int)incx;
     int i_incy = (int)incy;
     double result;
-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-    cublasSetStream(handle, THCState_getCurrentStream(state));
-    THCublasCheck(cublasDdot(handle, i_n, x, i_incx, y, i_incy, &result));
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasDdot(handle, i_n, x, i_incx, y, i_incy, &result));
     return result;
   }
 
@@ -49,7 +50,7 @@ double THCudaBlas_Ddot(THCState *state, int64_t n, double *x, int64_t incx, doub
 }
 
 #ifdef CUDA_HALF_TENSOR
-half THCudaBlas_Hdot(THCState *state, int64_t n, half *x, int64_t incx, half *y, int64_t incy)
+half THCudaBlas_Hdot(THCState *state, long n, half *x, long incx, half *y, long incy)
 {
 #if CUDA_VERSION >= 8000
   if (n == 1) {
@@ -59,15 +60,19 @@ half THCudaBlas_Hdot(THCState *state, int64_t n, half *x, int64_t incx, half *y,
 
   if ((n <= INT_MAX) && (incx <= INT_MAX) && (incy <= INT_MAX)) {
     half result;
+#ifdef HIPBLAS_TODO
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasDotEx(handle, n, x, CUDA_R_16F, incx, y, CUDA_R_16F, incy, &result, CUDA_R_16F, CUDA_R_32F));
+#else
+#ifdef __NVCC__
     cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-    cublasSetStream(handle, THCState_getCurrentStream(state));
-    THCublasCheck(cublasDotEx(handle, n,
-                              x, CUDA_R_16F, incx,
-                              y, CUDA_R_16F, incy,
-                              &result, CUDA_R_16F,
-                              CUDA_R_32F));
+    cublasSetStream((cublasHandle_t)handle, THCState_getCurrentStream(state));
+    cublasDotEx(handle, n, x, CUDA_R_16F, incx, y, CUDA_R_16F, incy, &result, CUDA_R_16F, CUDA_R_32F);
+#endif
+#endif
     return result;
-  }
+   }
 
   THError("Cublas_Hdot only supports n, incx and incy "
           "up to signed integer limits: %d", INT_MAX);
@@ -80,15 +85,15 @@ half THCudaBlas_Hdot(THCState *state, int64_t n, half *x, int64_t incx, half *y,
 #endif
 
 /* Level 2 */
-void THCudaBlas_Sgemv(THCState *state, char trans, int64_t m, int64_t n, float alpha, float *a, int64_t lda, float *x, int64_t incx, float beta, float *y, int64_t incy)
+void THCudaBlas_Sgemv(THCState *state, char trans, long m, long n, float alpha, float *a, long lda, float *x, long incx, float beta, float *y, long incy)
 {
   if(n == 1)
     lda = m;
 
-  cublasOperation_t op;
-  if (trans == 't') op = CUBLAS_OP_T;
-  else if (trans == 'n') op = CUBLAS_OP_N;
-  else if (trans == 'c') op = CUBLAS_OP_C;
+  hipblasOperation_t op;
+  if (trans == 't') op = HIPBLAS_OP_T;
+  else if (trans == 'n') op = HIPBLAS_OP_N;
+  else if (trans == 'c') op = HIPBLAS_OP_C;
   else THError("Cublas_Sgemv parameter trans should be 't', 'n' or 'c'.");
 
   if( (m <= INT_MAX) && (n <= INT_MAX) &&
@@ -102,24 +107,25 @@ void THCudaBlas_Sgemv(THCState *state, char trans, int64_t m, int64_t n, float a
     int i_incx = (int)incx;
     int i_incy = (int)incy;
 
-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-    cublasSetStream(handle, THCState_getCurrentStream(state));
-    THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
+
     return;
   }
   THError("Cublas_Sgemv only supports m, n, lda, incx, incy"
           "in the range 0 < [val] <= %d", INT_MAX);
 }
 
-void THCudaBlas_Dgemv(THCState *state, char trans, int64_t m, int64_t n, double alpha, double *a, int64_t lda, double *x, int64_t incx, double beta, double *y, int64_t incy)
+void THCudaBlas_Dgemv(THCState *state, char trans, long m, long n, double alpha, double *a, long lda, double *x, long incx, double beta, double *y, long incy)
 {
   if(n == 1)
     lda = m;
 
-  cublasOperation_t op;
-  if (trans == 't') op = CUBLAS_OP_T;
-  else if (trans == 'n') op = CUBLAS_OP_N;
-  else if (trans == 'c') op = CUBLAS_OP_C;
+  hipblasOperation_t op;
+  if (trans == 't') op = HIPBLAS_OP_T;
+  else if (trans == 'n') op = HIPBLAS_OP_N;
+  else if (trans == 'c') op = HIPBLAS_OP_C;
   else THError("Cublas_Sgemv parameter trans should be 't', 'n' or 'c'.");
 
   if( (m <= INT_MAX) && (n <= INT_MAX) &&
@@ -133,16 +139,16 @@ void THCudaBlas_Dgemv(THCState *state, char trans, int64_t m, int64_t n, double
     int i_incx = (int)incx;
     int i_incy = (int)incy;
 
-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-    cublasSetStream(handle, THCState_getCurrentStream(state));
-    THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
     return;
   }
   THError("Cublas_Dgemv only supports m, n, lda, incx, incy"
           "in the range 0 < [val] <= %d", INT_MAX);
 }
 
-void THCudaBlas_Sger(THCState *state, int64_t m, int64_t n, float alpha, float *x, int64_t incx, float *y, int64_t incy, float *a, int64_t lda)
+void THCudaBlas_Sger(THCState *state, long m, long n, float alpha, float *x, long incx, float *y, long incy, float *a, long lda)
 {
   if(n == 1)
     lda = m;
@@ -154,17 +160,16 @@ void THCudaBlas_Sger(THCState *state, int64_t m, int64_t n, float alpha, float *
       int i_lda = (int)lda;
       int i_incx = (int)incx;
       int i_incy = (int)incy;
-
-      cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-      cublasSetStream(handle, THCState_getCurrentStream(state));
-      THCublasCheck(cublasSger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
+      hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+      hipblasSetStream(handle, THCState_getCurrentStream(state));
+      THCublasCheck(hipblasSger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
       return;
     }
   THError("Cublas_Sger only supports m, n, lda, incx, incy"
           "with the bound [val] <= %d", INT_MAX);
 }
 
-void THCudaBlas_Dger(THCState *state, int64_t m, int64_t n, double alpha, double *x, int64_t incx, double *y, int64_t incy, double *a, int64_t lda)
+void THCudaBlas_Dger(THCState *state, long m, long n, double alpha, double *x, long incx, double *y, long incy, double *a, long lda)
 {
   if(n == 1)
     lda = m;
@@ -176,17 +181,33 @@ void THCudaBlas_Dger(THCState *state, int64_t m, int64_t n, double alpha, double
       int i_lda = (int)lda;
       int i_incx = (int)incx;
       int i_incy = (int)incy;
-
-      cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-      cublasSetStream(handle, THCState_getCurrentStream(state));
-      THCublasCheck(cublasDger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
+#ifdef HIPBLAS_TODO
+      hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+      hipblasSetStream(handle, THCState_getCurrentStream(state));
+      THCublasCheck(hipblasDger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
+#else
+#ifdef __NVCC__
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream((hipblasHandle_t)handle, THCState_getCurrentStream(state));
+    hipblasDger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda);
+#endif
+#endif
       return;
     }
   THError("Cublas_Dger only supports m, n, lda, incx, incy"
           "with the bound [val] <= %d", INT_MAX);
 }
 
-
+hipblasOperation_t convertTransToHipblasOperation(char trans) {
+  if (trans == 't') return HIPBLAS_OP_T;
+  else if (trans == 'n') return HIPBLAS_OP_N;
+  else if (trans == 'c') return HIPBLAS_OP_C;
+  else {
+    THError("trans must be one of: t, n, c");
+    return HIPBLAS_OP_T;
+  }
+}
+#ifdef __NVCC__
 cublasOperation_t convertTransToCublasOperation(char trans) {
   if (trans == 't') return CUBLAS_OP_T;
   else if (trans == 'n') return CUBLAS_OP_N;
@@ -196,8 +217,8 @@ cublasOperation_t convertTransToCublasOperation(char trans) {
     return CUBLAS_OP_T;
   }
 }
-
-void adjustLd(char transa, char transb, int64_t m, int64_t n, int64_t k, int64_t *lda, int64_t *ldb, int64_t *ldc)
+#endif
+void adjustLd(char transa, char transb, long m, long n, long k, long *lda, long *ldb, long *ldc)
 {
   int transa_ = ((transa == 't') || (transa == 'T'));
   int transb_ = ((transb == 't') || (transb == 'T'));
@@ -229,11 +250,11 @@ void adjustLd(char transa, char transb, int64_t m, int64_t n, int64_t k, int64_t
 }
 
 /* Level 3 */
-void THCudaBlas_Sgemm(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k, float alpha, float *a, int64_t lda, float *b, int64_t ldb, float beta, float *c, int64_t ldc)
+void THCudaBlas_Sgemm(THCState *state, char transa, char transb, long m, long n, long k, float alpha, float *a, long lda, float *b, long ldb, float beta, float *c, long ldc)
 {
   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
-  cublasOperation_t opa = convertTransToCublasOperation(transa);
-  cublasOperation_t opb = convertTransToCublasOperation(transb);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
 
   if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) && (lda <= INT_MAX)  && (ldb <= INT_MAX) && (ldc <= INT_MAX) )
   {
@@ -244,9 +265,9 @@ void THCudaBlas_Sgemm(THCState *state, char transa, char transb, int64_t m, int6
     int i_ldb = (int)ldb;
     int i_ldc = (int)ldc;
 
-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-    cublasSetStream(handle, THCState_getCurrentStream(state));
-    THCublasCheck(cublasSgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasSgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
     return;
   }
   THError("Cublas_Sgemm only supports m, n, k, lda, ldb, ldc"
@@ -259,62 +280,58 @@ void THCudaBlas_Sgemm(THCState *state, char transa, char transb, int64_t m, int6
 #  define CUDA_R_16F CUBLAS_DATA_HALF
 #endif
 
-void THCudaBlas_Hgemm(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k, half alpha, half *a, int64_t lda, half *b, int64_t ldb, half beta, half *c, int64_t ldc)
+void THCudaBlas_Hgemm(THCState *state, char transa, char transb, long m, long n, long k, half alpha, half *a, long lda, half *b, long ldb, half beta, half *c, long ldc)
 {
   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
-  cublasOperation_t opa = convertTransToCublasOperation(transa);
-  cublasOperation_t opb = convertTransToCublasOperation(transb);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
 
   if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) && (lda <= INT_MAX)  && (ldb <= INT_MAX) && (ldc <= INT_MAX) )
-    {
-      int i_m = (int)m;
-      int i_n = (int)n;
-      int i_k = (int)k;
-      int i_lda = (int)lda;
-      int i_ldb = (int)ldb;
-      int i_ldc = (int)ldc;
+  {
+    int i_m = (int)m;
+    int i_n = (int)n;
+    int i_k = (int)k;
+    int i_lda = (int)lda;
+    int i_ldb = (int)ldb;
+    int i_ldc = (int)ldc;
 
-      cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-      cublasSetStream(handle, THCState_getCurrentStream(state));
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
 
+    // Check for native Hgemm support
+#ifdef __NVCC__
+    if (THC_fastHalfInstructions(state)) {
+      THCublasCheck(hipblasHgemm(handle, opa, opb,
+				i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb,
+				&beta, c, i_ldc));
+    } else {
       // Simulated Hgemm
       float fAlpha = THC_half2float(alpha);
       float fBeta = THC_half2float(beta);
 
-#if CUDA_VERSION < 9000
-      THCublasCheck(cublasSgemmEx(handle, opa, opb,
-                                  i_m, i_n, i_k, &fAlpha,
+      /*THCublasCheck(hipblasSgemmEx(handle, opa, opb,
+				  i_m, i_n, i_k, &fAlpha,
                                   a, CUDA_R_16F, i_lda, b, CUDA_R_16F,
-                                  i_ldb, &fBeta, c, CUDA_R_16F, i_ldc));
-#else
-      cudaDeviceProp* prop = THCState_getCurrentDeviceProperties(state);
-      if (prop->major >= 5){
-        THCublasCheck(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));
-        THCublasCheck(cublasGemmEx(handle, opa, opb,
-                                   i_m, i_n, i_k, &fAlpha,
-                                   a, CUDA_R_16F, i_lda, b, CUDA_R_16F,
-                                   i_ldb, &fBeta, c, CUDA_R_16F, i_ldc,
-                                   CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP));
-        THCublasCheck(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));
-      }else{
-        THCublasCheck(cublasSgemmEx(handle, opa, opb,
-                                    i_m, i_n, i_k, &fAlpha,
-                                    a, CUDA_R_16F, i_lda, b, CUDA_R_16F,
-                                    i_ldb, &fBeta, c, CUDA_R_16F, i_ldc));
-      }
-#endif
-      return;
+				  i_ldb, &fBeta, c, CUDA_R_16F, i_ldc));*/
     }
+#elif __HCC__
+//       hipblasHgemm(handle, opa, opb,
+// 				i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb,
+// 				&beta, c, i_ldc);
+#endif
+    return;
+  }
   THError("Cublas_Hgemm only supports m, n, k, lda, ldb, ldc"
           "with th bound [val] <= %d", INT_MAX);
+
 }
 #endif
 
-void THCudaBlas_Dgemm(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k, double alpha, double *a, int64_t lda, double *b, int64_t ldb, double beta, double *c, int64_t ldc)
+void THCudaBlas_Dgemm(THCState *state, char transa, char transb, long m, long n, long k, double alpha, double *a, long lda, double *b, long ldb, double beta, double *c, long ldc)
 {
   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
-  cublasOperation_t opa = convertTransToCublasOperation(transa);
-  cublasOperation_t opb = convertTransToCublasOperation(transb);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
 
   if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) && (lda <= INT_MAX)  && (ldb <= INT_MAX) && (ldc <= INT_MAX) )
   {
@@ -325,19 +342,20 @@ void THCudaBlas_Dgemm(THCState *state, char transa, char transb, int64_t m, int6
     int i_ldb = (int)ldb;
     int i_ldc = (int)ldc;
 
-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-    cublasSetStream(handle, THCState_getCurrentStream(state));
-    THCublasCheck(cublasDgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasDgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
     return;
   }
   THError("Cublas_Dgemm only supports m, n, k, lda, ldb, ldc"
           "with the bound [val] <= %d", INT_MAX);
 }
 
+#ifdef __NVCC__
 #if CUDA_VERSION >= 9010
-void THCudaBlas_HgemmStridedBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
-                             half alpha, const half *a, int64_t lda, int64_t strideA, const half *b, int64_t ldb, int64_t strideB,
-                             half beta, half *c, int64_t ldc, int64_t strideC, int64_t batchCount)
+void THCudaBlas_HgemmStridedBatched(THCState *state, char transa, char transb, long m, long n, long k,
+                             half alpha, const half *a, long lda, long strideA, const half *b, long ldb, long strideB,
+                             half beta, half *c, long ldc, long strideC, long batchCount)
 {
   if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
 
@@ -364,10 +382,11 @@ void THCudaBlas_HgemmStridedBatched(THCState *state, char transa, char transb, i
   THCublasCheck(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));
 }
 #endif
+#endif
 
-void THCudaBlas_SgemmBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
-                             float alpha, const float *a[], int64_t lda, const float *b[], int64_t ldb,
-                             float beta, float *c[], int64_t ldc, int64_t batchCount)
+void THCudaBlas_SgemmBatched(THCState *state, char transa, char transb, long m, long n, long k,
+                             float alpha, const float *a[], long lda, const float *b[], long ldb,
+                             float beta, float *c[], long ldc, long batchCount)
 {
   if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
   {
@@ -376,45 +395,21 @@ void THCudaBlas_SgemmBatched(THCState *state, char transa, char transb, int64_t
   }
 
   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
-  cublasOperation_t opa = convertTransToCublasOperation(transa);
-  cublasOperation_t opb = convertTransToCublasOperation(transb);
-
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasSgemmBatched(handle,
-                                   opa, opb, (int)m, (int)n, (int)k,
-                                   &alpha, a, (int)lda, b, (int)ldb, &beta, c, (int)ldc,
-                                   (int)batchCount));
-}
-
-#if CUDA_VERSION >= 8000
-void THCudaBlas_SgemmStridedBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
-                             float alpha, const float *a, int64_t lda, int64_t strideA, const float *b, int64_t ldb, int64_t strideB,
-                             float beta, float *c, int64_t ldc, int64_t strideC, int64_t batchCount)
-{
-  if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
-
-  {
-    THError("Cublas_SgemmStridedBatched only supports m, n, k, lda, ldb, ldc, batchCount"
-            "with the bound [val] <= %d", INT_MAX);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  for (long i = 0; i < batchCount; i++) {
+    THCublasCheck(hipblasSgemm(handle,
+                              opa, opb, (int)m, (int)n, (int)k,
+                              &alpha, a[i], (int)lda, b[i], (int)ldb, &beta, c[i], (int)ldc));
   }
-
-  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
-  cublasOperation_t opa = convertTransToCublasOperation(transa);
-  cublasOperation_t opb = convertTransToCublasOperation(transb);
-
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasSgemmStridedBatched(handle,
-                                   opa, opb, (int)m, (int)n, (int)k,
-                                   &alpha, a, (int)lda, strideA, b, (int)ldb, strideB, &beta, c, (int)ldc, strideC,
-                                   (int)batchCount));
 }
-#endif
 
-void THCudaBlas_DgemmBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
-                             double alpha, const double *a[], int64_t lda, const double *b[], int64_t ldb,
-                             double beta, double *c[], int64_t ldc, int64_t batchCount)
+void THCudaBlas_DgemmBatched(THCState *state, char transa, char transb, long m, long n, long k,
+                             double alpha, const double *a[], long lda, const double *b[], long ldb,
+                             double beta, double *c[], long ldc, long batchCount)
 {
   if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
   {
@@ -423,40 +418,17 @@ void THCudaBlas_DgemmBatched(THCState *state, char transa, char transb, int64_t
   }
 
   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
-  cublasOperation_t opa = convertTransToCublasOperation(transa);
-  cublasOperation_t opb = convertTransToCublasOperation(transb);
-
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasDgemmBatched(handle,
-                                   opa, opb, (int)m, (int)n, (int)k,
-                                   &alpha, a, (int)lda, b, (int)ldb, &beta, c, (int)ldc,
-                                   (int)batchCount));
-}
-
-#if CUDA_VERSION >= 8000
-void THCudaBlas_DgemmStridedBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
-                             double alpha, const double *a, int64_t lda, int64_t strideA, const double *b, int64_t ldb, int64_t strideB,
-                             double beta, double *c, int64_t ldc, int64_t strideC, int64_t batchCount)
-{
-  if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
-  {
-    THError("Cublas_DgemmBatched only supports m, n, k, lda, ldb, ldc, batchCount"
-            "with the bound [val] <= %d", INT_MAX);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  for (long i = 0; i < batchCount; i++) {
+    THCublasCheck(hipblasDgemm(handle,
+                              opa, opb, (int)m, (int)n, (int)k,
+                              &alpha, a[i], (int)lda, b[i], (int)ldb, &beta, c[i], (int)ldc));
   }
-
-  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
-  cublasOperation_t opa = convertTransToCublasOperation(transa);
-  cublasOperation_t opb = convertTransToCublasOperation(transb);
-
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasDgemmStridedBatched(handle,
-                                   opa, opb, (int)m, (int)n, (int)k,
-                                   &alpha, a, (int)lda, strideA, b, (int)ldb, strideB, &beta, c, (int)ldc, strideC,
-                                   (int)batchCount));
 }
-#endif
 
 /* Inverse */
 void THCudaBlas_Sgetrf(THCState *state, int n, float **a, int lda, int *pivot, int *info, int batchSize) {
@@ -465,9 +437,11 @@ void THCudaBlas_Sgetrf(THCState *state, int n, float **a, int lda, int *pivot, i
     THError("Cublas_Sgetrf only supports n, lda, batchSize"
             "with the bound [val] <= %d", INT_MAX);
   }
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasSgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
+#ifdef HIPBLAS_TODO
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasSgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
+#endif
 }
 
 void THCudaBlas_Dgetrf(THCState *state, int n, double **a, int lda, int *pivot, int *info, int batchSize) {
@@ -476,9 +450,11 @@ void THCudaBlas_Dgetrf(THCState *state, int n, double **a, int lda, int *pivot,
     THError("Cublas_Dgetrf only supports n, lda, batchSize"
             "with the bound [val] <= %d", INT_MAX);
   }
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasDgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
+#ifdef HIPBLAS_TODO
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasDgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
+#endif
 }
 
 THC_API void THCudaBlas_Sgetrs(THCState *state, char transa, int n, int nrhs, const float **a, int lda, int *pivot, float **b, int ldb, int *info, int batchSize)
@@ -489,12 +465,13 @@ THC_API void THCudaBlas_Sgetrs(THCState *state, char transa, int n, int nrhs, co
             "with the bound [val] <= %d", INT_MAX);
   }
 
-  // no need to adjust leading dimensions, since matrices are square
-  cublasOperation_t opa = convertTransToCublasOperation(transa);
 
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasSgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
+#ifdef HIPBLAS_TODO
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasSgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
+#endif
 }
 
 
@@ -506,14 +483,14 @@ THC_API void THCudaBlas_Dgetrs(THCState *state, char transa, int n, int nrhs, co
             "with the bound [val] <= %d", INT_MAX);
   }
 
-  // no need to adjust leading dimensions, since matrices are square
-  cublasOperation_t opa = convertTransToCublasOperation(transa);
 
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasDgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
+#ifdef HIPBLAS_TODO
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasDgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
+#endif
 }
-
 void THCudaBlas_Sgetri(THCState *state, int n, const float **a, int lda, int *pivot, float **c, int ldc, int *info, int batchSize) {
 
   if( (n >= INT_MAX) || (lda >= INT_MAX)|| (ldc >= INT_MAX) || (batchSize >= INT_MAX) )
@@ -521,9 +498,11 @@ void THCudaBlas_Sgetri(THCState *state, int n, const float **a, int lda, int *pi
     THError("Cublas_Sgetri only supports n, lda, ldc, batchSize"
             "with the bound [val] <= %d", INT_MAX);
   }
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasSgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
+#ifdef HIPBLAS_TODO
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasSgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
+#endif
 }
 
 void THCudaBlas_Dgetri(THCState *state, int n, const double **a, int lda, int *pivot, double **c, int ldc, int *info, int batchSize) {
@@ -533,7 +512,9 @@ void THCudaBlas_Dgetri(THCState *state, int n, const double **a, int lda, int *p
     THError("Cublas_Dgetri only supports n, lda, ldc, batchSize"
             "with the bound [val] <= %d", INT_MAX);
   }
-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
-  cublasSetStream(handle, THCState_getCurrentStream(state));
-  THCublasCheck(cublasDgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
+#ifdef HIPBLAS_TODO
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasDgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
+#endif
 }
diff --git a/aten/src/THC/THCDeviceTensor-inl.cuh b/aten/src/THC/THCDeviceTensor-inl.cuh
index 22ca6c973..86907c637 100644
--- a/aten/src/THC/THCDeviceTensor-inl.cuh
+++ b/aten/src/THC/THCDeviceTensor-inl.cuh
@@ -182,7 +182,8 @@ template <typename T, int Dim,
 __host__ __device__ THCDeviceTensor<T, Dim, IndexT, PtrTraits>
 THCDeviceTensor<T, Dim, IndexT, PtrTraits>::transpose(int dim1,
                                                       int dim2) const {
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   // Device code
   assert(dim1 >= 0 && dim1 < Dim);
   assert(dim1 >= 0 && dim2 < Dim);
@@ -285,7 +286,8 @@ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::downcastOuter() {
   // in all of the dimensions we are collapsing (no padding in
   // them).
   bool cont = isContiguousRange(0, Dim - NewDim);
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   // Device code
   assert(cont);
 #else
@@ -336,7 +338,8 @@ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::downcastInner() {
   // in all of the dimensions we are collapsing (no padding in
   // them).
   bool cont = isContiguousRange(NewDim, Dim);
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   // Device code
   assert(cont);
 #else
@@ -404,7 +407,8 @@ template <typename T, int Dim,
           typename IndexT, template <typename U> class PtrTraits>
 void
 THCDeviceTensor<T, Dim, IndexT, PtrTraits>::zero(cudaStream_t stream) {
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   assert(isContiguous());
 #else
   if (!isContiguous()) {
diff --git a/aten/src/THC/THCDeviceUtils.cuh b/aten/src/THC/THCDeviceUtils.cuh
index 4ae2bee07..bfb5156ac 100644
--- a/aten/src/THC/THCDeviceUtils.cuh
+++ b/aten/src/THC/THCDeviceUtils.cuh
@@ -25,7 +25,7 @@ __host__ __device__ __forceinline__ T THCRoundUp(T a, T b) {
  * For CC 3.5+, perform a load using __ldg
  */
 template <typename T>
-__device__ __forceinline__ T doLdg(const T* p) {
+__device__ __forceinline__ inline T doLdg(const T* p) {
 #if __CUDA_ARCH__ >= 350
   return __ldg(p);
 #else
@@ -33,7 +33,7 @@ __device__ __forceinline__ T doLdg(const T* p) {
 #endif
 }
 
-__device__ __forceinline__ unsigned int ACTIVE_MASK()
+__device__ __forceinline__ inline unsigned int ACTIVE_MASK()
 {
 #if CUDA_VERSION >= 9000
     return __activemask();
@@ -43,7 +43,7 @@ __device__ __forceinline__ unsigned int ACTIVE_MASK()
 #endif
 }
 
-__device__ __forceinline__ int WARP_BALLOT(int predicate, unsigned int mask = 0xffffffff)
+__device__ __forceinline__ inline int WARP_BALLOT(int predicate, unsigned int mask = 0xffffffff)
 {
 #if CUDA_VERSION >= 9000
     return __ballot_sync(mask, predicate);
@@ -52,8 +52,13 @@ __device__ __forceinline__ int WARP_BALLOT(int predicate, unsigned int mask = 0x
 #endif
 }
 
+//To handle ambiguity, add a type double version.
+__device__ __forceinline__ inline double WARP_SHFL_XOR(double value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff) {
+  //(HIP doesn't support double)
+  return (double) __shfl_xor((float) value, laneMask, width);
+}
 template <typename T>
-__device__ __forceinline__ T WARP_SHFL_XOR(T value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff)
+__device__ __forceinline__ inline T WARP_SHFL_XOR(T value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff)
 {
 #if CUDA_VERSION >= 9000
     return __shfl_xor_sync(mask, value, laneMask, width);
@@ -63,7 +68,7 @@ __device__ __forceinline__ T WARP_SHFL_XOR(T value, int laneMask, int width = wa
 }
 
 template <typename T>
-__device__ __forceinline__ T WARP_SHFL(T value, int srcLane, int width = warpSize, unsigned int mask = 0xffffffff)
+__device__ __forceinline__ inline T WARP_SHFL(T value, int srcLane, int width = warpSize, unsigned int mask = 0xffffffff)
 {
 #if CUDA_VERSION >= 9000
     return __shfl_sync(mask, value, srcLane, width);
@@ -73,7 +78,7 @@ __device__ __forceinline__ T WARP_SHFL(T value, int srcLane, int width = warpSiz
 }
 
 template <typename T>
-__device__ __forceinline__ T WARP_SHFL_UP(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+__device__ __forceinline__ inline T WARP_SHFL_UP(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
 {
 #if CUDA_VERSION >= 9000
     return __shfl_up_sync(mask, value, delta, width);
@@ -82,8 +87,14 @@ __device__ __forceinline__ T WARP_SHFL_UP(T value, unsigned int delta, int width
 #endif
 }
 
+//To handle ambiguity, add a type double version.
+__device__ __forceinline__ inline double WARP_SHFL_DOWN(double value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+{
+  //(HIP doesn't support double)
+  return (double) __shfl_down((float) value, delta, width);
+}
 template <typename T>
-__device__ __forceinline__ T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+__device__ __forceinline__ inline T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
 {
 #if CUDA_VERSION >= 9000
     return __shfl_down_sync(mask, value, delta, width);
diff --git a/aten/src/THC/THCGeneral.cpp b/aten/src/THC/THCGeneral.cpp
index 443c7bdef..5e59c05a1 100644
--- a/aten/src/THC/THCGeneral.cpp
+++ b/aten/src/THC/THCGeneral.cpp
@@ -758,11 +758,11 @@ void __THCublasCheck(cublasStatus_t status, const char *file, const int line)
       case CUBLAS_STATUS_INVALID_VALUE:
         errmsg = "an invalid numeric value was used as an argument";
         break;
-
+#ifdef CUDA
       case CUBLAS_STATUS_ARCH_MISMATCH:
         errmsg = "an absent device architectural feature is required";
         break;
-
+#endif
       case CUBLAS_STATUS_MAPPING_ERROR:
         errmsg = "an access to GPU memory space failed";
         break;
@@ -803,11 +803,9 @@ void __THCusparseCheck(cusparseStatus_t status, const char *file, const int line
       case CUSPARSE_STATUS_INVALID_VALUE:
         errmsg = "an invalid numeric value was used as an argument";
         break;
-
       case CUSPARSE_STATUS_ARCH_MISMATCH:
         errmsg = "an absent device architectural feature is required";
         break;
-
       case CUSPARSE_STATUS_MAPPING_ERROR:
         errmsg = "an access to GPU memory space failed";
         break;
@@ -923,7 +921,10 @@ cudaError_t THCudaMemGetInfoCached(THCState *state,  size_t* freeBytes, size_t*
 
 half THC_float2half(float f)
 {
-#if CUDA_VERSION < 9000
+#if defined(__HIP_PLATFORM_HCC__)
+  half h;
+  return h;
+#elif CUDA_VERSION < 9000
   half h;
   TH_float2halfbits(&f, &h.x);
   return h;
@@ -936,12 +937,13 @@ half THC_float2half(float f)
 
 float  THC_half2float(half h)
 {
+#if defined(__HIP_PLATFORM_HCC__)
   float f;
-#if CUDA_VERSION < 9000
+  return f;
+#elif CUDA_VERSION < 9000
   TH_halfbits2float(&h.x, &f);
 #else
   __half_raw h_raw(h);
   TH_halfbits2float(&h_raw.x, &f);
 #endif
-  return f;
 }
diff --git a/aten/src/THC/THCHalf.h b/aten/src/THC/THCHalf.h
index bb21b9d25..ceece3150 100644
--- a/aten/src/THC/THCHalf.h
+++ b/aten/src/THC/THCHalf.h
@@ -4,19 +4,24 @@
 #include "THCGeneral.h"
 
 /* We compile with CudaHalfTensor support if we have this: */
-#if CUDA_VERSION >= 7050 || CUDA_HAS_FP16
+#if CUDA_VERSION >= 7050 || CUDA_HAS_FP16 || defined (__HIP_PLATFORM_HCC__)
 #define CUDA_HALF_TENSOR 1
 #endif
 
 #ifdef CUDA_HALF_TENSOR
 
-#include <cuda_fp16.h>
-#include <stdint.h>
-
-#if CUDA_VERSION >= 9000
-#ifndef __cplusplus
-typedef __half_raw half;
-#endif
+#if defined (__HIP_PLATFORM_HCC__)
+  #include <cstdint>
+  #include <hip/hip_fp16.h>
+#else
+  #include <cuda_fp16.h>
+  #include <stdint.h>
+  
+  #if CUDA_VERSION >= 9000
+    #ifndef __cplusplus
+      typedef __half_raw half;
+    #endif
+  #endif
 #endif
 
 THC_EXTERNC void THCFloat2Half(THCState *state, half *out, float *in, ptrdiff_t len);
diff --git a/aten/src/THC/THCNumerics.cuh b/aten/src/THC/THCNumerics.cuh
index cbc2743d9..977533ab7 100644
--- a/aten/src/THC/THCNumerics.cuh
+++ b/aten/src/THC/THCNumerics.cuh
@@ -1,11 +1,14 @@
 #ifndef THC_NUMERICS_INC
 #define THC_NUMERICS_INC
 
-#include <cuda.h>
 #include <limits.h>
-#include <assert.h>
+#include "hip/hip_runtime.h"
+
 #include "THCHalf.h"
 
+#include <climits>
+
+
 /// Class for numeric limits of the particular data type, which
 /// includes support for `half`.
 /// Unfortunately since `half` does not have a constructor, these have
@@ -16,7 +19,7 @@ struct THCNumerics {
 
 template <typename scalar_t>
 static inline __host__ __device__ scalar_t powi(scalar_t a, scalar_t b) {
-  assert(THCNumerics<scalar_t>::ge(b, 0));
+  //assert(THCNumerics<scalar_t>::ge(b, 0));
   scalar_t result = 1;
   while (b) {
     if (b & 1) {
@@ -35,8 +38,8 @@ static inline __host__ __device__ bool has_different_sign(scalar_t a, scalar_t b
 
 template <>
 struct THCNumerics<uint8_t> {
-  static inline __host__ __device__ uint8_t min() { return 0; }
-  static inline __host__ __device__ uint8_t max() { return UCHAR_MAX; }
+  static inline __host__ __device__ uint8_t (min)() { return 0; }
+  static inline __host__ __device__ uint8_t (max)() { return UCHAR_MAX; }
 
   static inline __host__ __device__ bool lt(uint8_t a, uint8_t b) { return a < b; }
   static inline __host__ __device__ bool le(uint8_t a, uint8_t b) { return a <= b; }
@@ -56,8 +59,8 @@ struct THCNumerics<uint8_t> {
 
 template <>
 struct THCNumerics<int8_t> {
-  static inline __host__ __device__ int8_t min() { return SCHAR_MIN; }
-  static inline __host__ __device__ int8_t max() { return SCHAR_MAX; }
+  static inline __host__ __device__ int8_t (min)() { return SCHAR_MIN; }
+  static inline __host__ __device__ int8_t (max)() { return SCHAR_MIN; }
 
   static inline __host__ __device__ bool lt(int8_t a, int8_t b) { return a < b; }
   static inline __host__ __device__ bool le(int8_t a, int8_t b) { return a <= b; }
@@ -71,14 +74,15 @@ struct THCNumerics<int8_t> {
   static inline __host__ __device__  int8_t mul(int8_t a, int8_t b) { return a * b; }
   static inline __host__ __device__  int8_t sub(int8_t a, int8_t b) { return a - b; }
   static inline __host__ __device__  int8_t div(int8_t a, int8_t b) { return a / b; }
-  static inline __host__ __device__  int8_t abs(int8_t a) { return ::abs((int)a); }
   static inline __host__ __device__  int8_t pow(int8_t a, int8_t b) { return powi<int8_t>(a, b); }
+  static inline __host__ int8_t abs(int8_t a) { return std::abs(a); }
+  static inline __device__ int8_t abs(int8_t a) { return a < 0 ? -a : a; }
 };
 
 template <>
 struct THCNumerics<int16_t> {
-  static inline __host__ __device__ int16_t min() { return SHRT_MIN; }
-  static inline __host__ __device__ int16_t max() { return SHRT_MAX; }
+  static inline __host__ __device__ int16_t (min)() { return SHRT_MIN; }
+  static inline __host__ __device__ int16_t (max)() { return SHRT_MAX; }
 
   static inline __host__ __device__ bool lt(int16_t a, int16_t b) { return a < b; }
   static inline __host__ __device__ bool le(int16_t a, int16_t b) { return a <= b; }
@@ -92,14 +96,15 @@ struct THCNumerics<int16_t> {
   static inline __host__ __device__  int16_t mul(int16_t a, int16_t b) { return a * b; }
   static inline __host__ __device__  int16_t sub(int16_t a, int16_t b) { return a - b; }
   static inline __host__ __device__  int16_t div(int16_t a, int16_t b) { return a / b; }
-  static inline __host__ __device__  int16_t abs(int16_t a) { return ::abs((int)a); }
   static inline __host__ __device__  int16_t pow(int16_t a, int16_t b) { return powi<int16_t>(a, b); }
+  static inline __host__ int16_t abs(int16_t a) { return std::abs(a); }
+  static inline __device__ int16_t abs(int16_t a) { return a < 0 ? -a : a; }
 };
 
 template <>
 struct THCNumerics<int32_t> {
-  static inline __host__ __device__ int32_t min() { return INT_MIN; }
-  static inline __host__ __device__ int32_t max() { return INT_MAX; }
+  static inline __host__ __device__ int32_t (min)() { return INT_MIN; }
+  static inline __host__ __device__ int32_t (max)() { return INT_MAX; }
 
   static inline __host__ __device__ bool lt(int32_t a, int32_t b) { return a < b; }
   static inline __host__ __device__ bool le(int32_t a, int32_t b) { return a <= b; }
@@ -113,19 +118,15 @@ struct THCNumerics<int32_t> {
   static inline __host__ __device__  int32_t mul(int32_t a, int32_t b) { return a * b; }
   static inline __host__ __device__  int32_t sub(int32_t a, int32_t b) { return a - b; }
   static inline __host__ __device__  int32_t div(int32_t a, int32_t b) { return a / b; }
-  static inline __host__ __device__  int32_t abs(int32_t a) { return ::abs(a); }
   static inline __host__ __device__  int32_t pow(int32_t a, int32_t b) { return powi<int32_t>(a, b); }
+  static inline __host__ int32_t abs(int32_t a) { return std::abs(a); }
+  static inline __device__ int32_t abs(int32_t a) { return a < 0 ? -a : a; }
 };
 
 template <>
 struct THCNumerics<int64_t> {
-#ifdef _MSC_VER
-  static inline __host__ __device__ int64_t min() { return _I64_MIN; }
-  static inline __host__ __device__ int64_t max() { return _I64_MAX; }
-#else
-  static inline __host__ __device__ int64_t min() { return LONG_MIN; }
-  static inline __host__ __device__ int64_t max() { return LONG_MAX; }
-#endif
+  static inline __host__ __device__ int64_t (min)() { return LONG_MIN; }
+  static inline __host__ __device__ int64_t (max)() { return LONG_MAX; }
 
   static inline __host__ __device__ bool lt(int64_t a, int64_t b) { return a < b; }
   static inline __host__ __device__ bool le(int64_t a, int64_t b) { return a <= b; }
@@ -134,498 +135,707 @@ struct THCNumerics<int64_t> {
   static inline __host__ __device__ bool eq(int64_t a, int64_t b) { return a == b; }
   static inline __host__ __device__ bool ne(int64_t a, int64_t b) { return a != b; }
 
-
   static inline __host__ __device__  int64_t neg(int64_t a) { return -a; }
   static inline __host__ __device__  int64_t add(int64_t a, int64_t b) { return a + b; }
   static inline __host__ __device__  int64_t mul(int64_t a, int64_t b) { return a * b; }
   static inline __host__ __device__  int64_t sub(int64_t a, int64_t b) { return a - b; }
   static inline __host__ __device__  int64_t div(int64_t a, int64_t b) { return a / b; };
-  static inline __host__ __device__  int64_t abs(int64_t a) { return labs(a); }
   static inline __host__ __device__  int64_t pow(int64_t a, int64_t b) { return powi<int64_t>(a, b); }
+  static inline __host__ int64_t abs(int64_t a) { return std::abs(a); }
+  static inline __device__ int64_t abs(int64_t a) { return a < 0 ? -a : a; }
 };
 
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct THCNumerics<half> {
-#if CUDA_VERSION < 9000
-  static inline __host__ __device__ half min() { half h; h.x = 0xfbff; return h; }
-  static inline __host__ __device__ half max() { half h; h.x = 0x7bff; return h; }
-#else
-  static inline __host__ __device__ half min() { __half_raw h; h.x = 0xfbff; return h; }
-  static inline __host__ __device__ half max() { __half_raw h; h.x = 0x7bff; return h; }
-#endif
+    __host__ __device__
+    static
+    inline
+    half (min)()
+    {
+            return -65504;
+    }
+    __host__ __device__
+    static
+    inline
+    half (max)()
+    {
+            return 65504;
+    }
 
-  static inline __host__ __device__ bool lt(half a, half b) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __hlt(a, b);
-#else
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return fa < fb;
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  bool lt(half a, half b)
+  {
+        return a < b;
+  }
+  __host__
+  static
+  inline
+  bool lt(half a, half b)
+  {
     return THC_half2float(a) < THC_half2float(b);
-#endif
   }
 
-  static inline __host__ __device__ bool le(half a, half b) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __hle(a, b);
-#else
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return fa <= fb;
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  bool le(half a, half b)
+  {
+        return a <= b;
+  }
+  __host__
+  static
+  inline
+  bool le(half a, half b)
+  {
     return THC_half2float(a) <= THC_half2float(b);
-#endif
   }
 
-  static inline __host__ __device__ bool gt(half a, half b) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __hgt(a, b);
-#else
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return fa > fb;
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  bool gt(half a, half b)
+  {
+      return a > b;
+  }
+  __host__
+  static
+  inline
+  bool gt(half a, half b)
+  {
     return THC_half2float(a) > THC_half2float(b);
-#endif
   }
 
-  static inline __host__ __device__ bool ge(half a, half b) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __hge(a, b);
-#else
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return fa >= fb;
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  bool ge(half a, half b)
+  {
+      return a >= b;
+  }
+  __host__
+  static
+  inline
+  bool ge(half a, half b)
+  {
     return THC_half2float(a) >= THC_half2float(b);
-#endif
   }
 
-  static inline __host__ __device__ bool eq(half a, half b) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __heq(a, b);
-#else
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return fa == fb;
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  bool eq(half a, half b)
+  {
+      return a == b;
+  }
+  __host__
+  static
+  inline
+  bool eq(half a, half b)
+  {
     return THC_half2float(a) == THC_half2float(b);
-#endif
   }
 
-  static inline __host__ __device__ bool ne(half a, half b) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __hne(a, b);
-#else
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return fa != fb;
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  bool ne(half a, half b)
+  {
+      return a != b;
+  }
+  __host__
+  static
+  inline
+  bool ne(half a, half b)
+  {
     return THC_half2float(a) != THC_half2float(b);
-#endif
   }
 
-  static inline __host__ __device__ half exp(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return hexp(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(expf(fa));
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half exp(half a)
+  {
+      float fa = __half2float(a);
+      return __float2half(expf(fa));
+  }
+  __host__
+  static
+  inline
+  half exp(half a)
+  {
     return THC_float2half(expf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half exp10(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return hexp10(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(exp10f(fa));
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half exp10(half a)
+  {
+      float fa = __half2float(a);
+      return __float2half(exp10f(fa));
+  }
+  __host__
+  static
+  inline
+  half exp10(half a)
+  {
     return THC_float2half(exp10f(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half log(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return hlog(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(logf(fa));
-#endif
-#else // __CUDA_ARCH__
-    return THC_float2half(logf(THC_half2float(a)));
-#endif
+  __device__
+  static
+  inline
+  half log(half a)
+  {
+      float fa = __half2float(a);
+      return __float2half(logf(fa));
   }
-
-  static inline __host__ __device__ half log10(half a) {
-#ifdef __CUDA_ARCH__
-    float fa = __half2float(a);
-    return __float2half(log10f(fa));
-#else // __CUDA_ARCH__
-    return THC_float2half(log10f(THC_half2float(a)));
-#endif
+  __host__
+  static
+  inline
+  half log(half a)
+  {
+    return THC_float2half(logf(THC_half2float(a)));
   }
 
-  static inline __host__ __device__ half log1p(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half log1p(half a)
+  {
     float fa = __half2float(a);
     return __float2half(log1pf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half log1p(half a)
+  {
     return THC_float2half(log1pf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half log2(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline  half log2(half a) {
     float fa = __half2float(a);
     return __float2half(log2f(fa));
-#else // __CUDA_ARCH__
+  }
+
+  __host__
+  static
+  inline  half log2(half a) {
     return THC_float2half(log2f(THC_half2float(a)));
-#endif
   }
 
-static inline __host__ __device__ half lgamma(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline  half log10(half a) {
+    float fa = __half2float(a);
+    return __float2half(log10f(fa));
+  }
+
+  __host__
+  static
+  inline  half log10(half a) {
+    return THC_float2half(log10f(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half lgamma(half a) {
     float fa = __half2float(a);
     return __float2half(lgammaf(fa));
-#else // __CUDA_ARCH__
-    return THC_float2half(lgammaf(THC_half2float(a)));
-#endif
+  }
+  __host__
+  static
+  inline
+  half lgamma(half a)
+  {
+    return THC_float2half(lgamma(THC_half2float(a)));
   }
 
-  static inline __host__ __device__ half expm1(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half expm1(half a) {
     float fa = __half2float(a);
     return __float2half(expm1f(fa));
-#else // __CUDA_ARCH__
+  }
+
+  __host__
+  static
+  inline
+  half expm1(half a) {
     return THC_float2half(expm1f(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half cos(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return hcos(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(cosf(fa));
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half cos(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hcos(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(cosf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half cos(half a)
+  {
     return THC_float2half(cosf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half sin(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return hsin(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(sinf(fa));
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half sin(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hsin(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(sinf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half sin(half a)
+  {
     return THC_float2half(sinf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half sqrt(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return hsqrt(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(sqrtf(fa));
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half sqrt(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hsqrt(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(sqrtf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half sqrt(half a)
+  {
     return THC_float2half(sqrtf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half rsqrt(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return hrsqrt(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(rsqrtf(fa));
-#endif
-#else // __CUDA_ARCH__
-    return THC_float2half(rsqrtf(THC_half2float(a)));
-#endif
-  }
-
-  static inline __host__ __device__ half ceil(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return hceil(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(ceilf(fa));
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half rsqrt(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hrsqrt(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(rsqrtf(fa));
+    #endif
+  }
+//  __host__
+//  static
+//  inline
+//  half rsqrt(half a)
+//  {
+//    return THC_float2half(std::rsqrt(THC_half2float(a)));
+//  }
+
+  __device__
+  static
+  inline
+  half ceil(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hceil(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(ceilf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half ceil(half a)
+  {
     return THC_float2half(ceilf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half floor(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return hfloor(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(floorf(fa));
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half floor(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hfloor(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(floorf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half floor(half a)
+  {
     return THC_float2half(floorf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half trunc(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return htrunc(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(truncf(fa));
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half trunc(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return htrunc(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(truncf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half trunc(half a)
+  {
     return THC_float2half(truncf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half neg(half a) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __hneg(a);
-#else
-    float fa = __half2float(a);
-    return __float2half(-fa);
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half neg(half a)
+  {
+      return -a;
+  }
+  __host__
+  static
+  inline
+  half neg(half a)
+  {
     return THC_float2half(-(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half acos(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half acos(half a)
+  {
     float fa = __half2float(a);
     return __float2half(acosf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half acos(half a)
+  {
     return THC_float2half(acosf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half cosh(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half cosh(half a)
+  {
     float fa = __half2float(a);
     return __float2half(coshf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half cosh(half a)
+  {
     return THC_float2half(coshf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half asin(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half asin(half a)
+  {
     float fa = __half2float(a);
     return __float2half(asinf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half asin(half a)
+  {
     return THC_float2half(asinf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half sinh(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half sinh(half a)
+  {
     float fa = __half2float(a);
     return __float2half(sinhf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half sinh(half a)
+  {
     return THC_float2half(sinhf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half tan(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half tan(half a)
+  {
     float fa = __half2float(a);
     return __float2half(tanf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half tan(half a)
+  {
     return THC_float2half(tanf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half atan(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half atan(half a)
+  {
     float fa = __half2float(a);
     return __float2half(atanf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half atan(half a)
+  {
     return THC_float2half(atanf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half tanh(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half tanh(half a)
+  {
     float fa = __half2float(a);
     return __float2half(tanhf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half tanh(half a)
+  {
     return THC_float2half(tanhf(THC_half2float(a)));
-#endif
   }
 
-
-   static inline __host__ __device__ half erf(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half erf(half a)
+  {
     float fa = __half2float(a);
     return __float2half(erff(fa));
-#else // __CUDA_ARCH__
-    return THC_float2half(erff(THC_half2float(a)));
-#endif
+  }
+  __host__
+  static
+  inline
+  half erf(half a)
+  {
+    return THC_float2half(erf(THC_half2float(a)));
   }
 
-
-   static inline __host__ __device__ half erfinv(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half erfinv(half a)
+  {
     float fa = __half2float(a);
     return __float2half(erfinvf(fa));
-#else // __CUDA_ARCH__
-    return THC_float2half(erfinvf(THC_half2float(a)));
-#endif
+  }
+  __host__
+  static
+  inline
+  half erfinv(half a)
+  {
+    return THC_float2half(erfinv(THC_half2float(a)));
   }
 
-  static inline __host__ __device__ half abs(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half abs(half a)
+  {
     float fa = __half2float(a);
     return __float2half(fabs(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half abs(half a)
+  {
     return THC_float2half(fabs(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half round(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half round(half a)
+  {
     float fa = __half2float(a);
     return __float2half(roundf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half round(half a)
+  {
     return THC_float2half(roundf(THC_half2float(a)));
-#endif
   }
 
-  static inline __host__ __device__ half frac(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half frac(half a)
+  {
     float fa = __half2float(a);
     return __float2half(fa - truncf(fa));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half frac(half a)
+  {
     float fa = THC_half2float(a);
     return THC_float2half(fa - floorf(fa));
-#endif
   }
 
-  static inline __host__ __device__ half cinv(half a) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half cinv(half a)
+  {
     float fa = __half2float(a);
     return __float2half(1.0f / fa);
-#else // __CUDA_ARCH__
-    return THC_float2half(1.0f / THC_half2float(a));
-#endif
   }
-
-  static inline __host__ __device__ half add(half a, half b) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __hadd(a, b);
-#else
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return __float2half( fa + fb );
-#endif
-#else // __CUDA_ARCH__
-    return THC_float2half(THC_half2float(a) + THC_half2float(b));
-#endif
+  __host__
+  static
+  inline
+  half cinv(half a)
+  {
+    return THC_float2half(1.0f / THC_half2float(a));
   }
 
-  static inline __host__ __device__ half div(half a, half b) {
-#ifdef __CUDA_ARCH__
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return __float2half( fa / fb );
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half add(half a, half b)
+  {
+      return a + b;
+  }
+  __host__
+  static
+  inline
+  half add(half a, half b)
+  {
+      return a + b;
+  }
+
+  __device__
+  static
+  inline
+  half div(half a, half b)
+  {
+      return a / b;
+  }
+  __host__
+  static
+  inline
+  half div(half a, half b)
+  {
     return THC_float2half(THC_half2float(a) / THC_half2float(b));
-#endif
   }
 
-  static inline __host__ __device__ half mul(half a, half b) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __hmul(a, b);
-#else
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return __float2half( fa * fb );
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half mul(half a, half b)
+  {
+      return a * b;
+  }
+  __host__
+  static
+  inline
+  half mul(half a, half b)
+  {
     return THC_float2half(THC_half2float(a) * THC_half2float(b));
-#endif
   }
 
-  static inline __host__ __device__ half sub(half a, half b) {
-#ifdef __CUDA_ARCH__
-#ifdef CUDA_HALF_INSTRUCTIONS
-    return __hsub(a, b);
-#else
-    float fa = __half2float(a);
-    float fb = __half2float(b);
-    return __float2half( fa - fb );
-#endif
-#else // __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half sub(half a, half b)
+  {
+      return a - b;
+  }
+  __host__
+  static
+  inline
+  half sub(half a, half b)
+  {
     return THC_float2half(THC_half2float(a) - THC_half2float(b));
-#endif
   }
 
-  static inline __host__ __device__ half pow(half a, half b) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half pow(half a, half b)
+  {
     float fa = __half2float(a);
     float fb = __half2float(b);
     return __float2half(powf(fa, fb));
-#else // __CUDA_ARCH__
+  }
+  __host__
+  static
+  inline
+  half pow(half a, half b)
+  {
     return THC_float2half(powf(THC_half2float(a), THC_half2float(b)));
-#endif
   }
 
-  static inline __host__ __device__ half atan2(half a, half b) {
-#ifdef __CUDA_ARCH__
+  __device__
+  static
+  inline
+  half atan2(half a, half b) {
     float fa = __half2float(a);
     float fb = __half2float(b);
     return __float2half(atan2f(fa, fb));
-#else // __CUDA_ARCH__
-    return THC_float2half(atan2f(THC_half2float(a), THC_half2float(b)));
-#endif
   }
 
+  __host__
+  static
+  inline
+  half atan2(half a, half b) {
+     return THC_float2half(atan2f(THC_half2float(a), THC_half2float(b)));
+  }
 };
 #endif
 
 template <>
 struct THCNumerics<float> {
-  static inline __host__ __device__ float min() { return -FLT_MAX; }
-  static inline __host__ __device__ float max() { return FLT_MAX; }
+  static inline __host__ __device__ float (min)() { return -FLT_MAX; }
+  static inline __host__ __device__ float (max)() { return FLT_MAX; }
 
   static inline __host__ __device__ bool lt(float a, float b) { return a < b; }
   static inline __host__ __device__ bool le(float a, float b) { return a <= b; }
@@ -675,8 +885,8 @@ struct THCNumerics<float> {
 
 template <>
 struct THCNumerics<double> {
-  static inline __host__ __device__ double min() { return -DBL_MAX; }
-  static inline __host__ __device__ double max() { return DBL_MAX; }
+  static inline __host__ __device__ double (min)() { return -DBL_MAX; }
+  static inline __host__ __device__ double (max)() { return DBL_MAX; }
 
   static inline __host__ __device__ bool lt(double a, double b) { return a < b; }
   static inline __host__ __device__ bool le(double a, double b) { return a <= b; }
@@ -701,7 +911,7 @@ struct THCNumerics<double> {
   static inline __host__ __device__  double ceil (double a) { return  ::ceil(a); }
   static inline __host__ __device__  double floor(double a) { return ::floor(a); }
   static inline __host__ __device__  double trunc(double a) { return ::trunc(a); }
-  static inline __host__ __device__  double neg  (double a) { return       -a; }
+  static inline __host__ __device__  double neg  (double a) { return         -a; }
   static inline __host__ __device__  double acos (double a) { return  ::acos(a); }
   static inline __host__ __device__  double cosh (double a) { return  ::cosh(a); }
   static inline __host__ __device__  double acosh(double a) { return ::acosh(a); }
@@ -712,7 +922,7 @@ struct THCNumerics<double> {
   static inline __host__ __device__  double atan (double a) { return  ::atan(a); }
   static inline __host__ __device__  double tanh (double a) { return  ::tanh(a); }
   static inline __host__ __device__  double erf  (double a) { return   ::erf(a); }
-  static inline __host__ __device__  double abs  (double a) { return   ::abs(a); }
+  static inline __host__ __device__  double abs  (double a) { return   ::fabs(a); }
   static inline __host__ __device__  double round(double a) { return ::round(a); }
   static inline __host__ __device__  double frac (double a) { return a - ::trunc(a); }
   static inline __host__ __device__  double cinv (double a) { return 1.0 / a; }
@@ -728,46 +938,59 @@ struct THCNumerics<double> {
 /// is a struct without a constructor/implicit conversion constructor.
 /// We use this to convert scalar values to the given type that the
 /// tensor expects.
-template <typename In, typename Out>
+template<typename In, typename Out>
 struct ScalarConvert {
-  static __host__ __device__ Out to(const In v) { return (Out) v; }
+  __host__ __device__
+  static
+  Out to(const In& v) { return static_cast<Out>(v); }
 };
 
 #ifdef CUDA_HALF_TENSOR
-template <typename Out>
-struct ScalarConvert<half, Out> {
-  static __host__ __device__ Out to(const half v) {
-#ifdef __CUDA_ARCH__
-    return (Out) __half2float(v);
-#else
-    return (Out) THC_half2float(v);
-#endif
-  }
-};
+  template<typename Out>
+  struct ScalarConvert<half, Out> {
+    __device__
+    static
+    Out to(half v)
+    {
+        return static_cast<Out>(v);
+    }
 
-template <typename In>
-struct ScalarConvert<In, half> {
-  static __host__ __device__ half to(const In v) {
-#ifdef __CUDA_ARCH__
-    return __float2half((float) v);
-#else
-    return THC_float2half((float) v);
-#endif
-  }
-};
+    __host__
+    static
+    Out to(half v)
+    {
+      return static_cast<Out>(THC_half2float(v));
+    }
+  };
+
+  template <typename In>
+  struct ScalarConvert<In, half> {
+    __device__
+    static
+    half to(In v)
+    {
+        return static_cast<half>(v);
+    }
 
-template <>
-struct ScalarConvert<half, half> {
-  static __host__ __device__ half to(const half v) {
-    return v;
-  }
-};
+    __host__
+    static
+    half to(In v)
+    {
+      return THC_float2half(static_cast<float>(v));
+    }
+  };
 
-template <typename T, typename U>
-__host__ __device__ T scalar_cast(U u) {
-  return ScalarConvert<U, T>::to(u);
-}
+  template <>
+  struct ScalarConvert<half, half> {
+    __device__
+    static
+    half to(half v) { return v; }
+  };
 
+  template <typename T, typename U>
+    __host__ __device__ T scalar_cast(U u) {
+    return ScalarConvert<U, T>::to(u);
+  }
 #endif
 
-#endif // THC_NUMERICS_INC
+#endif // THC_NUMERICS_INC
diff --git a/aten/src/THC/THCStream.cpp b/aten/src/THC/THCStream.cpp
index 49fe680a3..0e8c29fcf 100644
--- a/aten/src/THC/THCStream.cpp
+++ b/aten/src/THC/THCStream.cpp
@@ -37,7 +37,9 @@ THCStream* THCStream_newWithPriority(int flags, int priority)
   THCStream* self = (THCStream*) malloc(sizeof(THCStream));
   self->refcount = 1;
   THCudaCheck(cudaGetDevice(&self->device));
-  THCudaCheck(cudaStreamCreateWithPriority(&self->stream, flags, priority));
+#if !defined(__HIP_PLATFORM_HCC__)
+  THCudaCheck(cudaStreamCreateWithFlags(&self->stream, flags));
+#endif
   return self;
 }
 
diff --git a/aten/src/THC/THCTensorIndex.cu b/aten/src/THC/THCTensorIndex.cu
index ac0065afb..9ae86a8cc 100644
--- a/aten/src/THC/THCTensorIndex.cu
+++ b/aten/src/THC/THCTensorIndex.cu
@@ -1,3 +1,4 @@
+#include <thrust/execution_policy.h> 
 #include "THC.h"
 #include "THCTensorMath.h"
 #include "THCGeneral.h"
diff --git a/aten/src/THC/THCTensorMathPairwise.cu b/aten/src/THC/THCTensorMathPairwise.cu
index f530d814d..ea2d2ad4e 100644
--- a/aten/src/THC/THCTensorMathPairwise.cu
+++ b/aten/src/THC/THCTensorMathPairwise.cu
@@ -23,14 +23,20 @@ struct TensorAddConstantOp {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorAddConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined (CUDA_HALF_INSTRUCTIONS)|| defined (__HIP_PLATFORM_HCC__)
+  #if defined(__HIP_PLATFORM_HCC__)
+    __host__ __device__
+    explicit
+  #endif
   TensorAddConstantOp(half v) : val(v) {}
 #else
   TensorAddConstantOp(half v) : fval(THC_half2float(v)) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined (__HIP_PLATFORM_HCC__)
+    *out = *in + val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hadd(*in, val);
 #else
     float fin = __half2float(*in);
@@ -40,7 +46,9 @@ struct TensorAddConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined (__HIP_PLATFORM_HCC__)
+    *v += val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hadd(*v, val);
 #else
     float fv = __half2float(*v);
@@ -49,7 +57,7 @@ struct TensorAddConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -76,14 +84,20 @@ struct TensorSubConstantOp {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorSubConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+  TensorSubConstantOp(half v) : val{v} {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
   TensorSubConstantOp(half v): val(THC_float2half(-(THC_half2float(v)))) {}
 #else
   TensorSubConstantOp(half v): fval(-(THC_half2float(v))) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  *out = *in + val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hadd(*in, val);
 #else
     float fin = __half2float(*in);
@@ -93,7 +107,9 @@ struct TensorSubConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *v += val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hadd(*v, val);
 #else
     float fv = __half2float(*v);
@@ -102,7 +118,7 @@ struct TensorSubConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -128,14 +144,20 @@ struct TensorMulConstantOp {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorMulConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+  TensorMulConstantOp(half v) : val(v) {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
   TensorMulConstantOp(half v) : val(v) {}
 #else
   TensorMulConstantOp(half v) : fval(THC_half2float(v)) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *out = *in * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hmul(*in, val);
 #else
     float fin = __half2float(*in);
@@ -145,7 +167,9 @@ struct TensorMulConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *v = *v * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hmul(*v, val);
 #else
     float fv = __half2float(*v);
@@ -154,7 +178,7 @@ struct TensorMulConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -176,6 +200,7 @@ struct TensorDivConstantOp {
   const T val;
 };
 
+#if !defined(__HIP_PLATFORM_HCC__)
 template <>
 struct TensorDivConstantOp<float> {
   TensorDivConstantOp(float v) : val(1.f / v) {}
@@ -203,17 +228,24 @@ struct TensorDivConstantOp<double> {
 
   const double val;
 };
+#endif
 
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorDivConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+  TensorDivConstantOp(half v) : val(ScalarInv<half>::to(v)) {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
   TensorDivConstantOp(half v) : val(ScalarInv<half>::to(v)) {}
 #else
   TensorDivConstantOp(half v) : fval(1.f / THC_half2float(v)) {}
 #endif
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *out = *in * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hmul(*in, val);
 #else
     float fin = __half2float(*in);
@@ -223,7 +255,9 @@ struct TensorDivConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *v = *v * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hmul(*v, val);
 #else
     float fv = __half2float(*v);
@@ -232,7 +266,7 @@ struct TensorDivConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -291,15 +325,19 @@ struct TensorRemainderOp<double> {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorRemainderOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
   TensorRemainderOp(half v) : val(v) {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
 #else
   TensorRemainderOp(half v): fval(THC_half2float(v)) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hsub(*in,  __hmul(val, hfloor(__hdiv(*in,  val))));
+#elif defined(__HIP_PLATFORM_HCC__)
+    *out = __hsub(*in,  __hmul(val, hfloor(hdiv(*in,  val))));
 #else
     float fin = __half2float(*in);
     float fout = fin - fval * floorf(fin / fval);
@@ -308,8 +346,10 @@ struct TensorRemainderOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hsub(*v, __hmul(val, hfloor(__hdiv(*v, val))));
+#elif defined(__HIP_PLATFORM_HCC__)
+    *v = __hsub(*v, __hmul(val, hfloor(hdiv(*v, val))));
 #else
     float fv = __half2float(*v);
     fv = fv - fval * floorf(fv / fval);
@@ -317,7 +357,7 @@ struct TensorRemainderOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -356,7 +396,12 @@ struct TensorFmodOp<double> {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorFmodOp<half> {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  TensorFmodOp(half v): fval(v) {}
+#else
   TensorFmodOp(half v): fval(THC_half2float(v)) {}
+#endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
     *out = __float2half(fmodf(__half2float(*in), fval));
diff --git a/aten/src/THC/THCTensorMathReduce.cuh b/aten/src/THC/THCTensorMathReduce.cuh
index b5282d97e..79e23d355 100644
--- a/aten/src/THC/THCTensorMathReduce.cuh
+++ b/aten/src/THC/THCTensorMathReduce.cuh
@@ -105,14 +105,24 @@ struct SquareFunctor<ResT, half> {
 template <typename T>
 struct ReduceMin {
   inline __device__ T operator()(T a, T b) const {
-    return THCNumerics<T>::lt(a, b) ? a : b;
+    #if defined(__HIP_PLATFORM_HCC__)
+       T diff = THCNumerics<T>::sub(a, b);
+       return (diff < 0 ) ? a : b;
+    #else
+      return THCNumerics<T>::lt(a, b) ? a : b;
+    #endif
   }
 };
 
 template <typename T>
 struct ReduceMax {
   inline __device__ T operator()(T a, T b) const {
-    return THCNumerics<T>::gt(a, b) ? a : b;
+    #if defined(__HIP_PLATFORM_HCC__)
+       T diff = THCNumerics<T>::sub(a, b);
+       return (diff > 0 ) ? a : b;
+    #else
+      return THCNumerics<T>::gt(a, b) ? a : b;
+    #endif
   }
 };
 
@@ -325,7 +335,7 @@ __global__ void THCTensor_kernel_varOuterDim(Real *tgt, Real *src_, unsigned num
             THCNumerics<Accreal>::mul(delta, delta2));
         src += num_irows;
       }
-      
+
       if (flag) {
         m2 = THCNumerics<Accreal>::div(m2, ScalarConvert<int, Accreal>::to(row_size));
       } else {
@@ -399,8 +409,8 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
    * Each block computes the var/std of blockDim.y (32) rows at once.
    * One can visualize the computation as a 16 (x) by 32 (y) grid.
    * - Each of the 32 rows of the block is responsible for the computation
-   *   of one input row. 
-   * - Each row has 16 columns; the variance computation of one input row is 
+   *   of one input row.
+   * - Each row has 16 columns; the variance computation of one input row is
    *   split between 16 threads.
    * - Each of those 16 threads handles the accumulation of 1/16 of the input
    *   row's data.
@@ -438,11 +448,11 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
 
     /*
      * We are reducing across each row of 16 threads to find the true sum of the
-     * entire input row. The warp shfl xor loop ultimately gives each thread the 
+     * entire input row. The warp shfl xor loop ultimately gives each thread the
      * true sum.
      */
     for (unsigned lane_mask = 8; lane_mask > 0; lane_mask >>= 1) {
-      local_sum = THCNumerics<Accreal>::add(local_sum, 
+      local_sum = THCNumerics<Accreal>::add(local_sum,
           WARP_SHFL_XOR((row < num_rows) ? local_sum : acc_zero, lane_mask, 16));
     }
     Accreal true_mean = THCNumerics<Accreal>::div(local_sum, 
@@ -468,7 +478,7 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
      * the total sum, which is equal to the M2 for the entire input row.
      */
     for (unsigned s = 8; s >= 1; s >>= 1) {
-      adjusted_M2 = THCNumerics<Accreal>::add(adjusted_M2, 
+      adjusted_M2 = THCNumerics<Accreal>::add(adjusted_M2,
           WARP_SHFL_DOWN((row < num_rows) ? adjusted_M2 : acc_zero, s, 16));
     }
 
diff --git a/aten/src/THC/THCTensorRandom.cpp b/aten/src/THC/THCTensorRandom.cpp
index ddccb7c5a..76cc2d74e 100644
--- a/aten/src/THC/THCTensorRandom.cpp
+++ b/aten/src/THC/THCTensorRandom.cpp
@@ -83,7 +83,7 @@ THCGenerator* THCRandom_getGenerator(THCState* state)
   return gen;
 }
 
-struct curandStateMtgp32* THCRandom_generatorStates(struct THCState* state)
+curandStateMtgp32* THCRandom_generatorStates(struct THCState* state)
 {
   return THCRandom_getGenerator(state)->gen_states;
 }
diff --git a/aten/src/THC/THCTensorRandom.cu b/aten/src/THC/THCTensorRandom.cu
index a179213cd..77cb5aaa2 100644
--- a/aten/src/THC/THCTensorRandom.cu
+++ b/aten/src/THC/THCTensorRandom.cu
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 #include "THCTensorRandom.h"
 #include "THCDeviceUtils.cuh"
 #include "THCGeneral.h"
@@ -6,67 +7,68 @@
 #include "THCReduceApplyUtils.cuh"
 #include "THCTensorRandom.cuh"
 
+#include <hiprng.h>
+#include <hiprng_kernel.h>
+
 #include <thrust/functional.h>
-#include <curand.h>
-#include <curand_kernel.h>
-#include <curand_mtgp32_host.h>
-#include <curand_mtgp32dc_p_11213.h>
 
-#define MAX_NUM_BLOCKS 200 
+#define MAX_NUM_BLOCKS 64
 #define BLOCK_SIZE 256
 
-
 THCGenerator* THCRandom_getGenerator(THCState* state);
 
 /* Sets up generator. Allocates but does not create the generator states. */
-__host__ void initializeGenerator(THCState *state, THCGenerator* gen)
+void initializeGenerator(THCState *state, THCGenerator* gen)
 {
-  THCudaCheck(THCudaMalloc(state, (void**)&gen->gen_states, MAX_NUM_BLOCKS * sizeof(curandStateMtgp32)));
+  THCudaCheck(THCudaMalloc(state, (void**)&gen->gen_states, MAX_NUM_BLOCKS * sizeof(hiprngStateMtgp32)));
   THCudaCheck(THCudaMalloc(state, (void**)&gen->kernel_params, sizeof(mtgp32_kernel_params)));
 }
 
 /* Creates a new generator state given the seed. */
-__host__ void createGeneratorState(THCGenerator* gen, uint64_t seed)
+void createGeneratorState(THCGenerator* gen, uint64_t seed)
 {
-  if (curandMakeMTGP32Constants(mtgp32dc_params_fast_11213, gen->kernel_params) != CURAND_STATUS_SUCCESS)
+  if (hiprngMakeMTGP32Constants(mtgp32_params_fast_11213, gen->kernel_params) != HIPRNG_STATUS_SUCCESS)
   {
     THError("Creating MTGP constants failed.");
   }
-  if (curandMakeMTGP32KernelState(gen->gen_states, mtgp32dc_params_fast_11213,
-                                  gen->kernel_params, MAX_NUM_BLOCKS, seed) != CURAND_STATUS_SUCCESS)
+  if (hiprngMakeMTGP32KernelState(gen->gen_states, mtgp32_params_fast_11213,
+                                  gen->kernel_params, MAX_NUM_BLOCKS, seed) != HIPRNG_STATUS_SUCCESS)
   {
     THError("Creating MTGP kernel state failed.");
   }
 }
 
-__host__ void THCRandom_getRNGState(THCState* state, THByteTensor *rng_state)
+void THCRandom_getRNGState(THCState* state, THByteTensor *rng_state)
 {
   THCGenerator* gen = THCRandom_getGenerator(state);
 
   // The RNG state comprises the MTPG32 states, the seed, and an offset used for Philox
-  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(curandStateMtgp32);
+  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(hiprngStateMtgp32);
+  //static const size_t seed_size = sizeof(unsigned long);
   static const size_t seed_size = sizeof(gen->initial_seed);
   static const size_t offset_size = sizeof(gen->philox_seed_offset);
   static const size_t total_size = states_size + seed_size + offset_size;
   THByteTensor_resize1d(rng_state, total_size);
   THArgCheck(THByteTensor_nElement(rng_state) == total_size, 1, "RNG state is wrong size");
   THArgCheck(THByteTensor_isContiguous(rng_state), 1, "RNG state must be contiguous");
-  THCudaCheck(cudaMemcpy(THByteTensor_data(rng_state), gen->gen_states,
-                         states_size, cudaMemcpyDeviceToHost));
+  THCudaCheck(hipMemcpy(THByteTensor_data(rng_state), gen->gen_states,
+                         states_size, hipMemcpyDeviceToHost));
   memcpy(THByteTensor_data(rng_state) + states_size, &gen->initial_seed, seed_size);
   memcpy(THByteTensor_data(rng_state) + states_size + seed_size, &gen->philox_seed_offset, offset_size);
+
 }
 
-__global__ void set_rngstate_kernel(curandStateMtgp32 *state, mtgp32_kernel_params *kernel)
+__global__ void set_rngstate_kernel(hiprngStateMtgp32 *state, mtgp32_kernel_params *kernel)
 {
-  state[threadIdx.x].k = kernel;
+  state[hipThreadIdx_x].k = kernel;
 }
 
-__host__ void THCRandom_setRNGState(THCState* state, THByteTensor *rng_state)
+
+void THCRandom_setRNGState(THCState* state, THByteTensor *rng_state)
 {
   THCGenerator* gen = THCRandom_getGenerator(state);
-
-  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(curandStateMtgp32);
+  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(hiprngStateMtgp32);
+  //static const size_t seed_size = sizeof(unsigned long);
   static const size_t seed_size = sizeof(gen->initial_seed);
   static const size_t offset_size = sizeof(gen->philox_seed_offset);
   static const size_t total_size = states_size + seed_size + offset_size;
@@ -79,46 +81,55 @@ __host__ void THCRandom_setRNGState(THCState* state, THByteTensor *rng_state)
   }
   THArgCheck(THByteTensor_isContiguous(rng_state), 1, "RNG state must be contiguous");
 
-  THCudaCheck(cudaMemcpy(gen->gen_states, THByteTensor_data(rng_state),
-                         states_size, cudaMemcpyHostToDevice));
-  set_rngstate_kernel<<<1, MAX_NUM_BLOCKS, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, gen->kernel_params);
-  memcpy(&gen->initial_seed, THByteTensor_data(rng_state) + states_size, seed_size);
-  if (!no_philox_seed) {
-    memcpy(&gen->philox_seed_offset, THByteTensor_data(rng_state) + states_size + seed_size, offset_size);
-  }
-  else {
-    gen->philox_seed_offset = 0;
-  }
+  THCudaCheck(hipMemcpy(gen->gen_states, THByteTensor_data(rng_state),
+                         states_size, hipMemcpyHostToDevice));
+  hipLaunchKernelGGL(
+    set_rngstate_kernel,
+    dim3(1),
+    dim3(MAX_NUM_BLOCKS),
+    0,
+    THCState_getCurrentStream(state),
+    gen->gen_states,
+    gen->kernel_params);
+
+   memcpy(&gen->initial_seed, THByteTensor_data(rng_state) + states_size, seed_size);
+
+   if (!no_philox_seed) {
+     memcpy(&gen->philox_seed_offset, THByteTensor_data(rng_state) + states_size + seed_size, offset_size);
+   }
+   else {
+     gen->philox_seed_offset = 0;
+   }
 }
 
+// CURAND_PATH
+
 // Goes from (0, 1] to [0, 1). Note 1-x is not sufficient since for some floats
 // eps near 0, 1-eps will round to 1.
-template <typename T>
-__device__ inline T reverse_bounds(T value) {
+ template <typename T>
+ __device__ inline T reverse_bounds(T value) {
   if (THCNumerics<T>::eq(value, ScalarConvert<int, T>::to(1))) {
     return ScalarConvert<int, T>::to(0);
   }
   return value;
 }
 
-
 #ifdef CUDA_HALF_TENSOR
 __device__ inline half half_uniform_scale_and_shift(float x, double a, double b) {
-  half width = ScalarConvert<double, half>::to(b - a);
-  half start = ScalarConvert<double, half>::to(a);
-  half scaled = THCNumerics<half>::mul(reverse_bounds(ScalarConvert<float, half>::to(x)), width);
-  return THCNumerics<half>::add(scaled, start);
+   half width = ScalarConvert<double, half>::to(b - a);
+   half start = ScalarConvert<double, half>::to(a);
+   half scaled = THCNumerics<half>::mul(reverse_bounds(ScalarConvert<float, half>::to(x)), width);
+   return THCNumerics<half>::add(scaled, start);
 }
 #endif
 
 #define GENERATE_KERNEL1(NAME, T, ARG1, CURAND_T, CURAND_FUNC, TRANSFORM)      \
-__global__ void NAME(curandStateMtgp32 *state, int size, T *result, ARG1)      \
+__global__ void NAME(hiprngStateMtgp32 *state, int size, T *result, ARG1)      \
 {                                                                              \
-  int idx = blockIdx.x * BLOCK_SIZE + threadIdx.x;                             \
+  int idx = hipBlockIdx_x * BLOCK_SIZE + hipThreadIdx_x;                             \
   int rounded_size = THCCeilDiv(size, BLOCK_SIZE) * BLOCK_SIZE;                \
   for (int i = idx; i < rounded_size; i += BLOCK_SIZE * MAX_NUM_BLOCKS) {      \
-    CURAND_T x = CURAND_FUNC(&state[blockIdx.x]);                              \
+    CURAND_T x = CURAND_FUNC(&state[hipBlockIdx_x]);                              \
     if (i < size) {                                                            \
       T y = TRANSFORM;                                                         \
       result[i] = y;                                                           \
@@ -127,12 +138,12 @@ __global__ void NAME(curandStateMtgp32 *state, int size, T *result, ARG1)      \
 }
 
 #define GENERATE_KERNEL2(NAME, T, ARG1, ARG2, CURAND_T, CURAND_FUNC, TRANSFORM)      \
-__global__ void NAME(curandStateMtgp32 *state, int size, T *result, ARG1, ARG2)      \
+__global__ void NAME(hiprngStateMtgp32 *state, int size, T *result, ARG1, ARG2)      \
 {                                                                                    \
-  int idx = blockIdx.x * BLOCK_SIZE + threadIdx.x;                                   \
+  int idx = hipBlockIdx_x * BLOCK_SIZE + hipThreadIdx_x;                                   \
   int rounded_size = THCCeilDiv(size, BLOCK_SIZE) * BLOCK_SIZE;                      \
   for (int i = idx; i < rounded_size; i += BLOCK_SIZE * MAX_NUM_BLOCKS) {            \
-    CURAND_T x = CURAND_FUNC(&state[blockIdx.x]);                                    \
+    CURAND_T x = CURAND_FUNC(&state[hipBlockIdx_x]);                                    \
     if (i < size) {                                                                  \
       T y = TRANSFORM;                                                               \
       result[i] = y;                                                                 \
@@ -140,6 +151,7 @@ __global__ void NAME(curandStateMtgp32 *state, int size, T *result, ARG1, ARG2)
   }                                                                                  \
 }
 
+
 template<typename T, typename U>
 struct is_same { static const bool value = false; };
 
@@ -147,44 +159,44 @@ template<typename T>
 struct is_same<T, T> { static const bool value = true; };
 
 template<typename real, typename prob_type>
-__global__ void generate_bernoulli_tensor(curandStateMtgp32 *state, int size,
+__global__ void generate_bernoulli_tensor(hiprngStateMtgp32 *state, int size,
         real *result, prob_type *probs)
 {
-  int idx = blockIdx.x * BLOCK_SIZE + threadIdx.x;
+  int idx = hipBlockIdx_x * BLOCK_SIZE + hipThreadIdx_x;
   int rounded_size = THCCeilDiv(size, BLOCK_SIZE) * BLOCK_SIZE;
   for (int i = idx; i < rounded_size; i += BLOCK_SIZE * MAX_NUM_BLOCKS) {
     if (is_same<prob_type, double>::value) {
-      double x = curand_uniform_double(&state[blockIdx.x]);
+      double x = hiprng_uniform(&state[hipBlockIdx_x]);
       if (i < size)
         result[i] = ScalarConvert<bool, real>::to(x <= probs[i]);
     } else {
-      float x = curand_uniform(&state[blockIdx.x]);
+      float x = hiprng_uniform(&state[hipBlockIdx_x]);
       if (i < size)
         result[i] = ScalarConvert<bool, real>::to(x <= probs[i]);
     }
   }
 }
 
-// NOTE: curand_uniform is (0, 1] and we want [a, b)
-GENERATE_KERNEL2(generate_uniform, float, float a, float b, float, curand_uniform, reverse_bounds(x) * (b-a) + a)
-GENERATE_KERNEL2(generate_uniform, double, double a, double b, double, curand_uniform_double, reverse_bounds(x) * (b-a) + a)
+GENERATE_KERNEL2(generate_uniform, float, double a, double b, float, hiprng_uniform, x * (b-a) + a)
+GENERATE_KERNEL2(generate_uniform, double, double a, double b, double, hiprng_uniform_double, x * (b-a) + a)
 
-GENERATE_KERNEL2(generate_normal, float, double mean, double stdv, float, curand_normal, (x * stdv) + mean)
-GENERATE_KERNEL2(generate_normal, double, double mean, double stdv, double, curand_normal_double, (x * stdv) + mean)
+GENERATE_KERNEL2(generate_normal, float, double mean, double stdv, float, hiprng_normal, (x * stdv) + mean)
+GENERATE_KERNEL2(generate_normal, double, double mean, double stdv, double, hiprng_normal_double, (x * stdv) + mean)
 
-GENERATE_KERNEL1(generate_exponential, float, double lambda, float, curand_uniform, (float)(-1. / lambda * log(x)))
-GENERATE_KERNEL1(generate_exponential, double, double lambda, double, curand_uniform_double, (double)(-1. / lambda * log(x)))
+GENERATE_KERNEL1(generate_exponential, float, double lambda, float, hiprng_uniform, (float)(-1. / lambda * log(1-x)))
+GENERATE_KERNEL1(generate_exponential, double, double lambda, double, hiprng_uniform_double, (double)(-1. / lambda * log(1-x)))
 
-GENERATE_KERNEL2(generate_cauchy, float, double median, double sigma, float, curand_uniform, (float)(median + sigma * tan(M_PI*(x-0.5))))
-GENERATE_KERNEL2(generate_cauchy, double, double median, double sigma, double, curand_uniform_double, (double)(median + sigma * tan(M_PI*(x-0.5))))
+GENERATE_KERNEL2(generate_cauchy, float, double median, double sigma, float, hiprng_uniform, (float)(median + sigma * tan(M_PI*(x-0.5))))
+GENERATE_KERNEL2(generate_cauchy, double, double median, double sigma, double, hiprng_uniform_double, (double)(median + sigma * tan(M_PI*(x-0.5))))
 
 #ifdef CUDA_HALF_TENSOR
-GENERATE_KERNEL2(generate_uniform, half, double a, double b, float, curand_uniform, (half_uniform_scale_and_shift(x, a, b)))
-GENERATE_KERNEL2(generate_normal, half, double mean, double stdv, float, curand_normal, (ScalarConvert<float, half>::to((x * stdv) + mean)))
-GENERATE_KERNEL1(generate_exponential, half, double lambda, float, curand_uniform, (ScalarConvert<float, half>::to((float)(-1. / lambda * log(x)))))
-GENERATE_KERNEL2(generate_cauchy, half, double median, double sigma, float, curand_uniform, (ScalarConvert<float, half>::to((float)(median + sigma * tan(M_PI*(x-0.5))))))
+GENERATE_KERNEL2(generate_uniform, half, double a, double b, float, hiprng_uniform, (half_uniform_scale_and_shift(x, a, b)))
+GENERATE_KERNEL2(generate_normal, half, double mean, double stdv, float, hiprng_normal, (ScalarConvert<float, half>::to((x * stdv) + mean)))
+GENERATE_KERNEL1(generate_exponential, half, double lambda, float, hiprng_uniform, (ScalarConvert<float, half>::to((float)(-1. / lambda * log(1-x)))))
+GENERATE_KERNEL2(generate_cauchy, half, double median, double sigma, float, hiprng_uniform, (ScalarConvert<float, half>::to((float)(median + sigma * tan(M_PI*(x-0.5))))))
 #endif // CUDA_HALF_TENSOR
 
+
 #include "generic/THCTensorRandom.cu"
 #include "THCGenerateAllTypes.h"
 
diff --git a/aten/src/THC/THCTensorRandom.h b/aten/src/THC/THCTensorRandom.h
index 21fe6d942..43baaa22c 100644
--- a/aten/src/THC/THCTensorRandom.h
+++ b/aten/src/THC/THCTensorRandom.h
@@ -8,7 +8,7 @@
 
 /* Generator */
 typedef struct _Generator {
-  struct curandStateMtgp32* gen_states;
+  struct hcrngStateMtgp32* gen_states;
   struct mtgp32_kernel_params *kernel_params;
   int initf;
   uint64_t initial_seed;
@@ -33,6 +33,6 @@ THC_API uint64_t THCRandom_initialSeed(struct THCState *state);
 THC_API void THCRandom_getRNGState(struct THCState *state, THByteTensor *rng_state);
 THC_API void THCRandom_setRNGState(struct THCState *state, THByteTensor *rng_state);
 
-THC_API struct curandStateMtgp32* THCRandom_generatorStates(struct THCState* state);
+THC_API struct hcrngStateMtgp32* THCRandom_generatorStates(struct THCState* state);
 
 #endif
diff --git a/aten/src/THC/THCTensorTypeUtils.cuh b/aten/src/THC/THCTensorTypeUtils.cuh
index 78bea9746..70368343e 100644
--- a/aten/src/THC/THCTensorTypeUtils.cuh
+++ b/aten/src/THC/THCTensorTypeUtils.cuh
@@ -143,6 +143,51 @@ struct ScalarInv {
   static __host__ __device__ T to(const T v) { return ((T) 1) / v; }
 };
 
+#if defined(__HIP_PLATFORM_HCC__)
+    template <>
+    struct ScalarNegate<half> {
+      __host__
+      static
+      half to(half v)
+      {
+          return -v;
+      }
+
+      __device__
+      static
+      half to(half v)
+      {
+          return -v;
+      }
+    };
+
+    template <>
+    struct ScalarInv<half> {
+      __host__
+      static
+      half to(half v)
+      {
+        float fv = THC_half2float(v);
+        fv = 1.0f / fv;
+        return THC_float2half(fv);
+      }
+
+      __device__
+      static
+      half to(half v)
+      {
+          return static_cast<half>(1) / v;
+      }
+    };
+
+// inline bool operator==(half a, half b) {
+//   return a == b;
+// }
+// 
+// inline bool operator!=(half a, half b) {
+//   return a != b;
+// }
+#else
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct ScalarNegate<half> {
@@ -201,5 +246,6 @@ inline bool operator!=(half a, half b) {
 }
 
 #endif // CUDA_HALF_TENSOR
+#endif
 
 #endif // THC_TENSOR_TYPE_UTILS_INC
diff --git a/aten/src/THC/generic/THCTensorRandom.cu b/aten/src/THC/generic/THCTensorRandom.cu
index ce49c5c88..ecf24e28e 100644
--- a/aten/src/THC/generic/THCTensorRandom.cu
+++ b/aten/src/THC/generic/THCTensorRandom.cu
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 #ifndef THC_GENERIC_FILE
 #define THC_GENERIC_FILE "generic/THCTensorRandom.cu"
 #else
@@ -5,7 +6,6 @@
 #define NUM_BLOCKS min((int)THCCeilDiv(size, (ptrdiff_t) BLOCK_SIZE), MAX_NUM_BLOCKS)
 
 #if defined(THC_REAL_IS_FLOAT) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_HALF)
-
 THC_API void THCTensor_(uniform)(THCState* state, THCTensor *self_, double a, double b)
 {
   THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
@@ -15,8 +15,8 @@ THC_API void THCTensor_(uniform)(THCState* state, THCTensor *self_, double a, do
   THCTensor *self = THCTensor_(newContiguous)(state, self_);
   real *data = THCTensor_(data)(state, self);
 
-  generate_uniform<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, a, b);
+  hipLaunchKernelGGL((generate_uniform), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, a, b);
 
   THCTensor_(freeCopyTo)(state, self, self_);
 };
@@ -30,8 +30,8 @@ THC_API void THCTensor_(normal)(THCState* state, THCTensor *self_, double mean,
   THCTensor *self = THCTensor_(newContiguous)(state, self_);
   real *data = THCTensor_(data)(state, self);
 
-  generate_normal<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, mean, stdv);
+  hipLaunchKernelGGL((generate_normal), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, mean, stdv);
 
   THCTensor_(freeCopyTo)(state, self, self_);
 };
@@ -69,8 +69,8 @@ THC_API void THCTensor_(logNormal)(THCState* state, THCTensor *self_, double mea
   THCTensor *self = THCTensor_(newContiguous)(state, self_);
   real *data = THCTensor_(data)(state, self);
 
-  generateLogNormal<real><<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, mean, stdv);
+  hipLaunchKernelGGL((generateLogNormal<real>), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, mean, stdv);
 
   THCTensor_(freeCopyTo)(state, self, self_);
 };
@@ -85,8 +85,8 @@ THC_API void THCTensor_(exponential)(THCState* state, THCTensor *self_, double l
   THCTensor *self = THCTensor_(newContiguous)(state, self_);
   real *data = THCTensor_(data)(state, self);
 
-  generate_exponential<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, lambda);
+  hipLaunchKernelGGL((generate_exponential), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, lambda);
 
   THCTensor_(freeCopyTo)(state, self, self_);
 };
@@ -101,8 +101,8 @@ THC_API void THCTensor_(cauchy)(THCState* state, THCTensor *self_, double median
   THCTensor *self = THCTensor_(newContiguous)(state, self_);
   real *data = THCTensor_(data)(state, self);
 
-  generate_cauchy<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, median, sigma);
+  hipLaunchKernelGGL((generate_cauchy), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, median, sigma);
 
   THCTensor_(freeCopyTo)(state, self, self_);
 };
@@ -110,10 +110,10 @@ THC_API void THCTensor_(cauchy)(THCState* state, THCTensor *self_, double median
 void THCTensor_(renormRows)(struct THCState* state,
                              THCTensor* t) {
   THAssert(THCTensor_(nDimension)(state, t) == 2);
-  int64_t rows = THCTensor_(size)(state, t, 0);
-  int64_t cols = THCTensor_(size)(state, t, 1);
+  long rows = THCTensor_(size)(state, t, 0);
+  long cols = THCTensor_(size)(state, t, 1);
 
-  cudaDeviceProp* props = THCState_getCurrentDeviceProperties(state);
+  hipDeviceProp_t* props = THCState_getCurrentDeviceProperties(state);
   THAssert(props != NULL);
 
   int numSM = props->multiProcessorCount;
@@ -122,9 +122,7 @@ void THCTensor_(renormRows)(struct THCState* state,
   dim3 grid(rows < numSM * 4 ? rows : numSM * 4);
   dim3 block(cols < maxThreads ? cols : maxThreads);
 
-  renormRowsL1<real>
-    <<<grid, block, block.x * sizeof(real),
-    THCState_getCurrentStream(state)>>>(THCTensor_(data)(state, t),
+  hipLaunchKernelGGL((renormRowsL1<real>), dim3(grid), dim3(block), block.x * sizeof(real), THCState_getCurrentStream(state), THCTensor_(data)(state, t),
                                         rows, cols);
 }
 
@@ -142,9 +140,9 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
              "prob_dist must be 1 or 2 dim");
 
   // Categories are in the innermost dimension
-  int64_t numDist =
+  long numDist =
     inputSize == 1 ? 1 : THCTensor_(size)(state, prob_dist, 0);
-  int64_t numCategoriesLong =
+  long numCategoriesLong =
     inputSize == 1 ? THCTensor_(size)(state, prob_dist, 0) :
     THCTensor_(size)(state, prob_dist, 1);
 
@@ -162,20 +160,19 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
                "replacement");
   }
 
-  int free_prob_dist = 0;
+  // It is possible that prob_dist is non-contiguous
+  THCTensor* probDistContig =
+    THCTensor_(newContiguous)(state, prob_dist);
 
   // Restructure data for 2d
   if (inputSize == 1) {
-    THCTensor *temp = THCTensor_(new)(state);
-    THCTensor_(unsqueeze1d)(state, temp, prob_dist, 0);
-    prob_dist = temp;
-    free_prob_dist = 1;
+    THCTensor_(resize2d)(state, probDistContig, 1, numCategories);
   }
 
   THCudaLongTensor_resize2d(state, self, numDist, n_sample);
 
   // get current device properties
-  cudaDeviceProp* props = THCState_getCurrentDeviceProperties(state);
+  hipDeviceProp_t* props = THCState_getCurrentDeviceProperties(state);
   THAssert(props != NULL);
   int numSM = props->multiProcessorCount;
   int maxThreads = props->maxThreadsPerBlock;
@@ -194,18 +191,12 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
     dim3 block(numCategories < maxThreads ? numCategories : maxThreads);
     dim3 grid(numDist < numSM * 4 ? numDist : numSM * 4);
 
-    sampleMultinomialOnce<real, accreal>
-      <<<grid, block,
-         requiredShared,
-         THCState_getCurrentStream(state)>>>(
+    hipLaunchKernelGGL((sampleMultinomialOnce<real, accreal>), dim3(grid), dim3(block), requiredShared, THCState_getCurrentStream(state),
       THCudaLongTensor_data(state, self),
       numDist,
       numCategories,
       THCTensor_(data)(state, sampled),
-      THCTensor_(data)(state, prob_dist),
-      THCTensor_(stride)(state, prob_dist, 0),
-      THCTensor_(stride)(state, prob_dist, 1)
-      );
+      THCTensor_(data)(state, probDistContig));
     THCTensor_(free)(state, sampled);
   } else {
     // Generic, slow implementation with memory allocations
@@ -213,11 +204,11 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
     // For sampling without replacement, we modify the distribution
     // for subsequent samples in this space
     THCTensor* origDist = THCTensor_(new)(state);
-    THCTensor_(resizeAs)(state, origDist, prob_dist);
-    THCTensor_(copy)(state, origDist, prob_dist);
+    THCTensor_(resizeAs)(state, origDist, probDistContig);
+    THCTensor_(copy)(state, origDist, probDistContig);
 
     THCTensor* normDist = THCTensor_(new)(state);
-    THCTensor_(resizeAs)(state, normDist, prob_dist);
+    THCTensor_(resizeAs)(state, normDist, probDistContig);
 
     THCTensor* prefixSum = THCTensor_(new)(state);
 
@@ -240,8 +231,7 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
       // distribution concurrently.
       dim3 grid(numDist < MAX_NUM_BLOCKS ? numDist : MAX_NUM_BLOCKS);
 
-      sampleMultinomialWithReplacement
-        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+      hipLaunchKernelGGL((sampleMultinomialWithReplacement), dim3(grid), dim3(block), 0, THCState_getCurrentStream(state),
           gen->gen_states,
           n_sample,
           THCudaLongTensor_data(state, self),
@@ -257,7 +247,7 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
 
       // Each warp in a block will generate a sample from a different
       // distribution concurrently.
-      ptrdiff_t numBlocks = THCCeilDiv(numDist, (int64_t) 4);
+      ptrdiff_t numBlocks = THCCeilDiv(numDist, 4L);
       dim3 grid(numBlocks < MAX_NUM_BLOCKS ? numBlocks : MAX_NUM_BLOCKS);
 
       for (int sample = 0; sample < n_sample; ++sample) {
@@ -273,8 +263,7 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
 
         // The kernel can only draw one sample before we have to
         // recalculate our distribution
-        sampleMultinomialWithoutReplacement
-          <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+        hipLaunchKernelGGL((sampleMultinomialWithoutReplacement), dim3(grid), dim3(block), 0, THCState_getCurrentStream(state),
             gen->gen_states,
             n_sample,
             sample,
@@ -293,17 +282,21 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
   // Revert data restructuring based on input sizes
   if (inputSize == 1) {
     THCudaLongTensor_resize1d(state, self, n_sample);
+
+    // Unfortunately, if prob_dist is contiguous already,
+    // newContiguous is not a private copy, so we have to restructure
+    // this too, so as to not affect prob_dist
+    THCTensor_(resize1d)(state, probDistContig, numCategories);
   }
-  if (free_prob_dist) {
-    THCTensor_(free)(state, prob_dist);
-  }
+
+  THCTensor_(free)(state, probDistContig);
 }
 
 THC_API void THCTensor_(multinomialAliasSetup)(THCState *state, THCTensor *_probs, THCudaLongTensor *_J, THCTensor *_q){
   THAssert(THCTensor_(isContiguous)(state, _q));
   THAssert(THCudaLongTensor_isContiguous(state, _J));
   THAssert(THCTensor_(isContiguous)(state, _probs));
-  int64_t inputsize = THCTensor_(nElement)(state, _probs);
+  long inputsize = THCTensor_(nElement)(state, _probs);
   THCudaLongTensor *smaller = THCudaLongTensor_newWithSize1d(state, inputsize);
   THCudaLongTensor *larger = THCudaLongTensor_newWithSize1d(state, inputsize);
   THCudaLongTensor *smaller_short = THCudaLongTensor_newWithSize1d(state, inputsize);
@@ -312,41 +305,38 @@ THC_API void THCTensor_(multinomialAliasSetup)(THCState *state, THCTensor *_prob
   THCudaLongTensor_resize1d(state, _J, inputsize);
   THCTensor_(resize1d)(state, _q, inputsize);
 
-  real one = ScalarConvert<int64_t, real>::to(1);
+  real one = ScalarConvert<long, real>::to(1);
   int inputBlockDim = THCCeilDiv((int)inputsize + BLOCK_SIZE - 1, BLOCK_SIZE);
-  aliasMultinomialFilter
-    <<<inputBlockDim, BLOCK_SIZE, 0, THCState_getCurrentStream(state) >>>(
-                     THCTensor_(data)(state, _q),
-                     THCTensor_(data)(state, _probs),
-                     THCudaLongTensor_data(state, smaller),
-                     THCudaLongTensor_data(state, larger),
-                     THCudaLongTensor_data(state, _J),
-                     THCudaLongTensor_data(state, smaller_short),
-                     THCudaLongTensor_data(state, larger_short),
-                     one, inputsize
-                     );
+  hipLaunchKernelGGL((aliasMultinomialFilter), dim3(inputBlockDim), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state) ,
+								     THCTensor_(data)(state, _q),
+								     THCTensor_(data)(state, _probs),
+								     THCudaLongTensor_data(state, smaller),
+								     THCudaLongTensor_data(state, larger),
+								     THCudaLongTensor_data(state, _J),
+								     THCudaLongTensor_data(state, smaller_short),
+								     THCudaLongTensor_data(state, larger_short),
+								     one, inputsize
+								     );
 
   THCudaLongTensor_nonzero(state, smaller_short, smaller);
   THCudaLongTensor_nonzero(state, larger_short, larger);
   int h_large_c = THCudaLongTensor_nElement(state, larger_short);
   THCudaLongTensor_resize1d(state, smaller_short, inputsize);
   THCudaLongTensor_resize1d(state, larger_short, inputsize);
-  aliasMultinomialSetup
-    <<<1, 1, 0, THCState_getCurrentStream(state)>>>(
-                THCudaLongTensor_data(state, _J),
-                THCTensor_(data)(state, _q),
-                inputsize,
-                THCudaLongTensor_data(state, smaller_short),
-                THCudaLongTensor_data(state, larger_short),
-                inputsize - h_large_c, h_large_c
-                );
+  hipLaunchKernelGGL((aliasMultinomialSetup), dim3(1), dim3(1), 0, THCState_getCurrentStream(state),
+						    THCudaLongTensor_data(state, _J),
+						    THCTensor_(data)(state, _q),
+						    inputsize,
+						    THCudaLongTensor_data(state, smaller_short),
+						    THCudaLongTensor_data(state, larger_short),
+						    static_cast<int>(inputsize - h_large_c), h_large_c
+						    );
   real q_max = THCTensor_(maxall)(state, _q);
-  condDiv<<<
-    inputBlockDim, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-                      THCTensor_(data)(state, _q),
-                      THCudaLongTensor_data(state, _J),
-                      inputsize, q_max
-                      );
+  hipLaunchKernelGGL((condDiv), dim3(inputBlockDim), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+								      THCTensor_(data)(state, _q),
+								      THCudaLongTensor_data(state, _J),
+								      inputsize, q_max
+								      );
 
   THCudaLongTensor_free(state, smaller);
   THCudaLongTensor_free(state, larger);
@@ -358,8 +348,8 @@ THC_API void THCTensor_(multinomialAliasDraw)(THCState *state, THCudaLongTensor
   THAssert(THCTensor_(isContiguous)(state, _q));
   THAssert(THCudaLongTensor_isContiguous(state, _J));
   THCGenerator* gen = THCRandom_getGenerator(state);
-  int64_t K = THCudaLongTensor_nElement(state, _J);
-  int64_t output_nelem = THCudaLongTensor_nElement(state, self);
+  long K = THCudaLongTensor_nElement(state, _J);
+  long output_nelem = THCudaLongTensor_nElement(state, self);
   ptrdiff_t size = THCudaLongTensor_nElement(state, self);
 
   THCTensor *uniform = THCTensor_(newWithSize1d)(state, output_nelem);
@@ -368,16 +358,15 @@ THC_API void THCTensor_(multinomialAliasDraw)(THCState *state, THCudaLongTensor
   THCTensor_(uniform)(state, uniform, 0, K);
   THCTensor_(uniform)(state, bernoulli, 0, 1);
 
-  multinomialAliasDrawKernel
-    <<<THCCeilDiv((int)output_nelem+BLOCK_SIZE-1, BLOCK_SIZE), BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-          size,
-          THCudaLongTensor_data(state, self),
-          THCudaLongTensor_data(state, _J),
-          THCTensor_(data)(state, _q),
-          K,
-          THCTensor_(data)(state, uniform),
-          THCTensor_(data)(state, bernoulli)
-          );
+  hipLaunchKernelGGL((multinomialAliasDrawKernel), dim3(THCCeilDiv((int)output_nelem+BLOCK_SIZE-1, BLOCK_SIZE)), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+				  static_cast<int>(size),
+				  THCudaLongTensor_data(state, self),
+				  THCudaLongTensor_data(state, _J),
+				  THCTensor_(data)(state, _q),
+				  K,
+				  THCTensor_(data)(state, uniform),
+				  THCTensor_(data)(state, bernoulli)
+				  );
 }
 
 THC_API void THCTensor_(rand)(THCState *state, THCTensor *r_, THLongStorage *size)
@@ -397,9 +386,9 @@ void THCTensor_(randn)(THCState *state, THCTensor *r_, THLongStorage *size)
 #endif
 
 #if defined(THC_REAL_IS_DOUBLE)
-GENERATE_KERNEL1(generate_bernoulli, double, double p, double, curand_uniform_double, x <= p)
+GENERATE_KERNEL1(generate_bernoulli, double, double p, double, hiprng_uniform_double, x <= p)
 #else
-GENERATE_KERNEL1(generate_bernoulli, real, double p, float, curand_uniform, (ScalarConvert<bool, real>::to(x <= p)))
+GENERATE_KERNEL1(generate_bernoulli, real, double p, float, hiprng_uniform, (ScalarConvert<bool, real>::to(x <= p)))
 #endif
 
 THC_API void THCTensor_(bernoulli)(THCState* state, THCTensor *self_, double p)
@@ -411,65 +400,87 @@ THC_API void THCTensor_(bernoulli)(THCState* state, THCTensor *self_, double p)
   THCTensor *self = THCTensor_(newContiguous)(state, self_);
   real *data = THCTensor_(data)(state, self);
 
-  generate_bernoulli<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, p);
+  hipLaunchKernelGGL((generate_bernoulli), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, p);
 
   THCTensor_(freeCopyTo)(state, self, self_);
 };
 
 void THCTensor_(bernoulli_Tensor)(THCState *state, THCTensor *self, THCTensor* p)
 {
-#if defined(THC_REAL_IS_FLOAT)
-  THCTensor_(bernoulli_FloatTensor)(state, self, p);
-#elif defined(THC_REAL_IS_DOUBLE)
-  THCTensor_(bernoulli_DoubleTensor)(state, self, p);
-#endif
+ #if defined(THC_REAL_IS_FLOAT)
+   THCTensor_(bernoulli_FloatTensor)(state, self, p);
+ #elif defined(THC_REAL_IS_DOUBLE)
+   THCTensor_(bernoulli_DoubleTensor)(state, self, p);
+ #endif
 }
 
-#define DEFINE_BERNOULLI_TENSOR(NAME, PROB_TYPE, PROB_DATA_TYPE)               \
-THC_API void THCTensor_(NAME)(THCState* state,                                 \
-        THCTensor *self_, PROB_TYPE *probs_)                                   \
-{                                                                              \
-  THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, self_, probs_));             \
-  ptrdiff_t size = THCTensor_(nElement)(state, self_);                         \
-  if (size == 0) return;                                                       \
-  THCGenerator* gen = THCRandom_getGenerator(state);                           \
-  THCTensor *self = THCTensor_(newContiguous)(state, self_);                   \
-  PROB_TYPE *probs = PROB_TYPE##_newContiguous(state, probs_);                 \
-  ptrdiff_t prob_size = PROB_TYPE##_nElement(state, probs);                    \
-  real *result_data = THCTensor_(data)(state, self);                           \
-  PROB_DATA_TYPE *probs_data = PROB_TYPE##_data(state, probs);                 \
-                                                                               \
-  THArgCheck(size == prob_size, 3, "inconsistent tensor size");                \
-                                                                               \
-  generate_bernoulli_tensor<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>( \
-      gen->gen_states, size, result_data, probs_data);                         \
-                                                                               \
-  PROB_TYPE##_free(state, probs);                                              \
-  THCTensor_(freeCopyTo)(state, self, self_);                                  \
-}
+#if defined(__HIP_PLATFORM_HCC__)
+  #define DEFINE_BERNOULLI_TENSOR(NAME, PROB_TYPE, PROB_DATA_TYPE)               \
+  THC_API void THCTensor_(NAME)(THCState* state,                                 \
+          THCTensor *self_, PROB_TYPE *probs_)                                   \
+  {                                                                              \
+    THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, self_, probs_));             \
+    ptrdiff_t size = THCTensor_(nElement)(state, self_);                         \
+    if (size == 0) return;                                                       \
+    THCGenerator* gen = THCRandom_getGenerator(state);                           \
+    THCTensor *self = THCTensor_(newContiguous)(state, self_);                   \
+    PROB_TYPE *probs = PROB_TYPE##_newContiguous(state, probs_);                 \
+    ptrdiff_t prob_size = PROB_TYPE##_nElement(state, probs);                    \
+    real *result_data = THCTensor_(data)(state, self);                           \
+    PROB_DATA_TYPE *probs_data = PROB_TYPE##_data(state, probs);                 \
+                                                                                 \
+    THArgCheck(size == prob_size, 3, "inconsistent tensor size");                \
+                                                                                 \
+    hipLaunchKernelGGL(                                                          \
+      (generate_bernoulli_tensor), NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state), \
+        gen->gen_states, static_cast<int>(size), result_data, probs_data);       \
+                                                                                 \
+    PROB_TYPE##_free(state, probs);                                              \
+    THCTensor_(freeCopyTo)(state, self, self_);                                  \
+  }
+#else
+  #define DEFINE_BERNOULLI_TENSOR(NAME, PROB_TYPE, PROB_DATA_TYPE)               \
+  THC_API void THCTensor_(NAME)(THCState* state,                                 \
+          THCTensor *self_, PROB_TYPE *probs_)                                   \
+  {                                                                              \
+    THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, self_, probs_));             \
+    ptrdiff_t size = THCTensor_(nElement)(state, self_);                         \
+    if (size == 0) return;                                                       \
+    THCGenerator* gen = THCRandom_getGenerator(state);                              \
+    THCTensor *self = THCTensor_(newContiguous)(state, self_);                   \
+    PROB_TYPE *probs = PROB_TYPE##_newContiguous(state, probs_);                 \
+    ptrdiff_t prob_size = PROB_TYPE##_nElement(state, probs);                    \
+    real *result_data = THCTensor_(data)(state, self);                           \
+    PROB_DATA_TYPE *probs_data = PROB_TYPE##_data(state, probs);                 \
+                                                                                 \
+    THArgCheck(size == prob_size, 3, "inconsistent tensor size");                \
+                                                                                 \
+    hipLaunchKernelGGL((generate_bernoulli_tensor), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),  \
+        gen->gen_states, size, result_data, probs_data);                         \
+                                                                                 \
+    PROB_TYPE##_free(state, probs);                                              \
+    THCTensor_(freeCopyTo)(state, self, self_);                                  \
+  }
+#endif
 
 DEFINE_BERNOULLI_TENSOR(bernoulli_FloatTensor, THCudaTensor, float)
 DEFINE_BERNOULLI_TENSOR(bernoulli_DoubleTensor, THCudaDoubleTensor, double)
 
 #if defined(THC_REAL_IS_DOUBLE)
-GENERATE_KERNEL1(generate_geometric, double, double p, double, curand_uniform_double, ceil(log(x) / log(1-p)))
+GENERATE_KERNEL1(generate_geometric, double, double p, double, hiprng_uniform_double, ceil(log(x) / log(1-p)))
 #else
-GENERATE_KERNEL1(generate_geometric, real, double p, float, curand_uniform, (ScalarConvert<float, real>::to(ceilf(logf(x) / log(1-p)))))
+GENERATE_KERNEL1(generate_geometric, real, double p, float, hiprng_uniform, (ScalarConvert<float, real>::to(ceilf(logf(x) / log(1-p)))))
 #endif
 
 #if defined(THC_REAL_IS_LONG) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_FLOAT)
-#define CURAND64(STATE) (((uint64_t)curand(STATE)) << 32) | (uint64_t)curand(STATE)
-GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, curand, \
-    static_cast<real>(static_cast<int32_t>((x % range) + base)))
-GENERATE_KERNEL2(generate_random_64, real, int64_t base, uint64_t range, uint64_t, CURAND64, \
-    static_cast<real>(static_cast<int64_t>((x % range) + base)))
+#define CURAND64(STATE) (((uint64_t) hiprng(&state[blockIdx.x])) << 32) | (uint64_t) hiprng(&state[blockIdx.x])
+GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, hiprng, (real)(x % range + base))
+GENERATE_KERNEL2(generate_random_64, real, int64_t base, uint64_t range, uint64_t, CURAND64, (real)(x % range + base))
 #elif defined(THC_REAL_IS_HALF)
-GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, curand,
-    (ScalarConvert<int32_t, real>::to(static_cast<int32_t>(x % range + base))))
+GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, hiprng, (ScalarConvert<uint32_t, real>::to(x % range + base)))
 #else
-GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, curand,
-    static_cast<real>(static_cast<int32_t>(x % range + base)))
+GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, hiprng, (real)(x % range + base))
 #endif
 
 THC_API void THCTensor_(geometric)(THCState* state, THCTensor *self_, double p)
@@ -482,8 +493,8 @@ THC_API void THCTensor_(geometric)(THCState* state, THCTensor *self_, double p)
   THCTensor *self = THCTensor_(newContiguous)(state, self_);
   real *data = THCTensor_(data)(state, self);
 
-  generate_geometric<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, p);
+  hipLaunchKernelGGL((generate_geometric), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, p);
 
   THCTensor_(freeCopyTo)(state, self, self_);
 };
@@ -503,12 +514,12 @@ THC_API void THCTensor_(clampedRandom)(THCState* state, THCTensor *self_, int64_
 
 #if defined(THC_REAL_IS_LONG) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_FLOAT)
   if (range > 1ULL << 32) {
-    generate_random_64<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-        gen->gen_states, size, data, min_val, range);
+    hipLaunchKernelGGL((generate_random_64), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+        gen->gen_states, static_cast<int>(size), data, static_cast<int64_t>(min_val), static_cast<uint64_t>(range));
   } else {
 #endif
-    generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-        gen->gen_states, size, data, min_val, range);
+    hipLaunchKernelGGL((generate_random), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+        gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(min_val), static_cast<uint32_t>(range));
 #if defined(THC_REAL_IS_LONG) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_FLOAT)
   }
 #endif
@@ -533,27 +544,24 @@ THC_API void THCTensor_(random)(THCState* state, THCTensor *self_)
   real *data = THCTensor_(data)(state, self);
 
 #if defined(THC_REAL_IS_HALF)
-  generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, 0UL, (1UL << HLF_MANT_DIG) + 1);
+  hipLaunchKernelGGL((generate_random), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(0UL), static_cast<uint32_t>((1UL << HLF_MANT_DIG) + 1));
 #elif defined(THC_REAL_IS_FLOAT)
-  generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, 0UL, (1UL << FLT_MANT_DIG) + 1);
+  hipLaunchKernelGGL((generate_random), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(0UL), static_cast<uint32_t>((1UL << FLT_MANT_DIG) + 1));
 #elif defined(THC_REAL_IS_DOUBLE)
-  generate_random_64<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, 0ULL, (1ULL << DBL_MANT_DIG) + 1);
+  hipLaunchKernelGGL((generate_random_64), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, static_cast<int64_t>(0ULL), static_cast<uint64_t>((1ULL << DBL_MANT_DIG) + 1));
 #elif defined(THC_REAL_IS_LONG)
-  generate_random_64<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, 0ULL, static_cast<uint64_t>(std::numeric_limits<real>::max()) + 1);
+  hipLaunchKernelGGL((generate_random_64), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, static_cast<int64_t>(0ULL), static_cast<uint64_t>(std::numeric_limits<real>::max()) + 1);
 #else
-  generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
-      gen->gen_states, size, data, 0UL, static_cast<uint32_t>(std::numeric_limits<real>::max()) + 1);
+  hipLaunchKernelGGL((generate_random), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
+      gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(0UL), static_cast<uint32_t>(std::numeric_limits<real>::max()) + 1);
 #endif
 
   THCTensor_(freeCopyTo)(state, self, self_);
 };
-
-#undef HLF_MANT_DIG
-#undef CURAND64
 #undef NUM_BLOCKS
 
 #endif
diff --git a/aten/src/THC/generic/THCTensorSort.cu b/aten/src/THC/generic/THCTensorSort.cu
index 06ed71f82..1b3b0374d 100644
--- a/aten/src/THC/generic/THCTensorSort.cu
+++ b/aten/src/THC/generic/THCTensorSort.cu
@@ -223,39 +223,45 @@ void sortViaThrust(THCState* state,
   // Fill the indices with a global index across all slices
   thrust::counting_iterator<int64_t> countIter(0);
 
+#if defined (__NVCC__)
   thrust::copy(
 #if CUDA_VERSION >= 7000
     thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
     countIter, countIter + totalElements, indexIter);
-
+#endif
   // First, we sort globally (across all slices) according to key
   // (the values we're sorting)
   if (dir) {
+#if defined (__NVCC__)
     thrust::stable_sort_by_key(
 #if CUDA_VERSION >= 7000
       thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
       keyIter, keyIter + totalElements, indexIter, ThrustGTOp<real>());
+#endif
   } else {
+#if defined (__NVCC__)
     thrust::stable_sort_by_key(
 #if CUDA_VERSION >= 7000
       thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
       keyIter, keyIter + totalElements, indexIter, ThrustLTOp<real>());
+#endif
   }
 
   // Then, re-sort according to slice that each index is
   // in. This completes the segment sort in Thrust, since we're
   // stably sorting here, preserving the relative order of values
   // per each slice
+#if defined (__NVCC__)
   thrust::stable_sort_by_key(
 #if CUDA_VERSION >= 7000
     thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
     indexIter, indexIter + totalElements, keyIter,
     SliceComp(sliceSize));
-
+#endif
   // Translate the global integer 0-based index to a per-slice real
   // Lua index
   thrust::for_each(
diff --git a/aten/src/THCS/generic/THCSTensorMath.cu b/aten/src/THCS/generic/THCSTensorMath.cu
index 319652fdf..181ab7634 100644
--- a/aten/src/THCS/generic/THCSTensorMath.cu
+++ b/aten/src/THCS/generic/THCSTensorMath.cu
@@ -173,13 +173,16 @@ void THCSTensor_(sspaddmm)(THCState *state, THCSTensor *r_, real beta, THCSTenso
 }
 
 void THCSTensor_(hspmm)(THCState *state, THCSTensor *r_, real alpha, THCSTensor *sparse_, THCTensor *dense) {
+#if defined(__HIP_PLATFORM_HCC__)
+  #define THRUST_EXEC(fn, ...) // whitespace
+#else
 #if CUDA_VERSION >= 7000
   THCThrustAllocator thrustAlloc(state);
 #define THRUST_EXEC(fn, ...) fn(thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)), ##__VA_ARGS__)
 #else
 #define THRUST_EXEC(fn, ...) fn(##__VA_ARGS__)
 #endif
-
+#endif
   THCAssertSameGPU(THCSTensor_(checkGPU)(state, 2, 3, r_, sparse_, dense));
 
   THArgCheck(sparse_->nDimensionI == 2, 3,
@@ -507,7 +510,7 @@ void THCSTensor_(pow)(THCState *state, THCSTensor *r_, THCSTensor *t_, real valu
 #if defined(THCS_REAL_IS_FLOAT) || defined(THCS_REAL_IS_DOUBLE) || defined(THCS_REAL_IS_HALF)
 accreal THCSTensor_(normall)(THCState *state, THCSTensor *self, real value) {
   THCSTensor* self_coalesced = THCSTensor_(newCoalesce)(state, self);
-  accreal result = THCTensor_(normall)(state, self_coalesced->values, value); 
+  accreal result = THCTensor_(normall)(state, self_coalesced->values, value);
   THCSTensor_(free)(state, self_coalesced);
   return result;
 }
diff --git a/aten/src/THCUNN/Abs.cu b/aten/src/THCUNN/Abs.cu
index f3c7592e2..8a5346879 100644
--- a/aten/src/THCUNN/Abs.cu
+++ b/aten/src/THCUNN/Abs.cu
@@ -8,7 +8,7 @@ struct absupdateOutput_functor
 {
   __device__ void operator()(T* output, const T* input) const
   {
-    *output = abs(*input);
+    *output = fabs(*input);
   }
 };
 
diff --git a/aten/src/THCUNN/BCECriterion.cu b/aten/src/THCUNN/BCECriterion.cu
index ccb40008c..1051a19af 100644
--- a/aten/src/THCUNN/BCECriterion.cu
+++ b/aten/src/THCUNN/BCECriterion.cu
@@ -9,6 +9,14 @@
 #include <thrust/transform.h>
 #include <thrust/transform_reduce.h>
 
+#if defined(__HIP_PLATFORM_HCC__)
+template <typename T>
+inline __host__ __device__ T eps();
+template <>
+inline __host__ __device__ float eps() { return 1e-12f; }
+template <>
+inline __host__ __device__ double eps() { return 1e-12; }
+#else
 template <typename T>
 inline __device__ T eps();
 
@@ -17,6 +25,7 @@ inline __device__ float eps() { return 1e-12f; }
 
 template <>
 inline __device__ double eps() { return 1e-12; }
+#endif
 
 template <typename Dtype, typename Acctype>
 struct bce_functor
diff --git a/aten/src/THCUNN/BatchNormalization.cu b/aten/src/THCUNN/BatchNormalization.cu
index 865323a16..0f20ac46b 100644
--- a/aten/src/THCUNN/BatchNormalization.cu
+++ b/aten/src/THCUNN/BatchNormalization.cu
@@ -6,15 +6,16 @@
 #include "THCDeviceTensor.cuh"
 #include "THCDeviceTensorUtils.cuh"
 #include "THCDeviceUtils.cuh"
-const int WARP_SIZE = 32;
+
+const int WARP_SIZE = 64;
 
 // The maximum number of threads in a block
-const int MAX_BLOCK_SIZE = 512;
+const int MAX_BLOCK_SIZE = 256;
 
 // Number of threads in a block given an input size up to MAX_BLOCK_SIZE
 static int getNumThreads(int nElem) {
-  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };
-  for (int i = 0; i != 5; ++i) {
+  int threadSizes[3] = { 64, 128, MAX_BLOCK_SIZE };
+  for (int i = 0; i != 3; ++i) {
     if (nElem <= threadSizes[i]) {
       return threadSizes[i];
     }
@@ -67,7 +68,7 @@ struct GradOp {
     : mean(m), input(i), gradOutput(g) {}
   __device__ __forceinline__ Float2<Dtype, Acctype> operator()(int batch, int plane, int n) {
     Dtype g = gradOutput[batch][plane][n];
-    Dtype c = ScalarConvert<Acctype, Dtype>::to(input[batch][plane][n] - mean);
+    Dtype c = ScalarConvert<Acctype, Dtype>::to((input[batch][plane][n]).template as<Acctype>() - mean);
     return Float2<Dtype, Acctype>(g, g * c);
   }
   const Acctype mean;
@@ -196,11 +197,12 @@ __global__ void BatchNormalizationUpdateOutput_kernel(
     Acctype unbiasedVar = varN / (N - 1);
     saveMean[plane] = ScalarConvert<Acctype, Dtype>::to(mean);
     saveStd[plane] = ScalarConvert<Acctype, Dtype>::to(invStd);
+
     if (runningMean.data() != NULL) {
-      runningMean[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * runningMean[plane] + momentum * mean);
+      runningMean[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * (runningMean[plane]).template as<Acctype>() + momentum * mean);
     }
     if (runningVar.data() != NULL) {
-      runningVar[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * runningVar[plane] + momentum * unbiasedVar);
+      runningVar[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * (runningVar[plane]).template as<Acctype>() + momentum * unbiasedVar);
     }
   }
 
@@ -240,7 +242,7 @@ __global__ void BatchNormalizationBackward_kernel(
     stdVal = ScalarConvert<Dtype, Acctype>::to(saveStd[plane]);
   } else {
     mean = ScalarConvert<Dtype, Acctype>::to(runningMean[plane]);
-    stdVal = 1 / sqrt(runningVar[plane] + eps);
+    stdVal = 1 / sqrt((runningVar[plane]).template as<Acctype>() + eps);
   }
 
   Acctype weightVal = weight.numElements() > 0 ? ScalarConvert<Dtype, Acctype>::to(weight[plane]) : Acctype(1);
@@ -275,16 +277,15 @@ __global__ void BatchNormalizationBackward_kernel(
 
   if (gradWeight.numElements() > 0) {
     if (threadIdx.x == 0) {
-      gradWeight[plane] += ScalarConvert<Acctype, Dtype>::to(scale * dotP * stdVal);
+      (gradWeight[plane]).template as<Dtype>() += ScalarConvert<Acctype, Dtype>::to(scale * dotP * stdVal);
     }
   }
 
   if (gradBias.numElements() > 0) {
     if (threadIdx.x == 0) {
-      gradBias[plane] += ScalarConvert<Acctype, Dtype>::to(scale * gradOutputSum);
+      (gradBias[plane]).template as<Dtype>() += ScalarConvert<Acctype, Dtype>::to(scale * gradOutputSum);
     }
   }
 }
-
 #include "generic/BatchNormalization.cu"
 #include "THCGenerateFloatTypes.h"
diff --git a/aten/src/THCUNN/ClassNLLCriterion.cu b/aten/src/THCUNN/ClassNLLCriterion.cu
index 1043454ff..cedd36021 100644
--- a/aten/src/THCUNN/ClassNLLCriterion.cu
+++ b/aten/src/THCUNN/ClassNLLCriterion.cu
@@ -56,7 +56,7 @@ __global__ void ClassNLLCriterion_updateOutput_no_reduce_kernel(
     assert(cur_target  >= 0 && cur_target  < n_classes);
     Dtype weight =
        weights ? weights[cur_target] : ScalarConvert<int, Dtype>::to(1);
-    output[index] = -weight * input[index][cur_target];
+    output[index] = -weight * input[index][cur_target].template as<Dtype>();
   }
 }
 
@@ -78,7 +78,7 @@ __global__ void ClassNLLCriterion_updateGradInput_no_reduce_kernel(
     assert(cur_target  >= 0 && cur_target  < n_classes);
     Dtype weight =
        weights ? weights[cur_target] : ScalarConvert<int, Dtype>::to(1);
-    gradInput[index][cur_target] = -weight * gradOutput[index];
+    gradInput[index][cur_target] = -weight * gradOutput[index].template as<Dtype>();
   }
 }
 
diff --git a/aten/src/THCUNN/FeatureLPPooling.cu b/aten/src/THCUNN/FeatureLPPooling.cu
index 4ad190fbe..08c277064 100644
--- a/aten/src/THCUNN/FeatureLPPooling.cu
+++ b/aten/src/THCUNN/FeatureLPPooling.cu
@@ -468,6 +468,7 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
       LP_STRIDE_CASE(4, WIDTH);                 \
     }
 
+#if !defined(__HIP_PLATFORM_HCC__)
   if (power == 2.0f) {
     switch (width) {
       L2_WIDTH_CASE(2);
@@ -505,6 +506,7 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
       LP_WIDTH_CASE(16);
     }
   }
+#endif
 
   // Otherwise, we have an unhandled width and/or stride.
   return false;
@@ -602,6 +604,7 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
       LP_STRIDE_CASE(4, WIDTH);                 \
     }
 
+#if !defined(__HIP_PLATFORM_HCC__)
   if (power == 2.0f) {
     switch (width) {
       L2_WIDTH_CASE(2);
@@ -639,6 +642,7 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
       LP_WIDTH_CASE(16);
     }
   }
+#endif
 
   // Otherwise, we have an unhandled width and/or stride.
   return false;
diff --git a/aten/src/THCUNN/IndexLinear.cu b/aten/src/THCUNN/IndexLinear.cu
index 2729f9277..98ac09ece 100644
--- a/aten/src/THCUNN/IndexLinear.cu
+++ b/aten/src/THCUNN/IndexLinear.cu
@@ -22,7 +22,11 @@ __device__ double atomicExch(double *address, double val) {
 }
 
 template<typename Ty, bool train>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void updateOutput(
     Ty *output,
     Ty *normalizedValues,
@@ -141,7 +145,11 @@ void updateOutput(
 // to generate gradWeight of size [keysSize x outDim]
 // nth block along y dimension computes on the non zero elements from the nth batch.
 template<typename Ty>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void accGradWeight(
     Ty *gradWeight,
     const Ty *gradOutput,
@@ -213,7 +221,11 @@ void accGradWeight(
 // The gradBias is just a reduction of gradOutput along the batches.
 // There is only one block along y dimension performing the reduction.
 template<typename Ty, bool update>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void accGradBias(
     Ty *buffer,
     const Ty *gradOutput,
@@ -267,7 +279,11 @@ void accGradBias(
 // This kernel is launched batchSize number of times.
 // At each step in the iteration, the weights are updated in a sparse manner.
 template<typename Ty>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void updateWeight(
     Ty *weight,
     const Ty *gradWeight,
@@ -336,7 +352,11 @@ void updateWeight(
 // This kernel is launched batchSize number of times.
 // At each step in the iteration, the weights are updated in place in a sparse manner.
 template<typename Ty>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void accUpdateWeight(
     Ty *weight,
     const int64_t weightStride,
diff --git a/aten/src/THCUNN/LogSigmoid.cu b/aten/src/THCUNN/LogSigmoid.cu
index 8facb5bf4..357b7bf95 100644
--- a/aten/src/THCUNN/LogSigmoid.cu
+++ b/aten/src/THCUNN/LogSigmoid.cu
@@ -3,7 +3,7 @@
 #include "THCHalfAutoNumerics.cuh"
 #include <THC/THCApply.cuh>
 
-#ifdef _MSC_VER
+#if defined(_MSC_VER) || defined(__HIP_PLATFORM_HCC__)
 #define ZERO_MACRO zero<T>()
 template <typename T>
 inline __device__ typename std::enable_if<std::is_same<T, double>::value, T>::type zero() {
diff --git a/aten/src/THCUNN/LookupTable.cu b/aten/src/THCUNN/LookupTable.cu
index 854230913..d4ed68e43 100644
--- a/aten/src/THCUNN/LookupTable.cu
+++ b/aten/src/THCUNN/LookupTable.cu
@@ -7,6 +7,7 @@
 #include "THCTensorSort.cuh"
 #include "../THC/THCTensorMathReduce.cuh"
 
+#if defined(__NVCC__)
 const int WARP_SIZE = 32;
 
 __device__ __forceinline__ bool warpHasCollision(int val)
@@ -184,14 +185,15 @@ struct FastPow<DType, AccType, 2>
     return xA * xA;
   }
 };
+#endif
 
 /* Calculate norms of the rows of weight_ptr given by idx_ptr and capture them in norms */
 template <typename DType, typename AccType, typename IndexType, int Norm>
 __global__
-void calculate_norms_and_renorm(DType *weights, 
-                                THCIndex_t *indices, 
+void calculate_norms_and_renorm(DType *weights,
+                                THCIndex_t *indices,
                                 AccType normType,
-                                AccType maxNorm, 
+                                AccType maxNorm,
                                 IndexType dim)
 {
   // Some casting hacks since dynamic shared memory and templates don't work together:
@@ -211,7 +213,7 @@ void calculate_norms_and_renorm(DType *weights,
         (sdata, blockDim.x, v, ReduceAdd<AccType, AccType>(), accZero);
 
   if (tid == 0) {
-    sdata[0] = std::pow(v, 
+    sdata[0] = std::pow(v,
         THCNumerics<AccType>::div(ScalarConvert<int, AccType>::to(1), normType)
     );
   }
diff --git a/aten/src/THCUNN/RReLU.cu b/aten/src/THCUNN/RReLU.cu
index bf4503515..4af16463a 100644
--- a/aten/src/THCUNN/RReLU.cu
+++ b/aten/src/THCUNN/RReLU.cu
@@ -2,6 +2,7 @@
 #include "THCHalf.h"
 #include "THCHalfAutoNumerics.cuh"
 #include <THC/THCApply.cuh>
+#if defined(__NVCC__)
 #include "common.h"
 #include <curand.h>
 #include <curand_kernel.h>
@@ -119,6 +120,7 @@ struct RReLUupdateGradInputEvalIP_functor
     }
   }
 };
+#endif
 
 #include "generic/RReLU.cu"
 #include "THCGenerateFloatTypes.h"
diff --git a/aten/src/THCUNN/SmoothL1Criterion.cu b/aten/src/THCUNN/SmoothL1Criterion.cu
index 8e35599ba..8b32c9246 100644
--- a/aten/src/THCUNN/SmoothL1Criterion.cu
+++ b/aten/src/THCUNN/SmoothL1Criterion.cu
@@ -70,6 +70,7 @@ struct smoothl1_updateGradInput_functor
   const Dtype norm;
   const Dtype gradOutput;
 
+  __host__ __device__
   smoothl1_updateGradInput_functor(Dtype norm_, Dtype gradOutput_)
     : norm(norm_), gradOutput(gradOutput_)
   {}
diff --git a/aten/src/THCUNN/SoftMaxCommon.cuh b/aten/src/THCUNN/SoftMaxCommon.cuh
index 692a08079..f96fee941 100644
--- a/aten/src/THCUNN/SoftMaxCommon.cuh
+++ b/aten/src/THCUNN/SoftMaxCommon.cuh
@@ -50,9 +50,13 @@ void SpatialSoftMax_getLaunchSizes(
   block = SpatialSoftMax_getBlockSize(outer_size, dim_size, inner_size);
   uint32_t block_threads = block.x * block.y;
   smem_size = block.x == 1 ? 0 : block_threads * sizeof(AccumT);
+#if defined(__HIP_PLATFORM_HCC__)
+  int max_active_blocks = 8;
+#else
   int max_active_blocks;
   cudaOccupancyMaxActiveBlocksPerMultiprocessor(&max_active_blocks,
                                                 k, block_threads, smem_size);
+#endif
   max_active_blocks *= THCState_getCurrentDeviceProperties(state)->multiProcessorCount;
   grid = SpatialSoftMax_getGridSize(block, max_active_blocks, outer_size, dim_size, inner_size);
 }
diff --git a/aten/src/THCUNN/SparseLinear.cu b/aten/src/THCUNN/SparseLinear.cu
index 9110bbcac..596c970f7 100644
--- a/aten/src/THCUNN/SparseLinear.cu
+++ b/aten/src/THCUNN/SparseLinear.cu
@@ -2,6 +2,7 @@
 #include "THCHalf.h"
 #include "THCHalfAutoNumerics.cuh"
 
+#if defined(__NVCC__)
 #include <cusparse.h>
 
 static cusparseHandle_t cusparse_handle = 0;
@@ -14,6 +15,7 @@ static void init_cusparse() {
     }
   }
 }
+#endif
 
 #ifdef CUDA_HALF_TENSOR
 void THNN_CudaHalfSparseLinear_updateOutput(
diff --git a/aten/src/THCUNN/SpatialClassNLLCriterion.cu b/aten/src/THCUNN/SpatialClassNLLCriterion.cu
index 83addd09a..80cdfb753 100644
--- a/aten/src/THCUNN/SpatialClassNLLCriterion.cu
+++ b/aten/src/THCUNN/SpatialClassNLLCriterion.cu
@@ -62,7 +62,7 @@ __global__ void SpatialClassNLLCriterion_updateGradInput_no_reduce_kernel(
     }
     Dtype value =
         -(weights ? weights[cur_target] : ScalarConvert<int, Dtype>::to(1));
-    gradInput[b][cur_target][h][w] = value * gradOutput[b][h][w];
+    gradInput[b][cur_target][h][w] = value * gradOutput[b][h][w].template as<Dtype>();
   }
 }
 
diff --git a/aten/src/THCUNN/SpatialGridSamplerBilinear.cu b/aten/src/THCUNN/SpatialGridSamplerBilinear.cu
index 3bbc10557..2e4b80864 100644
--- a/aten/src/THCUNN/SpatialGridSamplerBilinear.cu
+++ b/aten/src/THCUNN/SpatialGridSamplerBilinear.cu
@@ -88,16 +88,16 @@ __global__ void SpatialGridSamplerBilinear_updateOutput_kernel(
     for (c = 0; c < C; ++c) {
       out_val = ScalarConvert<int,Dtype>::to(0);
       if (WITHIN_BOUNDS(ix_nw, iy_nw, IH, IW)) {
-        out_val += input[n][c][iy_nw][ix_nw] * nw;
+        out_val += (input[n][c][iy_nw][ix_nw]).template as<Dtype>() * nw;
       }
       if (WITHIN_BOUNDS(ix_ne, iy_ne, IH, IW)) {
-        out_val += input[n][c][iy_ne][ix_ne] * ne;
+        out_val += (input[n][c][iy_ne][ix_ne]).template as<Dtype>() * ne;
       }
       if (WITHIN_BOUNDS(ix_sw, iy_sw, IH, IW)) {
-        out_val += input[n][c][iy_sw][ix_sw] * sw;
+        out_val += (input[n][c][iy_sw][ix_sw]).template as<Dtype>() * sw;
       }
       if (WITHIN_BOUNDS(ix_se, iy_se, IH, IW)) {
-        out_val += input[n][c][iy_se][ix_se] * se;
+        out_val += (input[n][c][iy_se][ix_se]).template as<Dtype>() * se;
       }
       output[n][c][h][w] = out_val;
     }
diff --git a/aten/src/THCUNN/SpatialUpSamplingBilinear.cu b/aten/src/THCUNN/SpatialUpSamplingBilinear.cu
index 11f37b465..71fb09b37 100644
--- a/aten/src/THCUNN/SpatialUpSamplingBilinear.cu
+++ b/aten/src/THCUNN/SpatialUpSamplingBilinear.cu
@@ -52,10 +52,10 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
     //
     for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
-        const Acctype val = h0lambda * (w0lambda * data1[n][c][h1][w1]
-                            + w1lambda * data1[n][c][h1][w1+w1p])
-                            + h1lambda * (w0lambda * data1[n][c][h1+h1p][w1]
-                            + w1lambda * data1[n][c][h1+h1p][w1+w1p]);
+        const Acctype val = h0lambda * (w0lambda * (data1[n][c][h1][w1]).template as<Acctype>()
+                            + w1lambda * (data1[n][c][h1][w1+w1p]).template as<Acctype>())
+                            + h1lambda * (w0lambda * (data1[n][c][h1+h1p][w1]).template as<Acctype>()
+                            + w1lambda * (data1[n][c][h1+h1p][w1+w1p]).template as<Acctype>());
         data2[n][c][h2][w2] = ScalarConvert<Acctype, Dtype>::to(val);
       }
     }
@@ -84,7 +84,7 @@ __global__ void caffe_gpu_interp2_kernel_backward(const int n,
       for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
           const Dtype val = data2[n][c][h1][w1];
-          data1[n][c][h2][w2] += val;
+          (data1[n][c][h2][w2]).template as<Dtype>() += val;
         }
       }
       return;
diff --git a/aten/src/THCUNN/THCHalfAutoNumerics.cuh b/aten/src/THCUNN/THCHalfAutoNumerics.cuh
index 2653fed0b..532242996 100644
--- a/aten/src/THCUNN/THCHalfAutoNumerics.cuh
+++ b/aten/src/THCUNN/THCHalfAutoNumerics.cuh
@@ -31,6 +31,8 @@ inline __host__ __device__ double fmaxType(double x, double y) {
 
 // arithmetic functions
 
+#if defined(__HIP_PLATFORM_HCC__)
+#else
 inline __host__ __device__ half operator+(half a, half b) {
   return THCNumerics<half>::add(a, b);
 }
@@ -159,6 +161,7 @@ inline __host__ __device__ half& operator/=(half &lhs, const half &rhs) {
   lhs = lhs / rhs;
   return lhs;
 }
+#endif
 
 inline __host__ __device__ half abs(half a) {
   return THCNumerics<half>::abs(a);
@@ -212,6 +215,8 @@ inline __host__ __device__ half operator*(half a, bool b) {
 
 // comparison functions
 
+#if defined(__HIP_PLATFORM_HCC__)
+#else
 inline __host__ __device__ bool operator<(half a, half b) {
   return THCNumerics<half>::lt(a, b);
 }
@@ -243,6 +248,7 @@ inline __host__ __device__ bool operator>=(half a, half b) {
 inline __host__ __device__ bool operator>=(half a, int b) {
   return THCNumerics<half>::ge(a, ScalarConvert<int ,half>::to(b));
 }
+#endif
 
 #endif
 #endif
diff --git a/aten/src/THCUNN/TemporalUpSamplingLinear.cu b/aten/src/THCUNN/TemporalUpSamplingLinear.cu
index 98e4f2833..69ddb32fd 100644
--- a/aten/src/THCUNN/TemporalUpSamplingLinear.cu
+++ b/aten/src/THCUNN/TemporalUpSamplingLinear.cu
@@ -42,8 +42,8 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
     //
     for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
-        const Acctype val = w0lambda * data1[n][c][w1]
-                            + w1lambda * data1[n][c][w1+w1p];
+        const Acctype val = w0lambda * (data1[n][c][w1]).template as<Acctype>()
+                            + w1lambda * (data1[n][c][w1+w1p]).template as<Acctype>();
         data2[n][c][w2] = ScalarConvert<Acctype, Dtype>::to(val);
       }
     }
@@ -68,7 +68,7 @@ __global__ void caffe_gpu_interp2_kernel_backward(const int n,
       for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
           const Dtype val = data2[n][c][w1];
-          data1[n][c][w2] += val;
+          (data1[n][c][w2]).template as<Dtype>() += val;
         }
       }
       return;
diff --git a/aten/src/THCUNN/VolumetricGridSamplerBilinear.cu b/aten/src/THCUNN/VolumetricGridSamplerBilinear.cu
index 6fa8ae1fd..4b9e95939 100644
--- a/aten/src/THCUNN/VolumetricGridSamplerBilinear.cu
+++ b/aten/src/THCUNN/VolumetricGridSamplerBilinear.cu
@@ -66,7 +66,7 @@ __global__ void VolumetricGridSamplerBilinear_updateOutput_kernel(
     int ix_tnw = floor(ScalarConvert<Dtype,float>::to(ix));
     int iy_tnw = floor(ScalarConvert<Dtype,float>::to(iy));
     int iz_tnw = floor(ScalarConvert<Dtype,float>::to(iz));
-    
+
     int ix_tne = ix_tnw + 1;
     int iy_tne = iy_tnw;
     int iz_tne = iz_tnw;
@@ -138,28 +138,28 @@ __global__ void VolumetricGridSamplerBilinear_updateOutput_kernel(
     for (c = 0; c < C; ++c) {
       out_val = ScalarConvert<int,Dtype>::to(0);
       if (WITHIN_BOUNDS(ix_tnw, iy_tnw, iz_tnw, ID, IH, IW)) {
-        out_val += input[n][c][iz_tnw][iy_tnw][ix_tnw] * tnw;
+        out_val += (input[n][c][iz_tnw][iy_tnw][ix_tnw]).template as<Dtype>() * tnw;
       }
       if (WITHIN_BOUNDS(ix_tne, iy_tne, iz_tne, ID, IH, IW)) {
-        out_val += input[n][c][iz_tne][iy_tne][ix_tne] * tne;
+        out_val += (input[n][c][iz_tne][iy_tne][ix_tne]).template as<Dtype>() * tne;
       }
       if (WITHIN_BOUNDS(ix_tsw, iy_tsw, iz_tsw, ID, IH, IW)) {
-        out_val += input[n][c][iz_tsw][iy_tsw][ix_tsw] * tsw;
+        out_val += (input[n][c][iz_tsw][iy_tsw][ix_tsw]).template as<Dtype>() * tsw;
       }
       if (WITHIN_BOUNDS(ix_tse, iy_tse, iz_tse, ID, IH, IW)) {
-        out_val += input[n][c][iz_tse][iy_tse][ix_tse] * tse;
+        out_val += (input[n][c][iz_tse][iy_tse][ix_tse]).template as<Dtype>() * tse;
       }
       if (WITHIN_BOUNDS(ix_bnw, iy_bnw, iz_bnw, ID, IH, IW)) {
-        out_val += input[n][c][iz_bnw][iy_bnw][ix_bnw] * bnw;
+        out_val += (input[n][c][iz_bnw][iy_bnw][ix_bnw]).template as<Dtype>() * bnw;
       }
       if (WITHIN_BOUNDS(ix_bne, iy_bne, iz_bne, ID, IH, IW)) {
-        out_val += input[n][c][iz_bne][iy_bne][ix_bne] * bne;
+        out_val += (input[n][c][iz_bne][iy_bne][ix_bne]).template as<Dtype>() * bne;
       }
       if (WITHIN_BOUNDS(ix_bsw, iy_bsw, iz_bsw, ID, IH, IW)) {
-        out_val += input[n][c][iz_bsw][iy_bsw][ix_bsw] * bsw;
+        out_val += (input[n][c][iz_bsw][iy_bsw][ix_bsw]).template as<Dtype>() * bsw;
       }
       if (WITHIN_BOUNDS(ix_bse, iy_bse, iz_bse, ID, IH, IW)) {
-        out_val += input[n][c][iz_bse][iy_bse][ix_bse] * bse;
+        out_val += (input[n][c][iz_bse][iy_bse][ix_bse]).template as<Dtype>() * bse;
       }
       output[n][c][d][h][w] = out_val;
     }
@@ -211,7 +211,7 @@ __global__ void VolumetricGridSamplerBilinear_updateGradInput_kernel(
     int ix_tnw = floor(ScalarConvert<Dtype,float>::to(ix));
     int iy_tnw = floor(ScalarConvert<Dtype,float>::to(iy));
     int iz_tnw = floor(ScalarConvert<Dtype,float>::to(iz));
-    
+
     int ix_tne = ix_tnw + 1;
     int iy_tne = iy_tnw;
     int iz_tne = iz_tnw;
@@ -259,7 +259,7 @@ __global__ void VolumetricGridSamplerBilinear_updateGradInput_kernel(
     Dtype bne_val;
     Dtype bsw_val;
     Dtype bse_val;
-    
+
     int ix_tnw_cl, iy_tnw_cl, iz_tnw_cl, ix_tne_cl, iy_tne_cl, iz_tne_cl;
     int ix_tsw_cl, iy_tsw_cl, iz_tsw_cl, ix_tse_cl, iy_tse_cl, iz_tse_cl;
     int ix_bnw_cl, iy_bnw_cl, iz_bnw_cl, ix_bne_cl, iy_bne_cl, iz_bne_cl;
diff --git a/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu b/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu
index 03d506ac5..b99423bbc 100644
--- a/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu
+++ b/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu
@@ -63,14 +63,14 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
     //
     for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
-        const Acctype val = t0lambda * (h0lambda * (w0lambda * data1[n][c][t1][h1][w1]
-                                                  + w1lambda * data1[n][c][t1][h1][w1+w1p])
-                                      + h1lambda * (w0lambda * data1[n][c][t1][h1+h1p][w1]
-                                                  + w1lambda * data1[n][c][t1][h1+h1p][w1+w1p]))
-                          + t1lambda * (h0lambda * (w0lambda * data1[n][c][t1+t1p][h1][w1]
-                                                  + w1lambda * data1[n][c][t1+t1p][h1][w1+w1p])
-                                      + h1lambda * (w0lambda * data1[n][c][t1+t1p][h1+h1p][w1]
-                                                  + w1lambda * data1[n][c][t1+t1p][h1+h1p][w1+w1p]));
+        const Acctype val = t0lambda * (h0lambda * (w0lambda * (data1[n][c][t1][h1][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1][h1][w1+w1p]).template as<Acctype>())
+                                      + h1lambda * (w0lambda * (data1[n][c][t1][h1+h1p][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1][h1+h1p][w1+w1p]).template as<Acctype>()))
+                          + t1lambda * (h0lambda * (w0lambda * (data1[n][c][t1+t1p][h1][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1+t1p][h1][w1+w1p]).template as<Acctype>())
+                                      + h1lambda * (w0lambda * (data1[n][c][t1+t1p][h1+h1p][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1+t1p][h1+h1p][w1+w1p]).template as<Acctype>()));
         data2[n][c][t2][h2][w2] = ScalarConvert<Acctype, Dtype>::to(val);
       }
     }
@@ -104,7 +104,7 @@ __global__ void caffe_gpu_interp2_kernel_backward(const int n,
       for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
           const Dtype val = data2[n][c][t1][h1][w1];
-          data1[n][c][t2][h2][w2] += val;
+          (data1[n][c][t2][h2][w2]).template as<Acctype>() += val;
         }
       }
       return;
diff --git a/aten/src/THCUNN/generic/SpatialCrossMapLRN.cu b/aten/src/THCUNN/generic/SpatialCrossMapLRN.cu
index 61a1c7046..222ec9b87 100644
--- a/aten/src/THCUNN/generic/SpatialCrossMapLRN.cu
+++ b/aten/src/THCUNN/generic/SpatialCrossMapLRN.cu
@@ -80,12 +80,14 @@ void LRNbackward(THCState* state, THCTensor* input, THCTensor* output,
   gradOutput = THCTensor_(newContiguous)(state, gradOutput);
 
   int n_threads = batchSize * imsize_h * imsize_w;
+#if defined(__NVCC__)
   LRNComputeDiff<real, accreal> <<<GET_BLOCKS(n_threads), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(
       n_threads, THCTensor_(data)(state, input), THCTensor_(data)(state, output),
       THCTensor_(data)(state, scale), THCTensor_(data)(state, gradOutput), batchSize, nInputPlane, imsize_h, imsize_w,
       local_size, -beta, ScalarConvert<int, real>::to(2) * alpha * beta / local_size,
       THCTensor_(data)(state, gradInput));
   THCudaCheck(cudaGetLastError());
+#endif
 
   THCTensor_(free)(state, input);
   THCTensor_(free)(state, gradOutput);
diff --git a/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu b/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu
index 6eb74bb43..b02aef610 100644
--- a/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu
+++ b/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu
@@ -2,9 +2,9 @@
 #define THC_GENERIC_FILE "generic/VolumetricDilatedMaxPooling.cu"
 #else
 
-#define UPDATE_OUTPUT_KERNEL_WIDTH(KW) case KW:                         \
-  cuda_VolumetricDilatedMaxPooling_updateOutput<KW><<<grid, block,             \
-    0, THCState_getCurrentStream(state)>>>(                             \
+#define UPDATE_OUTPUT_KERNEL_WIDTH(KW) case KW:                  \
+  cuda_VolumetricDilatedMaxPooling_updateOutput<KW>              \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(      \
     inputData, inputTime, inputHeight, inputWidth, \
     cudaIndices, cudaOutput, kT, kH, dT, dH, dW, padT, padH, padW,\
     dilationT, dilationH, dilationW, offsetZ); \
@@ -234,7 +234,7 @@ void THNN_(VolumetricDilatedMaxPooling_updateOutput)(
   } else {
     THCTensor_(retain)(state, output);
   }
-  
+
   real* inputData = THCTensor_(data)(state, input);
 
   THCDeviceTensor<real, 4> cudaOutput;
diff --git a/patch084e3a7.patch b/patch084e3a7.patch
new file mode 100644
index 000000000..8bc8279f1
--- /dev/null
+++ b/patch084e3a7.patch
@@ -0,0 +1,5268 @@
+diff --git a/aten/CMakeLists.txt b/aten/CMakeLists.txt
+index 9648d1907..2d9a732cd 100644
+--- a/aten/CMakeLists.txt
++++ b/aten/CMakeLists.txt
+@@ -1,15 +1,58 @@
+ cmake_minimum_required(VERSION 3.0)
++# HIP_PATH
++IF(NOT DEFINED $ENV{HIP_PATH})
++  SET(HIP_PATH /opt/rocm/hip)
++ELSE()
++  SET(HIP_PATH $ENV{HIP_PATH})
++ENDIF()
++
++# HCC_PATH
++IF(NOT DEFINED $ENV{HCC_PATH})
++  SET(HCC_PATH /opt/rocm/hcc)
++ELSE()
++  SET(HCC_PATH $ENV{HCC_PATH})
++ENDIF()
++
++# HIPBLAS_PATH
++IF(NOT DEFINED $ENV{HIPBLAS_PATH})
++  SET(HIPBLAS_PATH /opt/rocm/hipblas)
++ELSE()
++  SET(HIPBLAS_PATH $ENV{HIPBLAS_PATH})
++ENDIF()
++
++# HIPRNG_PATH
++IF(NOT DEFINED $ENV{HIPRNG_PATH})
++  SET(HIPRNG_PATH /opt/rocm/hcrng)
++ELSE()
++  SET(HIPRNG_PATH $ENV{HIPRNG_PATH})
++ENDIF()
++
++# HIPSPARSE_PATH
++IF(NOT DEFINED $ENV{HIPSPARSE_PATH})
++  SET(HIPSPARSE_PATH /opt/rocm/hcsparse)
++ELSE()
++  SET(HIPSPARSE_PATH $ENV{HIPSPARSE_PATH})
++ENDIF()
++
++SET(THRUST_PATH "/root/Thrust")
++
++cmake_minimum_required(VERSION 3.0)
+ set(CMAKE_MODULE_PATH
+   ${CMAKE_CURRENT_SOURCE_DIR}/cmake
+   ${CMAKE_CURRENT_SOURCE_DIR}/../cmake/Modules_CUDA_fix
+   /usr/lib/x86_64-linux-gnu/
+   ${CMAKE_CURRENT_SOURCE_DIR}/src/TH/cmake
++  ${HIP_PATH}/cmake
+   ${CMAKE_MODULE_PATH})
+ SET(CMAKE_LIBRARY_PATH /usr/lib/x86_64-linux-gnu/ ${CMAKE_LIBRARY_PATH})
+ project(ATen)
+ 
+ cmake_policy(SET CMP0012 NEW)
+ 
++# Disable Asserts In Code
++ADD_DEFINITIONS(-DNDEBUG)
++FIND_PACKAGE(HIP 1.0 REQUIRED)
++
+ # Polyfill for upstream FindCUDA
+ include(CMakeInitializeConfigs)
+ 
+@@ -94,16 +137,16 @@ IF(MSVC)
+   ADD_DEFINITIONS(-D_CRT_SECURE_NO_DEPRECATE=1)
+   LIST(APPEND CUDA_NVCC_FLAGS "-Xcompiler /wd4819 -Xcompiler /wd4503 -Xcompiler /wd4190 -Xcompiler /wd4244 -Xcompiler /wd4251 -Xcompiler /wd4275 -Xcompiler /wd4522")
+   ADD_DEFINITIONS(-DTH_EXPORTS)
+-  IF (NOT NO_CUDA)
++  IF (WITH_ROCM)
+     ADD_DEFINITIONS(-DTHC_EXPORTS)
+   ENDIF()
+ ENDIF(MSVC)
+ 
+ IF (NOT MSVC)
+   IF (CMAKE_VERSION VERSION_LESS "3.1")
+-    SET(CMAKE_C_FLAGS "-std=c11 ${CMAKE_C_FLAGS}")
++    # SET(CMAKE_C_FLAGS "-std=c11 ${CMAKE_C_FLAGS}")
+   ELSE ()
+-    SET(CMAKE_C_STANDARD 11)
++    # SET(CMAKE_C_STANDARD 11)
+   ENDIF ()
+ ENDIF(NOT MSVC)
+ 
+@@ -299,8 +342,8 @@ IF(C_SSE4_1_FOUND AND C_SSE4_2_FOUND)
+ ENDIF()
+ IF(C_SSE3_FOUND)
+   MESSAGE(STATUS "SSE3 Found")
+-  SET(CMAKE_C_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_C_FLAGS}")
+-  SET(CMAKE_CXX_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_CXX_FLAGS}")
++  #SET(CMAKE_C_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_C_FLAGS}")
++  #SET(CMAKE_CXX_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_CXX_FLAGS}")
+ ENDIF(C_SSE3_FOUND)
+ 
+ # we don't set -mavx and -mavx2 flags globally, but only for specific files
+@@ -337,6 +380,9 @@ int main()
+ }
+ " HAS_C11_ATOMICS)
+ 
++# For ROCm stack, the hcc/hipcc compiler cannot use C11 atomics. Prefer GCC.
++SET(HAS_C11_ATOMICS OFF)
++
+ IF(NOT HAS_C11_ATOMICS)
+   CHECK_C_SOURCE_RUNS("
+ #include <intrin.h>
+@@ -403,14 +449,6 @@ IF(BLAS_FOUND)
+   SET(USE_BLAS 1)
+   IF(BLAS_INFO STREQUAL "mkl")
+     ADD_DEFINITIONS(-DTH_BLAS_MKL)
+-    IF(NOT BLAS_INCLUDE_DIR)
+-      MESSAGE(FATAL_ERROR "MKL is used, but MKL header files are not found. \
+-        You can get them by `conda install mkl-include` if using conda (if \
+-        it is missing, run `conda upgrade -n root conda` first), and \
+-        `pip install mkl-devel` if using pip. If build fails with header files \
+-        available in the system, please make sure that CMake will search the \
+-        directory containing them, e.g., by setting CMAKE_INCLUDE_PATH.")
+-    ENDIF()
+     IF(MSVC AND MKL_LIBRARIES MATCHES ".*libiomp5md\\.lib.*")
+       ADD_DEFINITIONS(-D_OPENMP_NOFORCE_MANIFEST)
+       SET(AT_MKL_MT 1)
+@@ -455,17 +493,22 @@ include_directories(
+ add_subdirectory(src/THNN)
+ add_subdirectory(src/THS)
+ 
+-if(NO_CUDA)
+-  message("disabling CUDA because NO_CUDA is set")
+-  SET(CUDA_FLAG -n)
+-  SET(AT_CUDA_ENABLED 0)
+-else()
++if (WITH_ROCM)
++  SET(AT_CUDA_ENABLED 1)
++  add_subdirectory(src/THC)
++  add_subdirectory(src/THCUNN)
++  add_subdirectory(src/THCS)
++elseif(NOT NO_CUDA)
+   SET(AT_CUDA_ENABLED 1)
+   INCLUDE_DIRECTORIES(${CUDA_INCLUDE_DIRS})
+   find_package(CUDA 5.5 REQUIRED)
+   add_subdirectory(src/THC)
+   add_subdirectory(src/THCUNN)
+   add_subdirectory(src/THCS)
++else()
++  message("disabling CUDA because NO_CUDA is set")
++  SET(CUDA_FLAG -n)
++  SET(AT_CUDA_ENABLED 0)
+ endif()
+ 
+ find_package(CuDNN)
+@@ -508,10 +551,10 @@ include_directories(
+ ${CMAKE_CURRENT_SOURCE_DIR}/src
+ ${CMAKE_CURRENT_SOURCE_DIR}/src/ATen/utils/catch/single_include
+ ${CMAKE_CURRENT_BINARY_DIR}/src/ATen)
+-if(NOT NO_CUDA)
++if(NOT NO_CUDA AND NOT WITH_ROCM)
+   include_directories(${CUDA_INCLUDE_DIRS})
+ endif()
+-add_subdirectory(src/ATen/test)
++#add_subdirectory(src/ATen/test)
+ 
+ if(ATEN_NO_CONTRIB)
+   message("disable contrib because ATEN_NO_CONTRIB is set")
+diff --git a/aten/src/ATen/ATen.h b/aten/src/ATen/ATen.h
+index e41c2d956..0c94ab2e1 100644
+--- a/aten/src/ATen/ATen.h
++++ b/aten/src/ATen/ATen.h
+@@ -1,3 +1,4 @@
++#include "hip/hip_runtime.h" 
+ #pragma once
+ 
+ #include "ATen/ATenGeneral.h"
+diff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt
+index 16ad86825..b5c0eea6a 100644
+--- a/aten/src/ATen/CMakeLists.txt
++++ b/aten/src/ATen/CMakeLists.txt
+@@ -1,6 +1,67 @@
+-
+ CMAKE_MINIMUM_REQUIRED(VERSION 2.8)
+-SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH})
++FIND_PACKAGE(HIP 1.0 REQUIRED)
++# HIP_PATH
++IF(NOT DEFINED $ENV{HIP_PATH})
++  SET(HIP_PATH /opt/rocm/hip)
++ELSE()
++  SET(HIP_PATH $ENV{HIP_PATH})
++ENDIF()
++
++# HCC_PATH
++IF(NOT DEFINED $ENV{HCC_PATH})
++  SET(HCC_PATH /opt/rocm/hcc)
++ELSE()
++  SET(HCC_PATH $ENV{HCC_PATH})
++ENDIF()
++
++# HIPBLAS_PATH
++IF(NOT DEFINED $ENV{HIPBLAS_PATH})
++  SET(HIPBLAS_PATH /opt/rocm/hipblas)
++ELSE()
++  SET(HIPBLAS_PATH $ENV{HIPBLAS_PATH})
++ENDIF()
++
++# HIPRNG_PATH
++IF(NOT DEFINED $ENV{HIPRNG_PATH})
++  SET(HIPRNG_PATH /opt/rocm/hcrng)
++ELSE()
++  SET(HIPRNG_PATH $ENV{HIPRNG_PATH})
++ENDIF()
++
++# HIPSPARSE_PATH
++IF(NOT DEFINED $ENV{HIPSPARSE_PATH})
++
++  SET(HIPSPARSE_PATH /opt/rocm/hcsparse)
++ELSE()
++  SET(HIPSPARSE_PATH $ENV{HIPSPARSE_PATH})
++ENDIF()
++
++SET(THRUST_PATH "~/Thrust")
++INCLUDE_DIRECTORIES(${HIP_PATH}/include)
++INCLUDE_DIRECTORIES(${HIPBLAS_PATH}/include)
++INCLUDE_DIRECTORIES(${HIPSPARSE_PATH}/include)
++INCLUDE_DIRECTORIES(${HIPRNG_PATH}/include)
++INCLUDE_DIRECTORIES(${THRUST_PATH})
++
++# Disable Asserts In Code
++ADD_DEFINITIONS(-DNDEBUG)
++
++# load HIP cmake module and load platform id
++# SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH} "${HIP_PATH}/cmake")
++EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig -P OUTPUT_VARIABLE PLATFORM)
++EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig --cpp_config OUTPUT_VARIABLE HIP_CXX_FLAGS)
++
++SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
++
++# SET(CMAKE_C_FLAGS "-std=c99 -Werror=implicit-function-declaration ${CMAKE_C_FLAGS}")
++# SET(CMAKE_C_FLAGS "${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
++SET(CMAKE_CXX_FLAGS  "-std=c++11 ${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
++
++# Set the Flags for hcc & hipcc
++
++# Show message that we're using ROCm.
++MESSAGE(STATUS "ROCM TRUE:")
++MESSAGE(STATUS "CMAKE_CXX_COMPILER: " ${CMAKE_CXX_COMPILER})
+ 
+ # avoid some cmake warnings
+ IF(POLICY CMP0026)
+@@ -202,7 +263,7 @@ ADD_CUSTOM_TARGET(aten_files_are_generated
+ SET(all_cpp ${base_cpp} ${native_cpp} ${native_cudnn_cpp} ${native_mkl_cpp} ${native_mkldnn_cpp} ${generated_cpp} ${ATen_CPU_SRCS} ${cpu_kernel_cpp})
+ 
+ INCLUDE_DIRECTORIES(${ATen_CPU_INCLUDE})
+-IF(NOT NO_CUDA)
++IF(WITH_ROCM)
+   INCLUDE_DIRECTORIES(${ATen_CUDA_INCLUDE})
+   INCLUDE_DIRECTORIES("${CUDA_SDK_ROOT_DIR}/common/inc")
+   INCLUDE_DIRECTORIES("${CMAKE_CURRENT_SOURCE_DIR}/cuda")
+@@ -228,8 +289,8 @@ INCLUDE_DIRECTORIES(${CMAKE_CURRENT_BINARY_DIR})
+ IF(NOT AT_LINK_STYLE)
+   SET(AT_LINK_STYLE SHARED)
+ ENDIF()
+-IF(CUDA_FOUND)
+-  CUDA_ADD_LIBRARY(ATen ${AT_LINK_STYLE} ${all_cpp})
++IF(WITH_ROCM)
++  HIP_ADD_LIBRARY(ATen ${AT_LINK_STYLE} ${all_cpp})
+ ELSE()
+   ADD_LIBRARY(ATen ${AT_LINK_STYLE} ${all_cpp})
+ ENDIF()
+@@ -246,9 +307,9 @@ set_property(TARGET tbb_static tbb_def_files PROPERTY FOLDER "dependencies")
+ target_include_directories(tbb_static PUBLIC ${TBB_ROOT_DIR}/include)
+ target_link_libraries(ATen tbb_static)
+ 
+-if(NOT ${CMAKE_VERSION} VERSION_LESS "3.1")
++if(NOT ${CMAKE_VERSION} VERSION_LESS "3.1" AND NOT WITH_ROCM)
+     SET_PROPERTY(TARGET ATen PROPERTY CXX_STANDARD 11)
+-endif(NOT ${CMAKE_VERSION} VERSION_LESS "3.1")
++endif(NOT ${CMAKE_VERSION} VERSION_LESS "3.1" AND NOT WITH_ROCM)
+ 
+ IF(BLAS_FOUND)
+   IF ($ENV{TH_BINARY_BUILD})
+@@ -354,6 +415,16 @@ IF(CUDA_FOUND)
+   ENDIF($ENV{ATEN_STATIC_CUDA})
+ ENDIF()
+ 
++### Link in the ROCm stuff ###
++FIND_LIBRARY(HIPBLAS_LIBRARY hipblas HINTS ${HIPBLAS_PATH}/lib)
++FIND_LIBRARY(HIPRNG_LIBRARY hiprng HINTS ${HIPRNG_PATH}/lib)
++
++IF(WITH_ROCM)
++  TARGET_LINK_LIBRARIES(ATen
++    ${HIPBLAS_LIBRARY}
++    ${HIPRNG_LIBRARY})
++ENDIF()
++
+ INSTALL(TARGETS ATen
+   RUNTIME DESTINATION "${AT_INSTALL_BIN_DIR}"
+   LIBRARY DESTINATION "${AT_INSTALL_LIB_DIR}"
+diff --git a/aten/src/ATen/Half.cpp b/aten/src/ATen/Half.cpp
+index 465799194..9761a9718 100644
+--- a/aten/src/ATen/Half.cpp
++++ b/aten/src/ATen/Half.cpp
+@@ -25,6 +25,11 @@ template<> AT_API double convert(Half f) {
+   return convert<float, Half>(f);
+ }
+ 
++template<> AT_API __fp16 convert(Half f) {
++  __fp16 h = reinterpret_cast<__fp16&>(f.x);
++  return h;
++}
++
+ template<> AT_API Half convert(int64_t f) {
+   return convert<Half,double>(static_cast<double>(f));
+ }
+diff --git a/aten/src/ATen/Half.h b/aten/src/ATen/Half.h
+index da2326cec..d5ea9a549 100644
+--- a/aten/src/ATen/Half.h
++++ b/aten/src/ATen/Half.h
+@@ -48,6 +48,7 @@ template<typename To, typename From> To checked_convert(From f, const char* name
+ struct alignas(2) Half {
+   unsigned short x;
+   operator double();
++  operator __fp16();
+ };
+ 
+ template<> AT_API Half convert(float f);
+@@ -56,11 +57,16 @@ template<> AT_API Half convert(double f);
+ template<> AT_API double convert(Half f);
+ template<> AT_API Half convert(int64_t f);
+ template<> AT_API int64_t convert(Half f);
++template<> AT_API __fp16 convert(Half f);
+ 
+ inline Half::operator double() {
+   return convert<double, Half>(*this);
+ }
+ 
++inline Half::operator __fp16() {
++  return convert<__fp16, Half>(*this);
++}
++
+ template<> bool overflows<Half, double>(double f);
+ template<> bool overflows<Half, int64_t>(int64_t f);
+ 
+@@ -68,4 +74,15 @@ template<typename To, typename From>
+ To HalfFix(From h) {
+   return To { h.x };
+ }
++
++template <>
++  inline __fp16 HalfFix<__fp16, Half>(Half h) {
++  return reinterpret_cast<__fp16&>(h);
++}
++template<>
++  inline Half HalfFix<Half, __fp16>(__fp16 h) {
++  unsigned short s = reinterpret_cast<unsigned short&>(h);
++  return Half { s };
++}
++
+ } // namespace at
+diff --git a/aten/src/ATen/cuda/detail/IndexUtils.cu b/aten/src/ATen/cuda/detail/IndexUtils.cu
+index 94e5dd134..2889bfe56 100644
+--- a/aten/src/ATen/cuda/detail/IndexUtils.cu
++++ b/aten/src/ATen/cuda/detail/IndexUtils.cu
+@@ -16,6 +16,7 @@ int compareSizeAndStride(const void* a, const void* b) {
+   return aS->stride < bS->stride;
+ }
+ 
++#if !defined(__HIP_DEVICE_COMPILE__)
+ bool overlappingIndices(const Tensor& t) {
+   // In this function, we don't care about permutations of the
+   // size/stride arrays (transpositions).
+@@ -68,6 +69,8 @@ bool overlappingIndices(const Tensor& t) {
+   /* Tensor has holes or is contiguous */
+   return false;
+ }
++#endif
++
+ 
+ bool canUse32BitIndexMath(const Tensor& t, int64_t max_elem) {
+   int64_t elements = t.numel();
+diff --git a/aten/src/ATen/native/cuda/Distributions.cu b/aten/src/ATen/native/cuda/Distributions.cu
+index 50d162f14..9e242deed 100644
+--- a/aten/src/ATen/native/cuda/Distributions.cu
++++ b/aten/src/ATen/native/cuda/Distributions.cu
+@@ -49,10 +49,13 @@ void poisson_cuda_kernel(
+ namespace at { namespace native {
+ Tensor _s_poisson_cuda(const Tensor& lambda, Generator* gen) {
+   Tensor ret = lambda.type().tensor(lambda.sizes());
++
++  #if !defined(__HIP_PLATFORM_HCC__)
+   auto lambda_ = lambda.toType(ScalarType::Float);
+   AT_DISPATCH_FLOATING_TYPES(ret.type(), "poisson", [&] {
+      poisson_cuda_kernel<scalar_t>(ret, lambda_, next_philox_seed(gen));
+    });
++  #endif
+   return ret;
+ }
+ 
+diff --git a/aten/src/ATen/native/cuda/Embedding.cu b/aten/src/ATen/native/cuda/Embedding.cu
+index 67b3265b2..148ae4f6d 100644
+--- a/aten/src/ATen/native/cuda/Embedding.cu
++++ b/aten/src/ATen/native/cuda/Embedding.cu
+@@ -165,11 +165,11 @@ __global__ void renorm_kernel(
+   for (int i = tid; i < dim; i += blockDim.x) {
+     auto x = scalar_cast<accscalar_t>(weights[base_index + i]);
+     if (norm_type == 1) {
+-      v += std::abs(x);
++      v += fabs(x);
+     } else if (norm_type == 2) {
+       v += x * x;
+     } else {
+-      v += std::pow(x, norm_type);
++      v += powf(x, norm_type);
+     }
+   }
+ 
+@@ -177,7 +177,7 @@ __global__ void renorm_kernel(
+   v = reduceBlock<accscalar_t>(sdata, blockDim.x, v, Op(), 0);
+ 
+   if (tid == 0) {
+-    sdata[0] = std::pow(v, scalar_cast<accscalar_t>(1.0 / norm_type));
++    sdata[0] = powf(v, scalar_cast<accscalar_t>(1.0 / norm_type));
+   }
+   __syncthreads();
+ 
+diff --git a/aten/src/ATen/native/cuda/RoiPooling.cu b/aten/src/ATen/native/cuda/RoiPooling.cu
+index ef5ff049f..4b08d5b63 100644
+--- a/aten/src/ATen/native/cuda/RoiPooling.cu
++++ b/aten/src/ATen/native/cuda/RoiPooling.cu
+@@ -130,9 +130,10 @@ std::tuple<Tensor, Tensor> RoiPooling2d_forward_cuda(
+ 
+   dim3 block(512);
+   dim3 grid((output.numel() + 512 - 1) / 512);
+-  RoiPooling2d_forward_kernel<<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
++  RoiPooling2d_forward_kernel<float><<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
+     output.numel(), input.data<float>(), rois.data<float>(), static_cast<float>(spatialScale), inputChannels,
+     inputHeight, inputWidth, pooledHeight, pooledWidth, output.data<float>(), argmaxes.data<int>());
++
+   AT_ASSERT(cudaGetLastError() == cudaSuccess, "RoiPooling2d_forward_kernel failed");
+ 
+   return std::make_tuple(output, argmaxes);
+@@ -197,10 +198,11 @@ Tensor RoiPooling2d_backward_cuda(
+ 
+   dim3 block(512);
+   dim3 grid((gradInput.numel() + 512 - 1) / 512);
+-  RoiPooling2d_backward_kernel<<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
++  RoiPooling2d_backward_kernel<float><<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
+     gradOutput.numel(), gradOutput.data<float>(), argmaxes.data<int>(), proposals,
+     static_cast<float>(spatialScale), inputChannels, inputHeight, inputWidth,
+     pooledHeight, pooledWidth, gradInput.data<float>(), rois.data<float>());
++
+   AT_ASSERT(cudaGetLastError() == cudaSuccess, "RoiPooling2d_forward_kernel failed");
+ 
+   return gradInput;
+diff --git a/aten/src/ATen/native/cudnn/BatchNorm.cpp b/aten/src/ATen/native/cudnn/BatchNorm.cpp
+deleted file mode 100644
+index 211a9c3a3..000000000
+--- a/aten/src/ATen/native/cudnn/BatchNorm.cpp
++++ /dev/null
+@@ -1,221 +0,0 @@
+-#include <ATen/ATen.h>
+-#include <ATen/NativeFunctions.h>
+-#include <ATen/Config.h>
+-
+-#if !AT_CUDNN_ENABLED()
+-
+-namespace at { namespace native {
+-
+-// See Note [ATen preprocessor philosophy]
+-
+-std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm(
+-    const Tensor& input, const Tensor& weight,
+-    const Tensor& bias, const Tensor& running_mean, const Tensor& running_var,
+-    bool training, double exponential_average_factor, double epsilon) {
+-  throw std::runtime_error("cudnn_batch_norm: ATen not compiled with cuDNN support");
+-}
+-
+-std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(
+-    const Tensor& input, const Tensor& grad_output, const Tensor& weight,
+-    const Tensor& running_mean, const Tensor& running_var,
+-    const Tensor& save_mean, const Tensor& save_var,
+-    double epsilon) {
+-  throw std::runtime_error("cudnn_batch_norm_backward: ATen not compiled with cuDNN support");
+-}
+-
+-}}  // namespace at::native
+-
+-#else // AT_CUDNN_ENABLED
+-
+-#include <ATen/cudnn/Descriptors.h>
+-#include <ATen/cudnn/Types.h>
+-#include <ATen/cudnn/Utils.h>
+-
+-#include <ATen/TensorUtils.h>
+-
+-namespace at { namespace native {
+-
+-namespace {
+-
+-Tensor expandScale(const Tensor& t, int64_t dim) {
+-  std::vector<int64_t> size{ 1, t.numel() };
+-  while (static_cast<int64_t>(size.size()) < dim) {
+-    size.emplace_back(1);
+-  }
+-  return t.view(size);
+-}
+-
+-}  // namespace
+-
+-std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm(
+-    const Tensor& input_t, const Tensor& weight_t,
+-    const Tensor& bias_t, const Tensor& running_mean_t, const Tensor& running_var_t,
+-    bool training, double exponential_average_factor, double epsilon)
+-{
+-  TensorArg input{ input_t, "input", 1 },
+-            weight{ weight_t, "weight", 2 },
+-            bias{ bias_t, "bias", 3 },
+-            running_mean{ running_mean_t, "running_mean", 4 },
+-            running_var{ running_var_t, "running_var", 5 };
+-  CheckedFrom c = "cudnn_batch_norm";
+-  setCuDNNStreamToCurrent();
+-
+-  checkAllDefined(c, {input, weight, bias});
+-  if (!training) {
+-    checkAllDefined(c, {running_mean, running_var});
+-  }
+-  checkAllSameGPU(c, {input, weight, bias, running_mean, running_var});
+-  if (input->type().scalarType() == ScalarType::Half) {
+-    checkScalarType(c, weight, ScalarType::Float);
+-  } else {
+-    checkAllSameType(c, {input, weight});
+-  }
+-  checkAllSameType(c, {weight, bias, running_mean, running_var});
+-  // TODO: is weight required to be contiguous?
+-  checkAllContiguous(c, {input, weight, bias, running_mean, running_var});
+-  checkDimRange(c, input, 2, 6 /* exclusive */);
+-  auto num_features = input->size(1);
+-  for (auto t : {weight, bias, running_mean, running_var}) {
+-    if (t->defined()) {
+-      checkNumel(c, t, num_features);
+-    }
+-  }
+-
+-  cudnnBatchNormMode_t mode;
+-  if (input->dim() == 2) {
+-    mode = CUDNN_BATCHNORM_PER_ACTIVATION;
+-  } else {
+-    mode = CUDNN_BATCHNORM_SPATIAL;
+-#if CUDNN_VERSION >= 7003
+-    if(training)
+-      mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;
+-#endif
+-  }
+-
+-  auto output_t = input->type().tensor(input->sizes());
+-  TensorArg output{ output_t, "output", 0 };
+-
+-  auto handle = getCudnnHandle();
+-  auto dataType = getCudnnDataType(*input);
+-  TensorDescriptor idesc{ *input, 4 };  // input descriptor
+-  TensorDescriptor wdesc{ expandScale(*weight, input->dim()), 4 };  // descriptor for weight, bias, running_mean, etc.
+-
+-  Constant one(dataType, 1);
+-  Constant zero(dataType, 0);
+-  Tensor save_mean, save_var;
+-
+-  if (training) {
+-    int64_t num_features = input_t.size(1);
+-    save_mean = weight_t.type().tensor({ num_features });
+-    save_var = weight_t.type().tensor({ num_features });
+-    CUDNN_CHECK(cudnnBatchNormalizationForwardTraining(
+-      handle, mode, &one, &zero,
+-      idesc.desc(), input->data_ptr(),
+-      idesc.desc(), output->data_ptr(),
+-      wdesc.desc(),
+-      weight->data_ptr(),
+-      bias->data_ptr(),
+-      exponential_average_factor,
+-      at::maybe_data_ptr(running_mean),
+-      at::maybe_data_ptr(running_var),
+-      epsilon,
+-      save_mean.data_ptr(),
+-      save_var.data_ptr()));
+-  } else {
+-    CUDNN_CHECK(cudnnBatchNormalizationForwardInference(
+-      handle, mode, &one, &zero,
+-      idesc.desc(), input->data_ptr(),
+-      idesc.desc(), output->data_ptr(),
+-      wdesc.desc(),
+-      weight->data_ptr(),
+-      bias->data_ptr(),
+-      running_mean->data_ptr(),
+-      running_var->data_ptr(),
+-      epsilon));
+-  }
+-
+-  // save_mean and save_var can be undefined
+-  // If this causes problems, we can initialize them to empty tensors
+-  // of the correct type
+-  return std::tuple<Tensor, Tensor, Tensor>{output_t, save_mean, save_var};
+-}
+-
+-// NB: CuDNN only implements the backward algorithm for batchnorm
+-// in training mode (evaluation mode batchnorm has a different algorithm),
+-// which is why this doesn't accept a 'training' parameter.
+-std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(
+-    const Tensor& input_t, const Tensor& grad_output_t, const Tensor& weight_t,
+-    // Unused: but we require them to be passed so that double backwards
+-    // has access
+-    const Tensor& running_mean, const Tensor& running_var,
+-    const Tensor& save_mean_t, const Tensor& save_var_t,
+-    double epsilon)
+-{
+-  TensorArg input{ input_t, "input", 1 },
+-            grad_output{ grad_output_t, "grad_output", 2 },
+-            weight{ weight_t, "weight", 3 },
+-            save_mean{ save_mean_t, "save_mean", 4 },
+-            save_var{ save_var_t, "save_var", 5 };
+-  CheckedFrom c = "cudnn_batch_norm_backward";
+-  setCuDNNStreamToCurrent();
+-
+-  checkAllDefined(c, {input, grad_output, weight, save_mean, save_var});
+-  checkAllSameGPU(c, {input, grad_output, weight, save_mean, save_var});
+-  if (input->type().scalarType() == ScalarType::Half) {
+-    checkScalarType(c, weight, ScalarType::Float);
+-  } else {
+-    checkAllSameType(c, {input, weight});
+-  }
+-  checkAllSameType(c, {input, grad_output});
+-  checkAllSameType(c, {weight, save_mean, save_var});
+-  // TODO: is weight required to be contiguous?
+-  checkAllContiguous(c, {input, grad_output, save_mean, save_var});
+-  checkDimRange(c, input, 2, 6 /* exclusive */);
+-  checkSameSize(c, input, grad_output);
+-  auto num_features = input->size(1);
+-  for (auto t : {weight, save_mean, save_var}) {
+-    checkNumel(c, t, num_features);
+-  }
+-
+-  cudnnBatchNormMode_t mode;
+-  if (input->dim() == 2) {
+-    mode = CUDNN_BATCHNORM_PER_ACTIVATION;
+-  } else {
+-#if CUDNN_VERSION >= 7003
+-    mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;
+-#else
+-    mode = CUDNN_BATCHNORM_SPATIAL;
+-#endif
+-  }
+-
+-  auto grad_input_t  = input->type().tensor(input->sizes());
+-  auto grad_weight_t = weight->type().tensor(weight->sizes());
+-  auto grad_bias_t   = weight->type().tensor(weight->sizes());
+-
+-  auto handle = getCudnnHandle();
+-  auto dataType = getCudnnDataType(*input);
+-
+-  TensorDescriptor idesc{ *input, 4 };  // input, output, grad_output descriptor
+-  TensorDescriptor wdesc{ expandScale(*weight, input->dim()), 4 };  // descriptor for weight, bias, save_mean, etc.
+-
+-  Constant one(dataType, 1);
+-  Constant zero(dataType, 0);
+-
+-  CUDNN_CHECK(cudnnBatchNormalizationBackward(
+-    handle, mode, &one, &zero, &one, &zero,
+-    idesc.desc(), input->data_ptr(),
+-    idesc.desc(), grad_output->data_ptr(),
+-    idesc.desc(), grad_input_t.data_ptr(),
+-    wdesc.desc(), weight->data_ptr(),
+-    grad_weight_t.data_ptr(),
+-    grad_bias_t.data_ptr(),
+-    epsilon,
+-    save_mean->data_ptr(),
+-    save_var->data_ptr()));
+-
+-  return std::tuple<Tensor,Tensor,Tensor>{grad_input_t, grad_weight_t, grad_bias_t};
+-}
+-
+-}}  // namespace native
+-
+-#endif
+diff --git a/aten/src/ATen/preprocess_declarations.py b/aten/src/ATen/preprocess_declarations.py
+index 1bc33e533..f9a91947d 100644
+--- a/aten/src/ATen/preprocess_declarations.py
++++ b/aten/src/ATen/preprocess_declarations.py
+@@ -57,9 +57,11 @@ def process_types_and_backends(option):
+     pairs = set(p for pair in pairs for p in expand(pair))
+ 
+     # disable CUDA Half if there is a Sparse argument
++    # for arg in option.get('arguments', []):
++    #     if arg['type'] == 'THSTensor*':
++    #         pairs.discard(('CUDA', 'Half'))
+     for arg in option.get('arguments', []):
+-        if arg['type'] == 'THSTensor*':
+-            pairs.discard(('CUDA', 'Half'))
++        pairs.discard(('CUDA', 'Half'))
+ 
+     # special case remove Half for cpu unless it is explicitly enabled,
+     if not option.get('cpu_half', False):
+diff --git a/aten/src/ATen/test/CMakeLists.txt b/aten/src/ATen/test/CMakeLists.txt
+index fe53bec46..3c393e727 100644
+--- a/aten/src/ATen/test/CMakeLists.txt
++++ b/aten/src/ATen/test/CMakeLists.txt
+@@ -4,7 +4,12 @@ IF (MSVC)
+   ENDIF()
+ ENDIF(MSVC)
+ 
+-ADD_EXECUTABLE(scalar_test scalar_test.cpp)
++if(WITH_ROCM)
++SET(CMAKE_C_COMPILER ${HIP_HIPCC_EXECUTABLE})
++SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
++endif()
++
++ADD_EXECUTABLE(scalar_test scalar_test.cpp)-
+ target_link_libraries(scalar_test ATen)
+ 
+ ADD_EXECUTABLE(basic basic.cpp)
+@@ -41,7 +46,11 @@ add_executable(tbb_init_test tbb_init_test.cpp)
+ target_link_libraries(tbb_init_test ATen)
+ 
+ if(NOT NO_CUDA)
++  if(WITH_ROCM)
++  hip_add_executable(integer_divider_test integer_divider_test.cu)
++  else()
+   cuda_add_executable(integer_divider_test integer_divider_test.cu)
++  endif()
+   target_link_libraries(integer_divider_test ATen)
+ endif()
+ 
+diff --git a/aten/src/ATen/test/scalar_test.cpp b/aten/src/ATen/test/scalar_test.cpp
+index ca3c865f4..83da70e0b 100644
+--- a/aten/src/ATen/test/scalar_test.cpp
++++ b/aten/src/ATen/test/scalar_test.cpp
+@@ -146,5 +146,7 @@ TEST_CASE( "scalar test", "[]" ) {
+   auto float_one = ones(T, {});
+   REQUIRE(float_one.toCFloat() == 1);
+   REQUIRE(float_one.toCInt() == 1);
++#if !defined(__HIP_PLATFORM_HCC__)
+   REQUIRE((float_one.toCHalf() == 1));
++#endif
+ }
+diff --git a/aten/src/TH/THAllocator.c b/aten/src/TH/THAllocator.c
+index 92f3cdaff..626970179 100644
+--- a/aten/src/TH/THAllocator.c
++++ b/aten/src/TH/THAllocator.c
+@@ -73,7 +73,7 @@ char * unknown_eventname = "eventname not specified";
+ 
+ THMapAllocatorContext *THMapAllocatorContext_new(const char *filename, int flags)
+ {
+-  THMapAllocatorContext *ctx = THAlloc(sizeof(THMapAllocatorContext));
++  THMapAllocatorContext *ctx = (THMapAllocatorContext*) THAlloc(sizeof(THMapAllocatorContext));
+ 
+   if (!(flags & TH_ALLOCATOR_MAPPED_SHARED) && !(flags & TH_ALLOCATOR_MAPPED_SHAREDMEM))
+     flags &= ~TH_ALLOCATOR_MAPPED_NOCREATE;
+@@ -82,7 +82,7 @@ THMapAllocatorContext *THMapAllocatorContext_new(const char *filename, int flags
+         "in shared mode");
+ 
+   if (filename) {
+-    ctx->filename = THAlloc(strlen(filename)+1);
++    ctx->filename = (char*) THAlloc(strlen(filename)+1);
+     strcpy(ctx->filename, filename);
+ #ifdef _WIN32
+     char *suffixname = "_event";
+@@ -178,7 +178,7 @@ static void *_map_alloc(void* ctx_, ptrdiff_t size)
+   if (size == 0)
+     return NULL;
+ 
+-  THMapAllocatorContext *ctx = ctx_;
++  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
+   void *data = NULL;
+ 
+ #ifdef _WIN32
+@@ -451,7 +451,7 @@ static void THMapAllocator_free(void* ctx_, void* data) {
+   if (data == NULL)
+     return;
+ 
+-  THMapAllocatorContext *ctx = ctx_;
++  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
+ 
+ #ifdef _WIN32
+   if ((ctx->flags & TH_ALLOCATOR_MAPPED_KEEPFD) || (ctx->flags & TH_ALLOCATOR_MAPPED_SHAREDMEM))
+@@ -514,7 +514,7 @@ static void THMapAllocator_free(void* ctx, void* data) {
+ #if (defined(_WIN32) || defined(HAVE_MMAP)) && defined(TH_ATOMIC_IPC_REFCOUNT)
+ 
+ static void * THRefcountedMapAllocator_alloc(void *_ctx, ptrdiff_t size) {
+-  THMapAllocatorContext *ctx = _ctx;
++  THMapAllocatorContext *ctx = (THMapAllocatorContext*) _ctx;
+ 
+   if (ctx->flags & TH_ALLOCATOR_MAPPED_FROMFD)
+     THError("THRefcountedMapAllocator doesn't support TH_ALLOCATOR_MAPPED_FROMFD flag");
+@@ -555,7 +555,7 @@ static void *THRefcountedMapAllocator_realloc(void* ctx, void* ptr, ptrdiff_t si
+ }
+ 
+ static void THRefcountedMapAllocator_free(void* ctx_, void* data) {
+-  THMapAllocatorContext *ctx = ctx_;
++  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
+ 
+ #ifdef _WIN32
+   THMapInfo *info = (THMapInfo*)(((char*)data) - TH_ALLOC_ALIGNMENT);
+diff --git a/aten/src/TH/THAtomic.c b/aten/src/TH/THAtomic.c
+index 16f0ddb48..111e28364 100644
+--- a/aten/src/TH/THAtomic.c
++++ b/aten/src/TH/THAtomic.c
+@@ -24,7 +24,7 @@ void THAtomicSet(int32_t volatile *a, int32_t newvalue)
+ #if defined(USE_C11_ATOMICS)
+   atomic_store(a, newvalue);
+ #elif defined(USE_MSC_ATOMICS)
+-  assert(sizeof(int) == sizeof(int32_t));
++  
+   _InterlockedExchange((int32_t*)a, newvalue);
+ #elif defined(USE_GCC_ATOMICS)
+   __sync_lock_test_and_set(a, newvalue);
+diff --git a/aten/src/TH/THDiskFile.c b/aten/src/TH/THDiskFile.c
+index 41cc254f6..0e2dbb3f4 100644
+--- a/aten/src/TH/THDiskFile.c
++++ b/aten/src/TH/THDiskFile.c
+@@ -105,7 +105,7 @@ size_t fread__(void *ptr, size_t size, size_t nitems, FILE *stream)
+       {                                                                 \
+         if(sizeof(TYPE) > 1)                                            \
+         {                                                               \
+-          char *buffer = THAlloc(sizeof(TYPE)*n);                       \
++          char *buffer = (char*) THAlloc(sizeof(TYPE)*n);               \
+           THDiskFile_reverseMemory(buffer, data, sizeof(TYPE), n);      \
+           nwrite = fwrite(buffer, sizeof(TYPE), n, dfself->handle);     \
+           THFree(buffer);                                               \
+@@ -396,7 +396,7 @@ static size_t THDiskFile_readLong(THFile *self, int64_t *data, size_t n)
+     else /* if(dfself->longSize == 8) */
+     {
+       int big_endian = !THDiskFile_isLittleEndianCPU();
+-      int32_t *buffer = THAlloc(8*n);
++      int32_t *buffer = (int32_t*) THAlloc(8*n);
+       nread = fread__(buffer, 8, n, dfself->handle);
+       size_t i;
+       for(i = nread; i > 0; i--)
+@@ -449,14 +449,14 @@ static size_t THDiskFile_writeLong(THFile *self, int64_t *data, size_t n)
+       }
+       else
+       {
+-        char *buffer = THAlloc(sizeof(int64_t)*n);
++        char *buffer = (char*) THAlloc(sizeof(int64_t)*n);
+         THDiskFile_reverseMemory(buffer, data, sizeof(int64_t), n);
+         nwrite = fwrite(buffer, sizeof(int64_t), n, dfself->handle);
+         THFree(buffer);
+       }
+     } else if(dfself->longSize == 4)
+     {
+-      int32_t *buffer = THAlloc(4*n);
++      int32_t *buffer = (int32_t*) THAlloc(4*n);
+       size_t i;
+       for(i = 0; i < n; i++)
+         buffer[i] = (int32_t) data[i];
+@@ -468,7 +468,7 @@ static size_t THDiskFile_writeLong(THFile *self, int64_t *data, size_t n)
+     else /* if(dfself->longSize == 8) */
+     {
+       int big_endian = !THDiskFile_isLittleEndianCPU();
+-      int32_t *buffer = THAlloc(8*n);
++      int32_t *buffer = (int32_t*) THAlloc(8*n);
+       size_t i;
+       for(i = 0; i < n; i++)
+       {
+@@ -517,7 +517,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
+ 
+   if(format[1] == 'a')
+   {
+-    char *p = THAlloc(TBRS_BSZ);
++    char *p = (char*) THAlloc(TBRS_BSZ);
+     size_t total = TBRS_BSZ;
+     size_t pos = 0;
+ 
+@@ -526,7 +526,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
+       if(total-pos == 0) /* we need more space! */
+       {
+         total += TBRS_BSZ;
+-        p = THRealloc(p, total);
++        p = (char*) THRealloc(p, total);
+       }
+       pos += fread(p+pos, 1, total-pos, dfself->handle);
+       if (pos < total) /* eof? */
+@@ -548,7 +548,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
+   }
+   else
+   {
+-    char *p = THAlloc(TBRS_BSZ);
++    char *p = (char*) THAlloc(TBRS_BSZ);
+     size_t total = TBRS_BSZ;
+     size_t pos = 0;
+     size_t size;
+@@ -558,7 +558,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
+       if(total-pos <= 1) /* we can only write '\0' in there! */
+       {
+         total += TBRS_BSZ;
+-        p = THRealloc(p, total);
++        p = (char*) THRealloc(p, total);
+       }
+       if (fgets(p+pos, (int) (total-pos), dfself->handle) == NULL) /* eof? */
+       {
+@@ -677,10 +677,10 @@ THFile *THDiskFile_new(const char *name, const char *mode, int isQuiet)
+       THError("cannot open <%s> in mode %c%c", name, (isReadable ? 'r' : ' '), (isWritable ? 'w' : ' '));
+   }
+ 
+-  self = THAlloc(sizeof(THDiskFile));
++  self = (THDiskFile*) THAlloc(sizeof(THDiskFile));
+ 
+   self->handle = handle;
+-  self->name = THAlloc(strlen(name)+1);
++  self->name = (char*) THAlloc(strlen(name)+1);
+   strcpy(self->name, name);
+   self->isNativeEncoding = 1;
+   self->longSize = 0;
+@@ -781,10 +781,10 @@ THFile *THPipeFile_new(const char *name, const char *mode, int isQuiet)
+       THError("cannot open <%s> in mode %c%c.  This might be because eg the executable doesn't exist, but it could also be because you are out of memory.", name, (isReadable ? 'r' : ' '), (isWritable ? 'w' : ' '));
+   }
+ 
+-  self = THAlloc(sizeof(THDiskFile));
++  self = (THDiskFile*) THAlloc(sizeof(THDiskFile));
+ 
+   self->handle = handle;
+-  self->name = THAlloc(strlen(name)+1);
++  self->name = (char*) THAlloc(strlen(name)+1);
+   strcpy(self->name, name);
+   self->isNativeEncoding = 1;
+   self->longSize = 0;
+diff --git a/aten/src/TH/THMemoryFile.c b/aten/src/TH/THMemoryFile.c
+index cbbcfc1f5..206ce821a 100644
+--- a/aten/src/TH/THMemoryFile.c
++++ b/aten/src/TH/THMemoryFile.c
+@@ -527,7 +527,7 @@ static size_t THMemoryFile_writeLong(THFile *self, int64_t *data, size_t n)
+ 
+ static int8_t* THMemoryFile_cloneString(const int8_t *str, ptrdiff_t size)
+ {
+-  int8_t *cstr = THAlloc(size);
++  int8_t *cstr = (int8_t*) THAlloc(size);
+   memcpy(cstr, str, size);
+   return cstr;
+ }
+@@ -665,7 +665,7 @@ THFile *THMemoryFile_newWithStorage(THCharStorage *storage, const char *mode)
+     storage->data[0] = '\0';
+   }
+ 
+-  mfself = THAlloc(sizeof(THMemoryFile));
++  mfself = (THMemoryFile*) THAlloc(sizeof(THMemoryFile));
+ 
+   mfself->storage = storage;
+   mfself->size = (storage ? storage->size-1 : 0);
+diff --git a/aten/src/TH/THStorage.c b/aten/src/TH/THStorage.c
+index 37df9888e..40000322b 100644
+--- a/aten/src/TH/THStorage.c
++++ b/aten/src/TH/THStorage.c
+@@ -56,7 +56,7 @@ int THLongStorage_inferSize2(THLongStorage *output, int64_t *sizesA, int64_t dim
+   THArgCheck(dimsB, 1, "Can't expand empty tensor b");
+   ptrdiff_t ndim = dimsA > dimsB ? dimsA : dimsB;
+ 
+-  int64_t *expandedSizes = THAlloc(sizeof(int64_t)*ndim);
++  int64_t *expandedSizes = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
+ 
+   for (int64_t i = ndim - 1; i >= 0; --i) {
+     int64_t offset = ndim - 1 - i;
+@@ -92,7 +92,7 @@ int THLongStorage_inferSizeN(THLongStorage *output, int n, int64_t **sizes, int6
+     ndim = dims[ j ] > ndim ? dims[ j ] : ndim;
+   }
+ 
+-  int64_t *expandedSizes = THAlloc(sizeof(int64_t)*ndim);
++  int64_t *expandedSizes = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
+ 
+   for (int64_t i = ndim - 1; i >= 0; --i) {
+     expandedSizes[ i ] = 1;
+@@ -121,8 +121,8 @@ int THLongStorage_inferExpandGeometry(int64_t *tensorSizes, int64_t *tensorStrid
+                                         char *error_buffer, int buffer_len) {
+   ptrdiff_t ndim = THLongStorage_size(sizes);
+ 
+-  int64_t *expandedSizesCalc = THAlloc(sizeof(int64_t)*ndim);
+-  int64_t *expandedStridesCalc = THAlloc(sizeof(int64_t)*ndim);
++  int64_t *expandedSizesCalc = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
++  int64_t *expandedStridesCalc = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
+ 
+   // create a new geometry for the tensors
+   for (int64_t i = ndim - 1; i >= 0; --i) {
+diff --git a/aten/src/TH/generic/THStorage.c b/aten/src/TH/generic/THStorage.c
+index 70c596e63..389f894e9 100644
+--- a/aten/src/TH/generic/THStorage.c
++++ b/aten/src/TH/generic/THStorage.c
+@@ -31,8 +31,8 @@ THStorage* THStorage_(newWithAllocator)(ptrdiff_t size,
+                                         THAllocator *allocator,
+                                         void *allocatorContext)
+ {
+-  THStorage *storage = THAlloc(sizeof(THStorage));
+-  storage->data = allocator->malloc(allocatorContext, sizeof(real)*size);
++  THStorage *storage = (THStorage*) THAlloc(sizeof(THStorage));
++  storage->data = (real*) allocator->malloc(allocatorContext, sizeof(real)*size);
+   storage->size = size;
+   storage->refcount = 1;
+   storage->flag = TH_STORAGE_REFCOUNTED | TH_STORAGE_RESIZABLE | TH_STORAGE_FREEMEM;
+@@ -136,7 +136,7 @@ THStorage* THStorage_(newWithData)(real *data, ptrdiff_t size)
+ THStorage* THStorage_(newWithDataAndAllocator)(real* data, ptrdiff_t size,
+                                                THAllocator* allocator,
+                                                void* allocatorContext) {
+-  THStorage *storage = THAlloc(sizeof(THStorage));
++  THStorage *storage = (THStorage*) THAlloc(sizeof(THStorage));
+   storage->data = data;
+   storage->size = size;
+   storage->refcount = 1;
+@@ -157,7 +157,7 @@ void THStorage_(resize)(THStorage *storage, ptrdiff_t size)
+       if (size == 0) {
+         storage->data = NULL;
+       } else {
+-        storage->data = storage->allocator->malloc(
++        storage->data = (real*) storage->allocator->malloc(
+             storage->allocatorContext,
+             sizeof(real)*size);
+       }
+@@ -173,7 +173,7 @@ void THStorage_(resize)(THStorage *storage, ptrdiff_t size)
+         storage->allocator->free(storage->allocatorContext, old_data);
+       }
+     } else {
+-      storage->data = storage->allocator->realloc(
++      storage->data = (real*) storage->allocator->realloc(
+               storage->allocatorContext,
+               storage->data,
+               sizeof(real)*size);
+diff --git a/aten/src/TH/generic/THTensorMath.c b/aten/src/TH/generic/THTensorMath.c
+index 692742e88..f47dd08a6 100644
+--- a/aten/src/TH/generic/THTensorMath.c
++++ b/aten/src/TH/generic/THTensorMath.c
+@@ -6,13 +6,13 @@
+   #define NAN (nan(NULL))
+ #endif
+ 
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+ #include <omp.h>
+ #endif
+ 
+ #define TH_OMP_OVERHEAD_THRESHOLD 100000
+ 
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+ 
+ #ifndef _WIN32
+ #define PRAGMA(P) _Pragma(#P)
+@@ -45,7 +45,7 @@
+ }
+ #endif
+ 
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+ #define TH_TENSOR_APPLY2_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, CODE) \
+ { \
+   int inOmp = omp_in_parallel(); \
+@@ -73,7 +73,7 @@
+ }
+ #endif
+ 
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+ #define TH_TENSOR_APPLY3_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, TYPE3, TENSOR3, CODE) \
+ { \
+   int inOmp = omp_in_parallel(); \
+@@ -774,7 +774,7 @@ accreal THTensor_(sumall)(THTensor *tensor)
+ {
+   accreal sum = 0;
+   int serial_path = 0;
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+   int inOMP = omp_in_parallel();
+   if(inOMP) {
+     serial_path = 1;
+@@ -794,7 +794,7 @@ accreal THTensor_(prodall)(THTensor *tensor)
+ {
+   accreal prod = 1;
+   int serial_path = 0;
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+   int inOMP = omp_in_parallel();
+   if(inOMP) {
+     serial_path = 1;
+@@ -820,7 +820,7 @@ void THTensor_(add)(THTensor *r_, THTensor *t, real value)
+   if (r_Contig && tContig) {
+     TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(adds)(r__data, t_data, value, r__len););
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -861,7 +861,7 @@ void THTensor_(mul)(THTensor *r_, THTensor *t, real value)
+   if (r_Contig && tContig) {
+     TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(muls)(r__data, t_data, value, r__len););
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -887,7 +887,7 @@ void THTensor_(div)(THTensor *r_, THTensor *t, real value)
+   if (r_Contig && tContig) {
+     TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(divs)(r__data, t_data, value, r__len););
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -930,7 +930,7 @@ void THTensor_(lshift)(THTensor *r_, THTensor *t, real value)
+ #endif
+     }
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -982,7 +982,7 @@ void THTensor_(rshift)(THTensor *r_, THTensor *t, real value)
+ #endif
+     }
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -1027,7 +1027,7 @@ void THTensor_(fmod)(THTensor *r_, THTensor *t, real value)
+ #endif
+     }
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -1078,7 +1078,7 @@ void THTensor_(remainder)(THTensor *r_, THTensor *t, real value)
+ #endif
+     }
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -1128,7 +1128,7 @@ void THTensor_(bitand)(THTensor *r_, THTensor *t, real value)
+       rp[i] = tp[i] & value;
+     }
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -1167,7 +1167,7 @@ void THTensor_(bitor)(THTensor *r_, THTensor *t, real value)
+       rp[i] = tp[i] | value;
+     }
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -1206,7 +1206,7 @@ void THTensor_(bitxor)(THTensor *r_, THTensor *t, real value)
+       rp[i] = tp[i] ^ value;
+     }
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -1239,7 +1239,7 @@ void THTensor_(clamp)(THTensor *r_, THTensor *t, real min_value, real max_value)
+     for (i=0; i<r_Size; i++)
+       rp[i] = (tp[i] < min_value) ? min_value : (tp[i] > max_value ? max_value : tp[i]);
+   } else {
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -1272,7 +1272,7 @@ void THTensor_(cadd)(THTensor *r_, THTensor *t, real value, THTensor *src)
+         TH_TENSOR_APPLY3_CONTIG(real, r_, real, t, real, src, THVector_(cadd)(r__data, t_data, src_data, value, r__len););
+       }
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1309,7 +1309,7 @@ void THTensor_(cmul)(THTensor *r_, THTensor *t, THTensor *src)
+     if (r_Contig && tContig && srcContig) {
+       TH_TENSOR_APPLY3_CONTIG(real, r_, real, t, real, src, THVector_(cmul)(r__data, t_data, src_data, r__len););
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1388,7 +1388,7 @@ void THTensor_(cpow)(THTensor *r_, THTensor *t, THTensor *src)
+       for (i=0; i<r_Size; i++)
+         rp[i] = THTensor_(powOne)(tp[i], sp[i]);
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1420,7 +1420,7 @@ void THTensor_(cdiv)(THTensor *r_, THTensor *t, THTensor *src)
+     if (r_Contig && tContig && srcContig) {
+       TH_TENSOR_APPLY3_CONTIG(real, r_, real, t, real, src, THVector_(cdiv)(r__data, t_data, src_data, r__len););
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1470,7 +1470,7 @@ void THTensor_(clshift)(THTensor *r_, THTensor *t, THTensor *src)
+ #endif
+       }
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1536,7 +1536,7 @@ void THTensor_(crshift)(THTensor *r_, THTensor *t, THTensor *src)
+ #endif
+       }
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1595,7 +1595,7 @@ void THTensor_(cfmod)(THTensor *r_, THTensor *t, THTensor *src)
+ #endif
+       }
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1649,7 +1649,7 @@ void THTensor_(cremainder)(THTensor *r_, THTensor *t, THTensor *src)
+ #endif
+       }
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1706,7 +1706,7 @@ void THTensor_(cbitand)(THTensor *r_, THTensor *t, THTensor *src)
+         rp[i] = tp[i] & sp[i];
+       }
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1752,7 +1752,7 @@ void THTensor_(cbitor)(THTensor *r_, THTensor *t, THTensor *src)
+         rp[i] = tp[i] | sp[i];
+       }
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1798,7 +1798,7 @@ void THTensor_(cbitxor)(THTensor *r_, THTensor *t, THTensor *src)
+         rp[i] = tp[i] ^ sp[i];
+       }
+     } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+       int inOMP = omp_in_parallel();
+       if (inOMP) {
+         serial_path = 1;
+@@ -1833,7 +1833,7 @@ void THTensor_(tpow)(THTensor *r_, real value, THTensor *t)
+     for (i=0; i<r_Size; i++)
+       rp[i] = THTensor_(powOne)(value, tp[i]);
+   } else {
+-#if _OPENMP
++#if _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -1864,7 +1864,7 @@ void THTensor_(addcmul)(THTensor *r_, THTensor *t, real value, THTensor *src1, T
+   int src2Contig = THTensor_(isContiguous)(src2);
+   int serial_path = 0;
+   if( (src1Size == src2Size) && (src1Size == r_Size) ){
+-#if _OPENMP
++#if _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -1897,7 +1897,7 @@ void THTensor_(addcdiv)(THTensor *r_, THTensor *t, real value, THTensor *src1, T
+   int src2Contig = THTensor_(isContiguous)(src2);
+   int serial_path = 0;
+   if( (src1Size == src2Size) && (src1Size == r_Size) ){
+-#if _OPENMP
++#if _OPENMP_STUBBED
+     int inOMP = omp_in_parallel();
+     if (inOMP) {
+       serial_path = 1;
+@@ -2508,7 +2508,7 @@ void THTensor_(sum)(THTensor *r_, THTensor *t, int dimension, int keepdim)
+   THLongStorage_free(dim);
+ 
+   int serial_path = 0;
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+   int inOMP = omp_in_parallel();
+   if (inOMP) {
+     serial_path = 1;
+@@ -2588,7 +2588,7 @@ void THTensor_(prod)(THTensor *r_, THTensor *t, int dimension, int keepdim)
+   THLongStorage_free(dim);
+ 
+   int serial_path = 0;
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+   int inOMP = omp_in_parallel();
+   if (inOMP) {
+     serial_path = 1;
+@@ -3740,7 +3740,7 @@ TENSOR_IMPLEMENT_LOGICAL(eq,==)
+ TENSOR_IMPLEMENT_LOGICAL(ne,!=)
+ 
+ 
+-#ifdef _OPENMP
++#ifdef _OPENMP_STUBBED
+ 
+ #define LAB_IMPLEMENT_BASIC_FUNCTION(NAME, CFUNC)             \
+   void THTensor_(NAME)(THTensor *r_, THTensor *t)             \
+diff --git a/aten/src/THC/THCAllocator.c b/aten/src/THC/THCAllocator.c
+index 554a379d7..4ff8b0c55 100644
+--- a/aten/src/THC/THCAllocator.c
++++ b/aten/src/THC/THCAllocator.c
+@@ -64,7 +64,7 @@ static void *THCUVAAllocator_alloc(void* ctx, ptrdiff_t size) {
+   // See J.1.1 of the CUDA_C_Programming_Guide.pdf for UVA and coherence rules
+   // on various compute capabilities.
+   void* ptr;
+-  THCudaCheck(cudaMallocManaged(&ptr, size, cudaMemAttachGlobal));
++  THCudaCheck(cudaSuccess);
+   return ptr;
+ }
+ 
+diff --git a/aten/src/THC/THCApply.cuh b/aten/src/THC/THCApply.cuh
+index 42ac1caf2..e2818e4d4 100644
+--- a/aten/src/THC/THCApply.cuh
++++ b/aten/src/THC/THCApply.cuh
+@@ -167,7 +167,7 @@ inline dim3 getApplyBlock() {
+ 
+ inline bool getApplyGrid(THCState* state, uint64_t totalElements, dim3& grid) {
+   int curDevice = -1;
+-  cudaGetDevice(&curDevice);
++  hipGetDevice(&curDevice);
+   if (curDevice == -1) return false;
+ 
+   uint64_t numBlocks = THCCeilDiv(totalElements, static_cast<uint64_t>(THC_APPLY_THREADS_PER_BLOCK));
+@@ -234,6 +234,17 @@ bool THC_pointwiseApply1(THCState* state,
+   // (or vice versa), the contiguous tensor can be collapsed to one
+   // dimension, and the loop to translate the linear index to the array
+   // index can be similarly collapsed. That is what this unrolling is for.
++#if defined(__HIP_PLATFORM_HCC__)
++#define HANDLE_CASE(TYPE, A)                                            \
++ hipLaunchKernelGGL(                                                    \
++  (kernelPointwiseApply1<Op,                                            \
++                        typename TensorUtils<TensorTypeA>::DataType,    \
++                        TYPE, A>),                                      \
++      grid, block, 0, THCState_getCurrentStream(state),                 \
++      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
++          (aInfo),                                                      \
++      (TYPE) totalElements, op);
++#else
+ #define HANDLE_CASE(TYPE, A)                                            \
+   kernelPointwiseApply1<Op,                                             \
+                         typename TensorUtils<TensorTypeA>::DataType,    \
+@@ -242,7 +253,7 @@ bool THC_pointwiseApply1(THCState* state,
+       OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+           (aInfo),                                                      \
+       (TYPE) totalElements, op);
+-
++#endif
+ #define HANDLE_A_CASE(TYPE, A)                  \
+   {                                             \
+     if (aInfo.isContiguous()) {                 \
+@@ -288,11 +299,20 @@ bool THC_pointwiseApply1(THCState* state,
+     if (aInfo.isContiguous()) {
+       OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -2>
+         aOffset(aInfo);
++#if defined(__HIP_PLATFORM_HCC__)
++    hipLaunchKernelGGL(
++      (kernelPointwiseApply1<Op,
++                            typename TensorUtils<TensorTypeA>::DataType,
++                            uint64_t, -2>),
++          grid, block, 0, THCState_getCurrentStream(state),
++          aOffset, (uint64_t) totalElements, op);
++#else
+       kernelPointwiseApply1<Op,
+                             typename TensorUtils<TensorTypeA>::DataType,
+                             uint64_t, -2>
+         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+           aOffset, (uint64_t) totalElements, op);
++#endif
+     } else {
+ 
+ #if CUDA_VERSION < 9000
+@@ -300,11 +320,20 @@ bool THC_pointwiseApply1(THCState* state,
+ #endif
+       OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -1>
+         aOffset(aInfo);
++#if defined(__HIP_PLATFORM_HCC__)
++     hipLaunchKernelGGL(
++      (kernelPointwiseApply1<Op,
++                            typename TensorUtils<TensorTypeA>::DataType,
++                            uint64_t, -1>),
++          grid, block, 0, THCState_getCurrentStream(state),
++          aOffset, (uint64_t) totalElements, op);
++#else
+       kernelPointwiseApply1<Op,
+                             typename TensorUtils<TensorTypeA>::DataType,
+                             uint64_t, -1>
+         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+           aOffset, (uint64_t) totalElements, op);
++#endif
+     }
+   }
+ #undef HANDLE_CASE
+@@ -387,6 +416,20 @@ bool THC_pointwiseApply2(THCState* state,
+   // (or vice versa), the contiguous tensor can be collapsed to one
+   // dimension, and the loop to translate the linear index to the array
+   // index can be similarly collapsed. That is what this unrolling is for.
++#if defined(__HIP_PLATFORM_HCC__)
++#define HANDLE_CASE(TYPE, A, B)                                         \
++  hipLaunchKernelGGL(                                                   \
++   (kernelPointwiseApply2<Op,                                           \
++                        typename TensorUtils<TensorTypeA>::DataType,    \
++                        typename TensorUtils<TensorTypeB>::DataType,    \
++                        TYPE, A, B>),                                   \
++      grid, block, 0, THCState_getCurrentStream(state),                 \
++      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
++          (aInfo),                                                      \
++      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
++          (bInfo),                                                      \
++      (TYPE) totalElements, op);
++#else
+ #define HANDLE_CASE(TYPE, A, B)                                         \
+   kernelPointwiseApply2<Op,                                             \
+                         typename TensorUtils<TensorTypeA>::DataType,    \
+@@ -398,7 +441,7 @@ bool THC_pointwiseApply2(THCState* state,
+       OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
+           (bInfo),                                                      \
+       (TYPE) totalElements, op);
+-
++#endif
+ #define HANDLE_B_CASE(TYPE, A, B)               \
+   {                                             \
+     if (bInfo.isContiguous()) {                 \
+@@ -473,12 +516,22 @@ bool THC_pointwiseApply2(THCState* state,
+         aOffset(aInfo);
+       OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t, -2>
+         bOffset(bInfo);
++#if defined(__HIP_PLATFORM_HCC__)
++  hipLaunchKernelGGL(
++      (kernelPointwiseApply2<Op,
++                            typename TensorUtils<TensorTypeA>::DataType,
++                            typename TensorUtils<TensorTypeB>::DataType,
++                            uint64_t, -2, -2>),
++          grid, block, 0, THCState_getCurrentStream(state),
++          aOffset, bOffset, (uint64_t) totalElements, op);
++#else
+       kernelPointwiseApply2<Op,
+                             typename TensorUtils<TensorTypeA>::DataType,
+                             typename TensorUtils<TensorTypeB>::DataType,
+                             uint64_t, -2, -2>
+         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+           aOffset, bOffset, (uint64_t) totalElements, op);
++#endif
+     } else {
+ #if CUDA_VERSION < 9000
+       grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
+@@ -487,12 +540,22 @@ bool THC_pointwiseApply2(THCState* state,
+         aOffset(aInfo);
+       OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t, -1>
+         bOffset(bInfo);
++#if defined(__HIP_PLATFORM_HCC__)
++  hipLaunchKernelGGL(
++      (kernelPointwiseApply2<Op,
++                            typename TensorUtils<TensorTypeA>::DataType,
++                            typename TensorUtils<TensorTypeB>::DataType,
++                            uint64_t, -1, -1>),
++          grid, block, 0, THCState_getCurrentStream(state),
++          aOffset, bOffset, (uint64_t) totalElements, op);
++#else
+       kernelPointwiseApply2<Op,
+                             typename TensorUtils<TensorTypeA>::DataType,
+                             typename TensorUtils<TensorTypeB>::DataType,
+                             uint64_t, -1, -1>
+         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+           aOffset, bOffset, (uint64_t) totalElements, op);
++#endif
+     }
+   }
+ #undef HANDLE_CASE
+@@ -588,7 +651,23 @@ bool THC_pointwiseApply3(THCState* state,
+     oldC = c;
+     c = TensorUtils<TensorTypeC>::newContiguous(state, c);
+   }
+-
++#if defined(__HIP_PLATFORM_HCC__)
++#define HANDLE_CASE(TYPE, A, B, C)                                      \
++hipLaunchKernelGGL(                                                     \
++  (kernelPointwiseApply3<Op,                                            \
++                        typename TensorUtils<TensorTypeA>::DataType,    \
++                        typename TensorUtils<TensorTypeB>::DataType,    \
++                        typename TensorUtils<TensorTypeC>::DataType,    \
++                        TYPE, A, B, C>),                                \
++      grid, block, 0, THCState_getCurrentStream(state),                 \
++      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
++          (aInfo),                                                      \
++      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
++          (bInfo),                                                      \
++      OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, TYPE, C>  \
++          (cInfo),                                                      \
++      (TYPE) totalElements, op);
++#else
+ #define HANDLE_CASE(TYPE, A, B, C)                                      \
+   kernelPointwiseApply3<Op,                                             \
+                         typename TensorUtils<TensorTypeA>::DataType,    \
+@@ -603,6 +682,7 @@ bool THC_pointwiseApply3(THCState* state,
+       OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, TYPE, C>  \
+           (cInfo),                                                      \
+       (TYPE) totalElements, op);
++#endif
+ 
+ #define HANDLE_C_CASE(TYPE, A, B, C)            \
+   {                                             \
+@@ -708,6 +788,16 @@ bool THC_pointwiseApply3(THCState* state,
+         bOffset(bInfo);
+       OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, uint64_t, -2>
+         cOffset(cInfo);
++#if defined(__HIP_PLATFORM_HCC__)
++hipLaunchKernelGGL(
++      (kernelPointwiseApply3<Op,
++                            typename TensorUtils<TensorTypeA>::DataType,
++                            typename TensorUtils<TensorTypeB>::DataType,
++                            typename TensorUtils<TensorTypeC>::DataType,
++                            uint64_t, -2, -2, -2>),
++          grid, block, 0, THCState_getCurrentStream(state),
++          aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
++#else
+       kernelPointwiseApply3<Op,
+                             typename TensorUtils<TensorTypeA>::DataType,
+                             typename TensorUtils<TensorTypeB>::DataType,
+@@ -715,6 +805,7 @@ bool THC_pointwiseApply3(THCState* state,
+                             uint64_t, -2, -2, -2>
+         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+           aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
++#endif
+     } else {
+ #if CUDA_VERSION < 9000
+       grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
+@@ -726,6 +817,16 @@ bool THC_pointwiseApply3(THCState* state,
+         bOffset(bInfo);
+       OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, uint64_t, -1>
+         cOffset(cInfo);
++#if defined(__HIP_PLATFORM_HCC__)
++hipLaunchKernelGGL(
++      (kernelPointwiseApply3<Op,
++                            typename TensorUtils<TensorTypeA>::DataType,
++                            typename TensorUtils<TensorTypeB>::DataType,
++                            typename TensorUtils<TensorTypeC>::DataType,
++                            uint64_t, -1, -1, -1>),
++          grid, block, 0, THCState_getCurrentStream(state),
++          aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
++#else
+       kernelPointwiseApply3<Op,
+                             typename TensorUtils<TensorTypeA>::DataType,
+                             typename TensorUtils<TensorTypeB>::DataType,
+@@ -733,6 +834,7 @@ bool THC_pointwiseApply3(THCState* state,
+                             uint64_t, -1, -1, -1>
+         <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+           aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
++#endif
+     }
+   }
+ #undef HANDLE_CASE
+diff --git a/aten/src/THC/THCAsmUtils.cuh b/aten/src/THC/THCAsmUtils.cuh
+index 3b1db9a8c..ffd1742d9 100644
+--- a/aten/src/THC/THCAsmUtils.cuh
++++ b/aten/src/THC/THCAsmUtils.cuh
+@@ -8,69 +8,92 @@ struct Bitfield {};
+ 
+ template <>
+ struct Bitfield<unsigned int> {
+-  static __device__ __forceinline__
+-  unsigned int getBitfield(unsigned int val, int pos, int len) {
+-    unsigned int ret;
+-    asm("bfe.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(val), "r"(pos), "r"(len));
+-    return ret;
++  static __device__
++  inline
++  unsigned int getBitfield(unsigned int val, int pos, int len)
++  {
++    pos &= 0x1f;
++    len &= 0x1f;
++
++    unsigned int m = (1u << len) - 1u;
++    m <<= pos;
++    return val & m;
+   }
+ 
+-  static __device__ __forceinline__
+-  unsigned int setBitfield(unsigned int val, unsigned int toInsert, int pos, int len) {
+-    unsigned int ret;
+-    asm("bfi.b32 %0, %1, %2, %3, %4;" :
+-        "=r"(ret) : "r"(toInsert), "r"(val), "r"(pos), "r"(len));
+-    return ret;
++ static  __device__
++  inline
++  unsigned int setBitfield(
++    unsigned int val, unsigned int toInsert, int pos, int len)
++  {
++    pos &= 0x1f;
++    len &= 0x1f;
++
++    unsigned int m = (1u << len) - 1u;
++    toInsert &= m;
++    toInsert <<= pos;
++    m <<= pos;
++
++    return (val & ~m) | toInsert;
+   }
+ };
+ 
+-template <>
+-struct Bitfield<uint64_t> {
+-  static __device__ __forceinline__
+-  uint64_t getBitfield(uint64_t val, int pos, int len) {
+-    uint64_t ret;
+-    asm("bfe.u64 %0, %1, %2, %3;" : "=l"(ret) : "l"(val), "r"(pos), "r"(len));
+-    return ret;
++template<>
++struct Bitfield<uint64_t>{
++  static __device__
++  inline
++  uint64_t getBitfield(uint64_t val, int pos, int len)
++  {
++    pos &= 0x1f;
++    len &= 0x1f;
++
++    uint64_t m = (1u << len) - 1u;
++    m <<= pos;
++    return val & m;
+   }
+ 
+-  static __device__ __forceinline__
+-  uint64_t setBitfield(uint64_t val, uint64_t toInsert, int pos, int len) {
+-    uint64_t ret;
+-    asm("bfi.b64 %0, %1, %2, %3, %4;" :
+-        "=l"(ret) : "l"(toInsert), "l"(val), "r"(pos), "r"(len));
+-    return ret;
++  static __device__
++  inline
++  uint64_t setBitfield(
++    uint64_t val, uint64_t toInsert, int pos, int len)
++  {
++    pos &= 0x1f;
++    len &= 0x1f;
++
++    uint64_t m = (1u << len) - 1u;
++    toInsert &= m;
++    toInsert <<= pos;
++    m <<= pos;
++
++    return (val & ~m) | toInsert;
+   }
+ };
+ 
+-__device__ __forceinline__ int getLaneId() {
+-  int laneId;
+-  asm("mov.s32 %0, %laneid;" : "=r"(laneId) );
+-  return laneId;
++__device__ __forceinline__ inline int getLaneId() {
++    return hc::__lane_id();
+ }
+ 
+-__device__ __forceinline__ unsigned getLaneMaskLt() {
+-  unsigned mask;
+-  asm("mov.u32 %0, %%lanemask_lt;" : "=r"(mask));
+-  return mask;
++__device__ inline std::uint64_t getLaneMaskLt()
++{
++  std::uint64_t m = (1ull << getLaneId()) - 1ull;
++  return m;
+ }
+ 
+-__device__ __forceinline__ unsigned getLaneMaskLe() {
+-  unsigned mask;
+-  asm("mov.u32 %0, %%lanemask_le;" : "=r"(mask));
+-  return mask;
++__device__ inline std::uint64_t getLaneMaskLe()
++{
++  std::uint64_t m = (1ull << (getLaneId() + 1ull)) - 1ull;
++  return m;
+ }
+ 
+-__device__ __forceinline__ unsigned getLaneMaskGt() {
+-  unsigned mask;
+-  asm("mov.u32 %0, %%lanemask_gt;" : "=r"(mask));
+-  return mask;
++__device__ inline std::uint64_t getLaneMaskGt()
++{
++  std::uint64_t m = getLaneMaskLe();
++  return m ? ~m : m;
+ }
+ 
+-__device__ __forceinline__ unsigned getLaneMaskGe() {
+-  unsigned mask;
+-  asm("mov.u32 %0, %%lanemask_ge;" : "=r"(mask));
+-  return mask;
++__device__ inline std::uint64_t getLaneMaskGe()
++{
++  std::uint64_t m = getLaneMaskLt();
++  return ~m;
+ }
+ 
+-
+ #endif // THC_ASM_UTILS_INC
+diff --git a/aten/src/THC/THCAtomics.cuh b/aten/src/THC/THCAtomics.cuh
+index 9e54c56dc..f0283b996 100644
+--- a/aten/src/THC/THCAtomics.cuh
++++ b/aten/src/THC/THCAtomics.cuh
+@@ -103,7 +103,11 @@ static inline  __device__ void atomicAdd(half *address, half val) {
+ 
+   do {
+     assumed = old;
+-#if CUDA_VERSION < 9000
++#if defined(__HIP_PLATFORM_HCC__)
++    half hsum;
++    hsum = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
++    hsum = THCNumerics<half>::add(hsum, val);
++#elif CUDA_VERSION < 9000
+     half hsum;
+     hsum.x = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
+     hsum = THCNumerics<half>::add(hsum, val);
+@@ -113,7 +117,11 @@ static inline  __device__ void atomicAdd(half *address, half val) {
+     half tmpres = THCNumerics<half>::add(hsum, val);
+     hsum = __half_raw(tmpres);
+ #endif
++#if defined(__HIP_PLATFORM_HCC__)
++    // old = (size_t)address & 2 ? (old & 0xffff) | (hsum << 16) : (old & 0xffff0000) | hsum;
++#else
+     old = (size_t)address & 2 ? (old & 0xffff) | (hsum.x << 16) : (old & 0xffff0000) | hsum.x;
++#endif
+     old = atomicCAS(address_as_ui, assumed, old);
+   } while (assumed != old);
+ }
+diff --git a/aten/src/THC/THCBlas.cu b/aten/src/THC/THCBlas.cu
+index e2003da57..8abb1639b 100644
+--- a/aten/src/THC/THCBlas.cu
++++ b/aten/src/THC/THCBlas.cu
+@@ -2,7 +2,8 @@
+ #include "THCGeneral.h"
+ #include "THCHalf.h"
+ 
+-float THCudaBlas_Sdot(THCState *state, int64_t n, float *x, int64_t incx, float *y, int64_t incy)
++
++float THCudaBlas_Sdot(THCState *state, long n, float *x, long incx, float *y, long incy)
+ {
+   if (n == 1) {
+     incx = 1;
+@@ -14,9 +15,9 @@ float THCudaBlas_Sdot(THCState *state, int64_t n, float *x, int64_t incx, float
+     int i_incx = (int)incx;
+     int i_incy = (int)incy;
+     float result;
+-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-    cublasSetStream(handle, THCState_getCurrentStream(state));
+-    THCublasCheck(cublasSdot(handle, i_n, x, i_incx, y, i_incy, &result));
++    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++    hipblasSetStream(handle, THCState_getCurrentStream(state));
++    THCublasCheck(hipblasSdot(handle, i_n, x, i_incx, y, i_incy, &result));
+     return result;
+   }
+ 
+@@ -25,7 +26,7 @@ float THCudaBlas_Sdot(THCState *state, int64_t n, float *x, int64_t incx, float
+   return 0;
+ }
+ 
+-double THCudaBlas_Ddot(THCState *state, int64_t n, double *x, int64_t incx, double *y, int64_t incy)
++double THCudaBlas_Ddot(THCState *state, long n, double *x, long incx, double *y, long incy)
+ {
+   if (n == 1) {
+     incx = 1;
+@@ -37,9 +38,9 @@ double THCudaBlas_Ddot(THCState *state, int64_t n, double *x, int64_t incx, doub
+     int i_incx = (int)incx;
+     int i_incy = (int)incy;
+     double result;
+-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-    cublasSetStream(handle, THCState_getCurrentStream(state));
+-    THCublasCheck(cublasDdot(handle, i_n, x, i_incx, y, i_incy, &result));
++    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++    hipblasSetStream(handle, THCState_getCurrentStream(state));
++    THCublasCheck(hipblasDdot(handle, i_n, x, i_incx, y, i_incy, &result));
+     return result;
+   }
+ 
+@@ -49,7 +50,7 @@ double THCudaBlas_Ddot(THCState *state, int64_t n, double *x, int64_t incx, doub
+ }
+ 
+ #ifdef CUDA_HALF_TENSOR
+-half THCudaBlas_Hdot(THCState *state, int64_t n, half *x, int64_t incx, half *y, int64_t incy)
++half THCudaBlas_Hdot(THCState *state, long n, half *x, long incx, half *y, long incy)
+ {
+ #if CUDA_VERSION >= 8000
+   if (n == 1) {
+@@ -59,15 +60,19 @@ half THCudaBlas_Hdot(THCState *state, int64_t n, half *x, int64_t incx, half *y,
+ 
+   if ((n <= INT_MAX) && (incx <= INT_MAX) && (incy <= INT_MAX)) {
+     half result;
++#ifdef HIPBLAS_TODO
++    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++    hipblasSetStream(handle, THCState_getCurrentStream(state));
++    THCublasCheck(hipblasDotEx(handle, n, x, CUDA_R_16F, incx, y, CUDA_R_16F, incy, &result, CUDA_R_16F, CUDA_R_32F));
++#else
++#ifdef __NVCC__
+     cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-    cublasSetStream(handle, THCState_getCurrentStream(state));
+-    THCublasCheck(cublasDotEx(handle, n,
+-                              x, CUDA_R_16F, incx,
+-                              y, CUDA_R_16F, incy,
+-                              &result, CUDA_R_16F,
+-                              CUDA_R_32F));
++    cublasSetStream((cublasHandle_t)handle, THCState_getCurrentStream(state));
++    cublasDotEx(handle, n, x, CUDA_R_16F, incx, y, CUDA_R_16F, incy, &result, CUDA_R_16F, CUDA_R_32F);
++#endif
++#endif
+     return result;
+-  }
++   }
+ 
+   THError("Cublas_Hdot only supports n, incx and incy "
+           "up to signed integer limits: %d", INT_MAX);
+@@ -80,15 +85,15 @@ half THCudaBlas_Hdot(THCState *state, int64_t n, half *x, int64_t incx, half *y,
+ #endif
+ 
+ /* Level 2 */
+-void THCudaBlas_Sgemv(THCState *state, char trans, int64_t m, int64_t n, float alpha, float *a, int64_t lda, float *x, int64_t incx, float beta, float *y, int64_t incy)
++void THCudaBlas_Sgemv(THCState *state, char trans, long m, long n, float alpha, float *a, long lda, float *x, long incx, float beta, float *y, long incy)
+ {
+   if(n == 1)
+     lda = m;
+ 
+-  cublasOperation_t op;
+-  if (trans == 't') op = CUBLAS_OP_T;
+-  else if (trans == 'n') op = CUBLAS_OP_N;
+-  else if (trans == 'c') op = CUBLAS_OP_C;
++  hipblasOperation_t op;
++  if (trans == 't') op = HIPBLAS_OP_T;
++  else if (trans == 'n') op = HIPBLAS_OP_N;
++  else if (trans == 'c') op = HIPBLAS_OP_C;
+   else THError("Cublas_Sgemv parameter trans should be 't', 'n' or 'c'.");
+ 
+   if( (m <= INT_MAX) && (n <= INT_MAX) &&
+@@ -102,24 +107,25 @@ void THCudaBlas_Sgemv(THCState *state, char trans, int64_t m, int64_t n, float a
+     int i_incx = (int)incx;
+     int i_incy = (int)incy;
+ 
+-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-    cublasSetStream(handle, THCState_getCurrentStream(state));
+-    THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
++    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++    hipblasSetStream(handle, THCState_getCurrentStream(state));
++    THCublasCheck(hipblasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
++
+     return;
+   }
+   THError("Cublas_Sgemv only supports m, n, lda, incx, incy"
+           "in the range 0 < [val] <= %d", INT_MAX);
+ }
+ 
+-void THCudaBlas_Dgemv(THCState *state, char trans, int64_t m, int64_t n, double alpha, double *a, int64_t lda, double *x, int64_t incx, double beta, double *y, int64_t incy)
++void THCudaBlas_Dgemv(THCState *state, char trans, long m, long n, double alpha, double *a, long lda, double *x, long incx, double beta, double *y, long incy)
+ {
+   if(n == 1)
+     lda = m;
+ 
+-  cublasOperation_t op;
+-  if (trans == 't') op = CUBLAS_OP_T;
+-  else if (trans == 'n') op = CUBLAS_OP_N;
+-  else if (trans == 'c') op = CUBLAS_OP_C;
++  hipblasOperation_t op;
++  if (trans == 't') op = HIPBLAS_OP_T;
++  else if (trans == 'n') op = HIPBLAS_OP_N;
++  else if (trans == 'c') op = HIPBLAS_OP_C;
+   else THError("Cublas_Sgemv parameter trans should be 't', 'n' or 'c'.");
+ 
+   if( (m <= INT_MAX) && (n <= INT_MAX) &&
+@@ -133,16 +139,16 @@ void THCudaBlas_Dgemv(THCState *state, char trans, int64_t m, int64_t n, double
+     int i_incx = (int)incx;
+     int i_incy = (int)incy;
+ 
+-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-    cublasSetStream(handle, THCState_getCurrentStream(state));
+-    THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
++    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++    hipblasSetStream(handle, THCState_getCurrentStream(state));
++    THCublasCheck(hipblasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
+     return;
+   }
+   THError("Cublas_Dgemv only supports m, n, lda, incx, incy"
+           "in the range 0 < [val] <= %d", INT_MAX);
+ }
+ 
+-void THCudaBlas_Sger(THCState *state, int64_t m, int64_t n, float alpha, float *x, int64_t incx, float *y, int64_t incy, float *a, int64_t lda)
++void THCudaBlas_Sger(THCState *state, long m, long n, float alpha, float *x, long incx, float *y, long incy, float *a, long lda)
+ {
+   if(n == 1)
+     lda = m;
+@@ -154,17 +160,16 @@ void THCudaBlas_Sger(THCState *state, int64_t m, int64_t n, float alpha, float *
+       int i_lda = (int)lda;
+       int i_incx = (int)incx;
+       int i_incy = (int)incy;
+-
+-      cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-      cublasSetStream(handle, THCState_getCurrentStream(state));
+-      THCublasCheck(cublasSger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
++      hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++      hipblasSetStream(handle, THCState_getCurrentStream(state));
++      THCublasCheck(hipblasSger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
+       return;
+     }
+   THError("Cublas_Sger only supports m, n, lda, incx, incy"
+           "with the bound [val] <= %d", INT_MAX);
+ }
+ 
+-void THCudaBlas_Dger(THCState *state, int64_t m, int64_t n, double alpha, double *x, int64_t incx, double *y, int64_t incy, double *a, int64_t lda)
++void THCudaBlas_Dger(THCState *state, long m, long n, double alpha, double *x, long incx, double *y, long incy, double *a, long lda)
+ {
+   if(n == 1)
+     lda = m;
+@@ -176,17 +181,33 @@ void THCudaBlas_Dger(THCState *state, int64_t m, int64_t n, double alpha, double
+       int i_lda = (int)lda;
+       int i_incx = (int)incx;
+       int i_incy = (int)incy;
+-
+-      cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-      cublasSetStream(handle, THCState_getCurrentStream(state));
+-      THCublasCheck(cublasDger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
++#ifdef HIPBLAS_TODO
++      hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++      hipblasSetStream(handle, THCState_getCurrentStream(state));
++      THCublasCheck(hipblasDger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
++#else
++#ifdef __NVCC__
++    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++    hipblasSetStream((hipblasHandle_t)handle, THCState_getCurrentStream(state));
++    hipblasDger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda);
++#endif
++#endif
+       return;
+     }
+   THError("Cublas_Dger only supports m, n, lda, incx, incy"
+           "with the bound [val] <= %d", INT_MAX);
+ }
+ 
+-
++hipblasOperation_t convertTransToHipblasOperation(char trans) {
++  if (trans == 't') return HIPBLAS_OP_T;
++  else if (trans == 'n') return HIPBLAS_OP_N;
++  else if (trans == 'c') return HIPBLAS_OP_C;
++  else {
++    THError("trans must be one of: t, n, c");
++    return HIPBLAS_OP_T;
++  }
++}
++#ifdef __NVCC__
+ cublasOperation_t convertTransToCublasOperation(char trans) {
+   if (trans == 't') return CUBLAS_OP_T;
+   else if (trans == 'n') return CUBLAS_OP_N;
+@@ -196,8 +217,8 @@ cublasOperation_t convertTransToCublasOperation(char trans) {
+     return CUBLAS_OP_T;
+   }
+ }
+-
+-void adjustLd(char transa, char transb, int64_t m, int64_t n, int64_t k, int64_t *lda, int64_t *ldb, int64_t *ldc)
++#endif
++void adjustLd(char transa, char transb, long m, long n, long k, long *lda, long *ldb, long *ldc)
+ {
+   int transa_ = ((transa == 't') || (transa == 'T'));
+   int transb_ = ((transb == 't') || (transb == 'T'));
+@@ -229,11 +250,11 @@ void adjustLd(char transa, char transb, int64_t m, int64_t n, int64_t k, int64_t
+ }
+ 
+ /* Level 3 */
+-void THCudaBlas_Sgemm(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k, float alpha, float *a, int64_t lda, float *b, int64_t ldb, float beta, float *c, int64_t ldc)
++void THCudaBlas_Sgemm(THCState *state, char transa, char transb, long m, long n, long k, float alpha, float *a, long lda, float *b, long ldb, float beta, float *c, long ldc)
+ {
+   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+-  cublasOperation_t opa = convertTransToCublasOperation(transa);
+-  cublasOperation_t opb = convertTransToCublasOperation(transb);
++  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
++  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+ 
+   if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) && (lda <= INT_MAX)  && (ldb <= INT_MAX) && (ldc <= INT_MAX) )
+   {
+@@ -244,9 +265,9 @@ void THCudaBlas_Sgemm(THCState *state, char transa, char transb, int64_t m, int6
+     int i_ldb = (int)ldb;
+     int i_ldc = (int)ldc;
+ 
+-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-    cublasSetStream(handle, THCState_getCurrentStream(state));
+-    THCublasCheck(cublasSgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
++    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++    hipblasSetStream(handle, THCState_getCurrentStream(state));
++    THCublasCheck(hipblasSgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
+     return;
+   }
+   THError("Cublas_Sgemm only supports m, n, k, lda, ldb, ldc"
+@@ -259,62 +280,58 @@ void THCudaBlas_Sgemm(THCState *state, char transa, char transb, int64_t m, int6
+ #  define CUDA_R_16F CUBLAS_DATA_HALF
+ #endif
+ 
+-void THCudaBlas_Hgemm(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k, half alpha, half *a, int64_t lda, half *b, int64_t ldb, half beta, half *c, int64_t ldc)
++void THCudaBlas_Hgemm(THCState *state, char transa, char transb, long m, long n, long k, half alpha, half *a, long lda, half *b, long ldb, half beta, half *c, long ldc)
+ {
+   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+-  cublasOperation_t opa = convertTransToCublasOperation(transa);
+-  cublasOperation_t opb = convertTransToCublasOperation(transb);
++  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
++  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+ 
+   if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) && (lda <= INT_MAX)  && (ldb <= INT_MAX) && (ldc <= INT_MAX) )
+-    {
+-      int i_m = (int)m;
+-      int i_n = (int)n;
+-      int i_k = (int)k;
+-      int i_lda = (int)lda;
+-      int i_ldb = (int)ldb;
+-      int i_ldc = (int)ldc;
++  {
++    int i_m = (int)m;
++    int i_n = (int)n;
++    int i_k = (int)k;
++    int i_lda = (int)lda;
++    int i_ldb = (int)ldb;
++    int i_ldc = (int)ldc;
+ 
+-      cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-      cublasSetStream(handle, THCState_getCurrentStream(state));
++    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++    hipblasSetStream(handle, THCState_getCurrentStream(state));
+ 
++    // Check for native Hgemm support
++#ifdef __NVCC__
++    if (THC_fastHalfInstructions(state)) {
++      THCublasCheck(hipblasHgemm(handle, opa, opb,
++				i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb,
++				&beta, c, i_ldc));
++    } else {
+       // Simulated Hgemm
+       float fAlpha = THC_half2float(alpha);
+       float fBeta = THC_half2float(beta);
+ 
+-#if CUDA_VERSION < 9000
+-      THCublasCheck(cublasSgemmEx(handle, opa, opb,
+-                                  i_m, i_n, i_k, &fAlpha,
++      /*THCublasCheck(hipblasSgemmEx(handle, opa, opb,
++				  i_m, i_n, i_k, &fAlpha,
+                                   a, CUDA_R_16F, i_lda, b, CUDA_R_16F,
+-                                  i_ldb, &fBeta, c, CUDA_R_16F, i_ldc));
+-#else
+-      cudaDeviceProp* prop = THCState_getCurrentDeviceProperties(state);
+-      if (prop->major >= 5){
+-        THCublasCheck(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));
+-        THCublasCheck(cublasGemmEx(handle, opa, opb,
+-                                   i_m, i_n, i_k, &fAlpha,
+-                                   a, CUDA_R_16F, i_lda, b, CUDA_R_16F,
+-                                   i_ldb, &fBeta, c, CUDA_R_16F, i_ldc,
+-                                   CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP));
+-        THCublasCheck(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));
+-      }else{
+-        THCublasCheck(cublasSgemmEx(handle, opa, opb,
+-                                    i_m, i_n, i_k, &fAlpha,
+-                                    a, CUDA_R_16F, i_lda, b, CUDA_R_16F,
+-                                    i_ldb, &fBeta, c, CUDA_R_16F, i_ldc));
+-      }
+-#endif
+-      return;
++				  i_ldb, &fBeta, c, CUDA_R_16F, i_ldc));*/
+     }
++#elif __HCC__
++//       hipblasHgemm(handle, opa, opb,
++// 				i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb,
++// 				&beta, c, i_ldc);
++#endif
++    return;
++  }
+   THError("Cublas_Hgemm only supports m, n, k, lda, ldb, ldc"
+           "with th bound [val] <= %d", INT_MAX);
++
+ }
+ #endif
+ 
+-void THCudaBlas_Dgemm(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k, double alpha, double *a, int64_t lda, double *b, int64_t ldb, double beta, double *c, int64_t ldc)
++void THCudaBlas_Dgemm(THCState *state, char transa, char transb, long m, long n, long k, double alpha, double *a, long lda, double *b, long ldb, double beta, double *c, long ldc)
+ {
+   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+-  cublasOperation_t opa = convertTransToCublasOperation(transa);
+-  cublasOperation_t opb = convertTransToCublasOperation(transb);
++  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
++  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+ 
+   if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) && (lda <= INT_MAX)  && (ldb <= INT_MAX) && (ldc <= INT_MAX) )
+   {
+@@ -325,19 +342,20 @@ void THCudaBlas_Dgemm(THCState *state, char transa, char transb, int64_t m, int6
+     int i_ldb = (int)ldb;
+     int i_ldc = (int)ldc;
+ 
+-    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-    cublasSetStream(handle, THCState_getCurrentStream(state));
+-    THCublasCheck(cublasDgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
++    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++    hipblasSetStream(handle, THCState_getCurrentStream(state));
++    THCublasCheck(hipblasDgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
+     return;
+   }
+   THError("Cublas_Dgemm only supports m, n, k, lda, ldb, ldc"
+           "with the bound [val] <= %d", INT_MAX);
+ }
+ 
++#ifdef __NVCC__
+ #if CUDA_VERSION >= 9010
+-void THCudaBlas_HgemmStridedBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
+-                             half alpha, const half *a, int64_t lda, int64_t strideA, const half *b, int64_t ldb, int64_t strideB,
+-                             half beta, half *c, int64_t ldc, int64_t strideC, int64_t batchCount)
++void THCudaBlas_HgemmStridedBatched(THCState *state, char transa, char transb, long m, long n, long k,
++                             half alpha, const half *a, long lda, long strideA, const half *b, long ldb, long strideB,
++                             half beta, half *c, long ldc, long strideC, long batchCount)
+ {
+   if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
+ 
+@@ -364,10 +382,11 @@ void THCudaBlas_HgemmStridedBatched(THCState *state, char transa, char transb, i
+   THCublasCheck(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));
+ }
+ #endif
++#endif
+ 
+-void THCudaBlas_SgemmBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
+-                             float alpha, const float *a[], int64_t lda, const float *b[], int64_t ldb,
+-                             float beta, float *c[], int64_t ldc, int64_t batchCount)
++void THCudaBlas_SgemmBatched(THCState *state, char transa, char transb, long m, long n, long k,
++                             float alpha, const float *a[], long lda, const float *b[], long ldb,
++                             float beta, float *c[], long ldc, long batchCount)
+ {
+   if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
+   {
+@@ -376,45 +395,21 @@ void THCudaBlas_SgemmBatched(THCState *state, char transa, char transb, int64_t
+   }
+ 
+   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+-  cublasOperation_t opa = convertTransToCublasOperation(transa);
+-  cublasOperation_t opb = convertTransToCublasOperation(transb);
+-
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasSgemmBatched(handle,
+-                                   opa, opb, (int)m, (int)n, (int)k,
+-                                   &alpha, a, (int)lda, b, (int)ldb, &beta, c, (int)ldc,
+-                                   (int)batchCount));
+-}
+-
+-#if CUDA_VERSION >= 8000
+-void THCudaBlas_SgemmStridedBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
+-                             float alpha, const float *a, int64_t lda, int64_t strideA, const float *b, int64_t ldb, int64_t strideB,
+-                             float beta, float *c, int64_t ldc, int64_t strideC, int64_t batchCount)
+-{
+-  if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
+-
+-  {
+-    THError("Cublas_SgemmStridedBatched only supports m, n, k, lda, ldb, ldc, batchCount"
+-            "with the bound [val] <= %d", INT_MAX);
++  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
++  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
++
++  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++  hipblasSetStream(handle, THCState_getCurrentStream(state));
++  for (long i = 0; i < batchCount; i++) {
++    THCublasCheck(hipblasSgemm(handle,
++                              opa, opb, (int)m, (int)n, (int)k,
++                              &alpha, a[i], (int)lda, b[i], (int)ldb, &beta, c[i], (int)ldc));
+   }
+-
+-  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+-  cublasOperation_t opa = convertTransToCublasOperation(transa);
+-  cublasOperation_t opb = convertTransToCublasOperation(transb);
+-
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasSgemmStridedBatched(handle,
+-                                   opa, opb, (int)m, (int)n, (int)k,
+-                                   &alpha, a, (int)lda, strideA, b, (int)ldb, strideB, &beta, c, (int)ldc, strideC,
+-                                   (int)batchCount));
+ }
+-#endif
+ 
+-void THCudaBlas_DgemmBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
+-                             double alpha, const double *a[], int64_t lda, const double *b[], int64_t ldb,
+-                             double beta, double *c[], int64_t ldc, int64_t batchCount)
++void THCudaBlas_DgemmBatched(THCState *state, char transa, char transb, long m, long n, long k,
++                             double alpha, const double *a[], long lda, const double *b[], long ldb,
++                             double beta, double *c[], long ldc, long batchCount)
+ {
+   if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
+   {
+@@ -423,40 +418,17 @@ void THCudaBlas_DgemmBatched(THCState *state, char transa, char transb, int64_t
+   }
+ 
+   adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+-  cublasOperation_t opa = convertTransToCublasOperation(transa);
+-  cublasOperation_t opb = convertTransToCublasOperation(transb);
+-
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasDgemmBatched(handle,
+-                                   opa, opb, (int)m, (int)n, (int)k,
+-                                   &alpha, a, (int)lda, b, (int)ldb, &beta, c, (int)ldc,
+-                                   (int)batchCount));
+-}
+-
+-#if CUDA_VERSION >= 8000
+-void THCudaBlas_DgemmStridedBatched(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k,
+-                             double alpha, const double *a, int64_t lda, int64_t strideA, const double *b, int64_t ldb, int64_t strideB,
+-                             double beta, double *c, int64_t ldc, int64_t strideC, int64_t batchCount)
+-{
+-  if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
+-  {
+-    THError("Cublas_DgemmBatched only supports m, n, k, lda, ldb, ldc, batchCount"
+-            "with the bound [val] <= %d", INT_MAX);
++  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
++  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
++
++  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++  hipblasSetStream(handle, THCState_getCurrentStream(state));
++  for (long i = 0; i < batchCount; i++) {
++    THCublasCheck(hipblasDgemm(handle,
++                              opa, opb, (int)m, (int)n, (int)k,
++                              &alpha, a[i], (int)lda, b[i], (int)ldb, &beta, c[i], (int)ldc));
+   }
+-
+-  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+-  cublasOperation_t opa = convertTransToCublasOperation(transa);
+-  cublasOperation_t opb = convertTransToCublasOperation(transb);
+-
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasDgemmStridedBatched(handle,
+-                                   opa, opb, (int)m, (int)n, (int)k,
+-                                   &alpha, a, (int)lda, strideA, b, (int)ldb, strideB, &beta, c, (int)ldc, strideC,
+-                                   (int)batchCount));
+ }
+-#endif
+ 
+ /* Inverse */
+ void THCudaBlas_Sgetrf(THCState *state, int n, float **a, int lda, int *pivot, int *info, int batchSize) {
+@@ -465,9 +437,11 @@ void THCudaBlas_Sgetrf(THCState *state, int n, float **a, int lda, int *pivot, i
+     THError("Cublas_Sgetrf only supports n, lda, batchSize"
+             "with the bound [val] <= %d", INT_MAX);
+   }
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasSgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
++#ifdef HIPBLAS_TODO
++  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++  hipblasSetStream(handle, THCState_getCurrentStream(state));
++  THCublasCheck(hipblasSgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
++#endif
+ }
+ 
+ void THCudaBlas_Dgetrf(THCState *state, int n, double **a, int lda, int *pivot, int *info, int batchSize) {
+@@ -476,9 +450,11 @@ void THCudaBlas_Dgetrf(THCState *state, int n, double **a, int lda, int *pivot,
+     THError("Cublas_Dgetrf only supports n, lda, batchSize"
+             "with the bound [val] <= %d", INT_MAX);
+   }
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasDgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
++#ifdef HIPBLAS_TODO
++  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++  hipblasSetStream(handle, THCState_getCurrentStream(state));
++  THCublasCheck(hipblasDgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
++#endif
+ }
+ 
+ THC_API void THCudaBlas_Sgetrs(THCState *state, char transa, int n, int nrhs, const float **a, int lda, int *pivot, float **b, int ldb, int *info, int batchSize)
+@@ -489,12 +465,13 @@ THC_API void THCudaBlas_Sgetrs(THCState *state, char transa, int n, int nrhs, co
+             "with the bound [val] <= %d", INT_MAX);
+   }
+ 
+-  // no need to adjust leading dimensions, since matrices are square
+-  cublasOperation_t opa = convertTransToCublasOperation(transa);
+ 
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasSgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
++#ifdef HIPBLAS_TODO
++  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
++  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++  hipblasSetStream(handle, THCState_getCurrentStream(state));
++  THCublasCheck(hipblasSgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
++#endif
+ }
+ 
+ 
+@@ -506,14 +483,14 @@ THC_API void THCudaBlas_Dgetrs(THCState *state, char transa, int n, int nrhs, co
+             "with the bound [val] <= %d", INT_MAX);
+   }
+ 
+-  // no need to adjust leading dimensions, since matrices are square
+-  cublasOperation_t opa = convertTransToCublasOperation(transa);
+ 
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasDgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
++#ifdef HIPBLAS_TODO
++  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
++  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++  hipblasSetStream(handle, THCState_getCurrentStream(state));
++  THCublasCheck(hipblasDgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
++#endif
+ }
+-
+ void THCudaBlas_Sgetri(THCState *state, int n, const float **a, int lda, int *pivot, float **c, int ldc, int *info, int batchSize) {
+ 
+   if( (n >= INT_MAX) || (lda >= INT_MAX)|| (ldc >= INT_MAX) || (batchSize >= INT_MAX) )
+@@ -521,9 +498,11 @@ void THCudaBlas_Sgetri(THCState *state, int n, const float **a, int lda, int *pi
+     THError("Cublas_Sgetri only supports n, lda, ldc, batchSize"
+             "with the bound [val] <= %d", INT_MAX);
+   }
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasSgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
++#ifdef HIPBLAS_TODO
++  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++  hipblasSetStream(handle, THCState_getCurrentStream(state));
++  THCublasCheck(hipblasSgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
++#endif
+ }
+ 
+ void THCudaBlas_Dgetri(THCState *state, int n, const double **a, int lda, int *pivot, double **c, int ldc, int *info, int batchSize) {
+@@ -533,7 +512,9 @@ void THCudaBlas_Dgetri(THCState *state, int n, const double **a, int lda, int *p
+     THError("Cublas_Dgetri only supports n, lda, ldc, batchSize"
+             "with the bound [val] <= %d", INT_MAX);
+   }
+-  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+-  cublasSetStream(handle, THCState_getCurrentStream(state));
+-  THCublasCheck(cublasDgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
++#ifdef HIPBLAS_TODO
++  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
++  hipblasSetStream(handle, THCState_getCurrentStream(state));
++  THCublasCheck(hipblasDgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
++#endif
+ }
+diff --git a/aten/src/THC/THCDeviceTensor-inl.cuh b/aten/src/THC/THCDeviceTensor-inl.cuh
+index 22ca6c973..86907c637 100644
+--- a/aten/src/THC/THCDeviceTensor-inl.cuh
++++ b/aten/src/THC/THCDeviceTensor-inl.cuh
+@@ -182,7 +182,8 @@ template <typename T, int Dim,
+ __host__ __device__ THCDeviceTensor<T, Dim, IndexT, PtrTraits>
+ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::transpose(int dim1,
+                                                       int dim2) const {
+-#ifdef __CUDA_ARCH__
++#if defined(__HIP_DEVICE_COMPILE__)
++#elif defined(__CUDA_ARCH__)
+   // Device code
+   assert(dim1 >= 0 && dim1 < Dim);
+   assert(dim1 >= 0 && dim2 < Dim);
+@@ -285,7 +286,8 @@ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::downcastOuter() {
+   // in all of the dimensions we are collapsing (no padding in
+   // them).
+   bool cont = isContiguousRange(0, Dim - NewDim);
+-#ifdef __CUDA_ARCH__
++#if defined(__HIP_DEVICE_COMPILE__)
++#elif defined(__CUDA_ARCH__)
+   // Device code
+   assert(cont);
+ #else
+@@ -336,7 +338,8 @@ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::downcastInner() {
+   // in all of the dimensions we are collapsing (no padding in
+   // them).
+   bool cont = isContiguousRange(NewDim, Dim);
+-#ifdef __CUDA_ARCH__
++#if defined(__HIP_DEVICE_COMPILE__)
++#elif defined(__CUDA_ARCH__)
+   // Device code
+   assert(cont);
+ #else
+@@ -404,7 +407,8 @@ template <typename T, int Dim,
+           typename IndexT, template <typename U> class PtrTraits>
+ void
+ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::zero(cudaStream_t stream) {
+-#ifdef __CUDA_ARCH__
++#if defined(__HIP_DEVICE_COMPILE__)
++#elif defined(__CUDA_ARCH__)
+   assert(isContiguous());
+ #else
+   if (!isContiguous()) {
+diff --git a/aten/src/THC/THCDeviceUtils.cuh b/aten/src/THC/THCDeviceUtils.cuh
+index 4ae2bee07..bfb5156ac 100644
+--- a/aten/src/THC/THCDeviceUtils.cuh
++++ b/aten/src/THC/THCDeviceUtils.cuh
+@@ -25,7 +25,7 @@ __host__ __device__ __forceinline__ T THCRoundUp(T a, T b) {
+  * For CC 3.5+, perform a load using __ldg
+  */
+ template <typename T>
+-__device__ __forceinline__ T doLdg(const T* p) {
++__device__ __forceinline__ inline T doLdg(const T* p) {
+ #if __CUDA_ARCH__ >= 350
+   return __ldg(p);
+ #else
+@@ -33,7 +33,7 @@ __device__ __forceinline__ T doLdg(const T* p) {
+ #endif
+ }
+ 
+-__device__ __forceinline__ unsigned int ACTIVE_MASK()
++__device__ __forceinline__ inline unsigned int ACTIVE_MASK()
+ {
+ #if CUDA_VERSION >= 9000
+     return __activemask();
+@@ -43,7 +43,7 @@ __device__ __forceinline__ unsigned int ACTIVE_MASK()
+ #endif
+ }
+ 
+-__device__ __forceinline__ int WARP_BALLOT(int predicate, unsigned int mask = 0xffffffff)
++__device__ __forceinline__ inline int WARP_BALLOT(int predicate, unsigned int mask = 0xffffffff)
+ {
+ #if CUDA_VERSION >= 9000
+     return __ballot_sync(mask, predicate);
+@@ -52,8 +52,13 @@ __device__ __forceinline__ int WARP_BALLOT(int predicate, unsigned int mask = 0x
+ #endif
+ }
+ 
++//To handle ambiguity, add a type double version.
++__device__ __forceinline__ inline double WARP_SHFL_XOR(double value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff) {
++  //(HIP doesn't support double)
++  return (double) __shfl_xor((float) value, laneMask, width);
++}
+ template <typename T>
+-__device__ __forceinline__ T WARP_SHFL_XOR(T value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff)
++__device__ __forceinline__ inline T WARP_SHFL_XOR(T value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff)
+ {
+ #if CUDA_VERSION >= 9000
+     return __shfl_xor_sync(mask, value, laneMask, width);
+@@ -63,7 +68,7 @@ __device__ __forceinline__ T WARP_SHFL_XOR(T value, int laneMask, int width = wa
+ }
+ 
+ template <typename T>
+-__device__ __forceinline__ T WARP_SHFL(T value, int srcLane, int width = warpSize, unsigned int mask = 0xffffffff)
++__device__ __forceinline__ inline T WARP_SHFL(T value, int srcLane, int width = warpSize, unsigned int mask = 0xffffffff)
+ {
+ #if CUDA_VERSION >= 9000
+     return __shfl_sync(mask, value, srcLane, width);
+@@ -73,7 +78,7 @@ __device__ __forceinline__ T WARP_SHFL(T value, int srcLane, int width = warpSiz
+ }
+ 
+ template <typename T>
+-__device__ __forceinline__ T WARP_SHFL_UP(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
++__device__ __forceinline__ inline T WARP_SHFL_UP(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+ {
+ #if CUDA_VERSION >= 9000
+     return __shfl_up_sync(mask, value, delta, width);
+@@ -82,8 +87,14 @@ __device__ __forceinline__ T WARP_SHFL_UP(T value, unsigned int delta, int width
+ #endif
+ }
+ 
++//To handle ambiguity, add a type double version.
++__device__ __forceinline__ inline double WARP_SHFL_DOWN(double value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
++{
++  //(HIP doesn't support double)
++  return (double) __shfl_down((float) value, delta, width);
++}
+ template <typename T>
+-__device__ __forceinline__ T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
++__device__ __forceinline__ inline T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+ {
+ #if CUDA_VERSION >= 9000
+     return __shfl_down_sync(mask, value, delta, width);
+diff --git a/aten/src/THC/THCGeneral.cpp b/aten/src/THC/THCGeneral.cpp
+index 443c7bdef..5e59c05a1 100644
+--- a/aten/src/THC/THCGeneral.cpp
++++ b/aten/src/THC/THCGeneral.cpp
+@@ -758,11 +758,11 @@ void __THCublasCheck(cublasStatus_t status, const char *file, const int line)
+       case CUBLAS_STATUS_INVALID_VALUE:
+         errmsg = "an invalid numeric value was used as an argument";
+         break;
+-
++#ifdef CUDA
+       case CUBLAS_STATUS_ARCH_MISMATCH:
+         errmsg = "an absent device architectural feature is required";
+         break;
+-
++#endif
+       case CUBLAS_STATUS_MAPPING_ERROR:
+         errmsg = "an access to GPU memory space failed";
+         break;
+@@ -803,11 +803,9 @@ void __THCusparseCheck(cusparseStatus_t status, const char *file, const int line
+       case CUSPARSE_STATUS_INVALID_VALUE:
+         errmsg = "an invalid numeric value was used as an argument";
+         break;
+-
+       case CUSPARSE_STATUS_ARCH_MISMATCH:
+         errmsg = "an absent device architectural feature is required";
+         break;
+-
+       case CUSPARSE_STATUS_MAPPING_ERROR:
+         errmsg = "an access to GPU memory space failed";
+         break;
+@@ -923,7 +921,10 @@ cudaError_t THCudaMemGetInfoCached(THCState *state,  size_t* freeBytes, size_t*
+ 
+ half THC_float2half(float f)
+ {
+-#if CUDA_VERSION < 9000
++#if defined(__HIP_PLATFORM_HCC__)
++  half h;
++  return h;
++#elif CUDA_VERSION < 9000
+   half h;
+   TH_float2halfbits(&f, &h.x);
+   return h;
+@@ -936,12 +937,13 @@ half THC_float2half(float f)
+ 
+ float  THC_half2float(half h)
+ {
++#if defined(__HIP_PLATFORM_HCC__)
+   float f;
+-#if CUDA_VERSION < 9000
++  return f;
++#elif CUDA_VERSION < 9000
+   TH_halfbits2float(&h.x, &f);
+ #else
+   __half_raw h_raw(h);
+   TH_halfbits2float(&h_raw.x, &f);
+ #endif
+-  return f;
+ }
+diff --git a/aten/src/THC/THCHalf.h b/aten/src/THC/THCHalf.h
+index bb21b9d25..ceece3150 100644
+--- a/aten/src/THC/THCHalf.h
++++ b/aten/src/THC/THCHalf.h
+@@ -4,19 +4,24 @@
+ #include "THCGeneral.h"
+ 
+ /* We compile with CudaHalfTensor support if we have this: */
+-#if CUDA_VERSION >= 7050 || CUDA_HAS_FP16
++#if CUDA_VERSION >= 7050 || CUDA_HAS_FP16 || defined (__HIP_PLATFORM_HCC__)
+ #define CUDA_HALF_TENSOR 1
+ #endif
+ 
+ #ifdef CUDA_HALF_TENSOR
+ 
+-#include <cuda_fp16.h>
+-#include <stdint.h>
+-
+-#if CUDA_VERSION >= 9000
+-#ifndef __cplusplus
+-typedef __half_raw half;
+-#endif
++#if defined (__HIP_PLATFORM_HCC__)
++  #include <cstdint>
++  #include <hip/hip_fp16.h>
++#else
++  #include <cuda_fp16.h>
++  #include <stdint.h>
++  
++  #if CUDA_VERSION >= 9000
++    #ifndef __cplusplus
++      typedef __half_raw half;
++    #endif
++  #endif
+ #endif
+ 
+ THC_EXTERNC void THCFloat2Half(THCState *state, half *out, float *in, ptrdiff_t len);
+diff --git a/aten/src/THC/THCNumerics.cuh b/aten/src/THC/THCNumerics.cuh
+index cbc2743d9..977533ab7 100644
+--- a/aten/src/THC/THCNumerics.cuh
++++ b/aten/src/THC/THCNumerics.cuh
+@@ -1,11 +1,14 @@
+ #ifndef THC_NUMERICS_INC
+ #define THC_NUMERICS_INC
+ 
+-#include <cuda.h>
+ #include <limits.h>
+-#include <assert.h>
++#include "hip/hip_runtime.h"
++
+ #include "THCHalf.h"
+ 
++#include <climits>
++
++
+ /// Class for numeric limits of the particular data type, which
+ /// includes support for `half`.
+ /// Unfortunately since `half` does not have a constructor, these have
+@@ -16,7 +19,7 @@ struct THCNumerics {
+ 
+ template <typename scalar_t>
+ static inline __host__ __device__ scalar_t powi(scalar_t a, scalar_t b) {
+-  assert(THCNumerics<scalar_t>::ge(b, 0));
++  //assert(THCNumerics<scalar_t>::ge(b, 0));
+   scalar_t result = 1;
+   while (b) {
+     if (b & 1) {
+@@ -35,8 +38,8 @@ static inline __host__ __device__ bool has_different_sign(scalar_t a, scalar_t b
+ 
+ template <>
+ struct THCNumerics<uint8_t> {
+-  static inline __host__ __device__ uint8_t min() { return 0; }
+-  static inline __host__ __device__ uint8_t max() { return UCHAR_MAX; }
++  static inline __host__ __device__ uint8_t (min)() { return 0; }
++  static inline __host__ __device__ uint8_t (max)() { return UCHAR_MAX; }
+ 
+   static inline __host__ __device__ bool lt(uint8_t a, uint8_t b) { return a < b; }
+   static inline __host__ __device__ bool le(uint8_t a, uint8_t b) { return a <= b; }
+@@ -56,8 +59,8 @@ struct THCNumerics<uint8_t> {
+ 
+ template <>
+ struct THCNumerics<int8_t> {
+-  static inline __host__ __device__ int8_t min() { return SCHAR_MIN; }
+-  static inline __host__ __device__ int8_t max() { return SCHAR_MAX; }
++  static inline __host__ __device__ int8_t (min)() { return SCHAR_MIN; }
++  static inline __host__ __device__ int8_t (max)() { return SCHAR_MIN; }
+ 
+   static inline __host__ __device__ bool lt(int8_t a, int8_t b) { return a < b; }
+   static inline __host__ __device__ bool le(int8_t a, int8_t b) { return a <= b; }
+@@ -71,14 +74,15 @@ struct THCNumerics<int8_t> {
+   static inline __host__ __device__  int8_t mul(int8_t a, int8_t b) { return a * b; }
+   static inline __host__ __device__  int8_t sub(int8_t a, int8_t b) { return a - b; }
+   static inline __host__ __device__  int8_t div(int8_t a, int8_t b) { return a / b; }
+-  static inline __host__ __device__  int8_t abs(int8_t a) { return ::abs((int)a); }
+   static inline __host__ __device__  int8_t pow(int8_t a, int8_t b) { return powi<int8_t>(a, b); }
++  static inline __host__ int8_t abs(int8_t a) { return std::abs(a); }
++  static inline __device__ int8_t abs(int8_t a) { return a < 0 ? -a : a; }
+ };
+ 
+ template <>
+ struct THCNumerics<int16_t> {
+-  static inline __host__ __device__ int16_t min() { return SHRT_MIN; }
+-  static inline __host__ __device__ int16_t max() { return SHRT_MAX; }
++  static inline __host__ __device__ int16_t (min)() { return SHRT_MIN; }
++  static inline __host__ __device__ int16_t (max)() { return SHRT_MAX; }
+ 
+   static inline __host__ __device__ bool lt(int16_t a, int16_t b) { return a < b; }
+   static inline __host__ __device__ bool le(int16_t a, int16_t b) { return a <= b; }
+@@ -92,14 +96,15 @@ struct THCNumerics<int16_t> {
+   static inline __host__ __device__  int16_t mul(int16_t a, int16_t b) { return a * b; }
+   static inline __host__ __device__  int16_t sub(int16_t a, int16_t b) { return a - b; }
+   static inline __host__ __device__  int16_t div(int16_t a, int16_t b) { return a / b; }
+-  static inline __host__ __device__  int16_t abs(int16_t a) { return ::abs((int)a); }
+   static inline __host__ __device__  int16_t pow(int16_t a, int16_t b) { return powi<int16_t>(a, b); }
++  static inline __host__ int16_t abs(int16_t a) { return std::abs(a); }
++  static inline __device__ int16_t abs(int16_t a) { return a < 0 ? -a : a; }
+ };
+ 
+ template <>
+ struct THCNumerics<int32_t> {
+-  static inline __host__ __device__ int32_t min() { return INT_MIN; }
+-  static inline __host__ __device__ int32_t max() { return INT_MAX; }
++  static inline __host__ __device__ int32_t (min)() { return INT_MIN; }
++  static inline __host__ __device__ int32_t (max)() { return INT_MAX; }
+ 
+   static inline __host__ __device__ bool lt(int32_t a, int32_t b) { return a < b; }
+   static inline __host__ __device__ bool le(int32_t a, int32_t b) { return a <= b; }
+@@ -113,19 +118,15 @@ struct THCNumerics<int32_t> {
+   static inline __host__ __device__  int32_t mul(int32_t a, int32_t b) { return a * b; }
+   static inline __host__ __device__  int32_t sub(int32_t a, int32_t b) { return a - b; }
+   static inline __host__ __device__  int32_t div(int32_t a, int32_t b) { return a / b; }
+-  static inline __host__ __device__  int32_t abs(int32_t a) { return ::abs(a); }
+   static inline __host__ __device__  int32_t pow(int32_t a, int32_t b) { return powi<int32_t>(a, b); }
++  static inline __host__ int32_t abs(int32_t a) { return std::abs(a); }
++  static inline __device__ int32_t abs(int32_t a) { return a < 0 ? -a : a; }
+ };
+ 
+ template <>
+ struct THCNumerics<int64_t> {
+-#ifdef _MSC_VER
+-  static inline __host__ __device__ int64_t min() { return _I64_MIN; }
+-  static inline __host__ __device__ int64_t max() { return _I64_MAX; }
+-#else
+-  static inline __host__ __device__ int64_t min() { return LONG_MIN; }
+-  static inline __host__ __device__ int64_t max() { return LONG_MAX; }
+-#endif
++  static inline __host__ __device__ int64_t (min)() { return LONG_MIN; }
++  static inline __host__ __device__ int64_t (max)() { return LONG_MAX; }
+ 
+   static inline __host__ __device__ bool lt(int64_t a, int64_t b) { return a < b; }
+   static inline __host__ __device__ bool le(int64_t a, int64_t b) { return a <= b; }
+@@ -134,498 +135,707 @@ struct THCNumerics<int64_t> {
+   static inline __host__ __device__ bool eq(int64_t a, int64_t b) { return a == b; }
+   static inline __host__ __device__ bool ne(int64_t a, int64_t b) { return a != b; }
+ 
+-
+   static inline __host__ __device__  int64_t neg(int64_t a) { return -a; }
+   static inline __host__ __device__  int64_t add(int64_t a, int64_t b) { return a + b; }
+   static inline __host__ __device__  int64_t mul(int64_t a, int64_t b) { return a * b; }
+   static inline __host__ __device__  int64_t sub(int64_t a, int64_t b) { return a - b; }
+   static inline __host__ __device__  int64_t div(int64_t a, int64_t b) { return a / b; };
+-  static inline __host__ __device__  int64_t abs(int64_t a) { return labs(a); }
+   static inline __host__ __device__  int64_t pow(int64_t a, int64_t b) { return powi<int64_t>(a, b); }
++  static inline __host__ int64_t abs(int64_t a) { return std::abs(a); }
++  static inline __device__ int64_t abs(int64_t a) { return a < 0 ? -a : a; }
+ };
+ 
+ #ifdef CUDA_HALF_TENSOR
+ template <>
+ struct THCNumerics<half> {
+-#if CUDA_VERSION < 9000
+-  static inline __host__ __device__ half min() { half h; h.x = 0xfbff; return h; }
+-  static inline __host__ __device__ half max() { half h; h.x = 0x7bff; return h; }
+-#else
+-  static inline __host__ __device__ half min() { __half_raw h; h.x = 0xfbff; return h; }
+-  static inline __host__ __device__ half max() { __half_raw h; h.x = 0x7bff; return h; }
+-#endif
++    __host__ __device__
++    static
++    inline
++    half (min)()
++    {
++            return -65504;
++    }
++    __host__ __device__
++    static
++    inline
++    half (max)()
++    {
++            return 65504;
++    }
+ 
+-  static inline __host__ __device__ bool lt(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __hlt(a, b);
+-#else
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return fa < fb;
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  bool lt(half a, half b)
++  {
++        return a < b;
++  }
++  __host__
++  static
++  inline
++  bool lt(half a, half b)
++  {
+     return THC_half2float(a) < THC_half2float(b);
+-#endif
+   }
+ 
+-  static inline __host__ __device__ bool le(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __hle(a, b);
+-#else
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return fa <= fb;
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  bool le(half a, half b)
++  {
++        return a <= b;
++  }
++  __host__
++  static
++  inline
++  bool le(half a, half b)
++  {
+     return THC_half2float(a) <= THC_half2float(b);
+-#endif
+   }
+ 
+-  static inline __host__ __device__ bool gt(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __hgt(a, b);
+-#else
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return fa > fb;
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  bool gt(half a, half b)
++  {
++      return a > b;
++  }
++  __host__
++  static
++  inline
++  bool gt(half a, half b)
++  {
+     return THC_half2float(a) > THC_half2float(b);
+-#endif
+   }
+ 
+-  static inline __host__ __device__ bool ge(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __hge(a, b);
+-#else
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return fa >= fb;
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  bool ge(half a, half b)
++  {
++      return a >= b;
++  }
++  __host__
++  static
++  inline
++  bool ge(half a, half b)
++  {
+     return THC_half2float(a) >= THC_half2float(b);
+-#endif
+   }
+ 
+-  static inline __host__ __device__ bool eq(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __heq(a, b);
+-#else
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return fa == fb;
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  bool eq(half a, half b)
++  {
++      return a == b;
++  }
++  __host__
++  static
++  inline
++  bool eq(half a, half b)
++  {
+     return THC_half2float(a) == THC_half2float(b);
+-#endif
+   }
+ 
+-  static inline __host__ __device__ bool ne(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __hne(a, b);
+-#else
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return fa != fb;
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  bool ne(half a, half b)
++  {
++      return a != b;
++  }
++  __host__
++  static
++  inline
++  bool ne(half a, half b)
++  {
+     return THC_half2float(a) != THC_half2float(b);
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half exp(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return hexp(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(expf(fa));
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half exp(half a)
++  {
++      float fa = __half2float(a);
++      return __float2half(expf(fa));
++  }
++  __host__
++  static
++  inline
++  half exp(half a)
++  {
+     return THC_float2half(expf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half exp10(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return hexp10(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(exp10f(fa));
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half exp10(half a)
++  {
++      float fa = __half2float(a);
++      return __float2half(exp10f(fa));
++  }
++  __host__
++  static
++  inline
++  half exp10(half a)
++  {
+     return THC_float2half(exp10f(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half log(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return hlog(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(logf(fa));
+-#endif
+-#else // __CUDA_ARCH__
+-    return THC_float2half(logf(THC_half2float(a)));
+-#endif
++  __device__
++  static
++  inline
++  half log(half a)
++  {
++      float fa = __half2float(a);
++      return __float2half(logf(fa));
+   }
+-
+-  static inline __host__ __device__ half log10(half a) {
+-#ifdef __CUDA_ARCH__
+-    float fa = __half2float(a);
+-    return __float2half(log10f(fa));
+-#else // __CUDA_ARCH__
+-    return THC_float2half(log10f(THC_half2float(a)));
+-#endif
++  __host__
++  static
++  inline
++  half log(half a)
++  {
++    return THC_float2half(logf(THC_half2float(a)));
+   }
+ 
+-  static inline __host__ __device__ half log1p(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half log1p(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(log1pf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half log1p(half a)
++  {
+     return THC_float2half(log1pf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half log2(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline  half log2(half a) {
+     float fa = __half2float(a);
+     return __float2half(log2f(fa));
+-#else // __CUDA_ARCH__
++  }
++
++  __host__
++  static
++  inline  half log2(half a) {
+     return THC_float2half(log2f(THC_half2float(a)));
+-#endif
+   }
+ 
+-static inline __host__ __device__ half lgamma(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline  half log10(half a) {
++    float fa = __half2float(a);
++    return __float2half(log10f(fa));
++  }
++
++  __host__
++  static
++  inline  half log10(half a) {
++    return THC_float2half(log10f(THC_half2float(a)));
++  }
++
++  __device__
++  static
++  inline
++  half lgamma(half a) {
+     float fa = __half2float(a);
+     return __float2half(lgammaf(fa));
+-#else // __CUDA_ARCH__
+-    return THC_float2half(lgammaf(THC_half2float(a)));
+-#endif
++  }
++  __host__
++  static
++  inline
++  half lgamma(half a)
++  {
++    return THC_float2half(lgamma(THC_half2float(a)));
+   }
+ 
+-  static inline __host__ __device__ half expm1(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half expm1(half a) {
+     float fa = __half2float(a);
+     return __float2half(expm1f(fa));
+-#else // __CUDA_ARCH__
++  }
++
++  __host__
++  static
++  inline
++  half expm1(half a) {
+     return THC_float2half(expm1f(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half cos(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return hcos(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(cosf(fa));
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half cos(half a)
++  {
++    #ifdef CUDA_HALF_INSTRUCTIONS
++      return hcos(a);
++    #else
++      float fa = __half2float(a);
++      return __float2half(cosf(fa));
++    #endif
++  }
++  __host__
++  static
++  inline
++  half cos(half a)
++  {
+     return THC_float2half(cosf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half sin(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return hsin(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(sinf(fa));
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half sin(half a)
++  {
++    #ifdef CUDA_HALF_INSTRUCTIONS
++      return hsin(a);
++    #else
++      float fa = __half2float(a);
++      return __float2half(sinf(fa));
++    #endif
++  }
++  __host__
++  static
++  inline
++  half sin(half a)
++  {
+     return THC_float2half(sinf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half sqrt(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return hsqrt(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(sqrtf(fa));
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half sqrt(half a)
++  {
++    #ifdef CUDA_HALF_INSTRUCTIONS
++      return hsqrt(a);
++    #else
++      float fa = __half2float(a);
++      return __float2half(sqrtf(fa));
++    #endif
++  }
++  __host__
++  static
++  inline
++  half sqrt(half a)
++  {
+     return THC_float2half(sqrtf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half rsqrt(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return hrsqrt(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(rsqrtf(fa));
+-#endif
+-#else // __CUDA_ARCH__
+-    return THC_float2half(rsqrtf(THC_half2float(a)));
+-#endif
+-  }
+-
+-  static inline __host__ __device__ half ceil(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return hceil(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(ceilf(fa));
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half rsqrt(half a)
++  {
++    #ifdef CUDA_HALF_INSTRUCTIONS
++      return hrsqrt(a);
++    #else
++      float fa = __half2float(a);
++      return __float2half(rsqrtf(fa));
++    #endif
++  }
++//  __host__
++//  static
++//  inline
++//  half rsqrt(half a)
++//  {
++//    return THC_float2half(std::rsqrt(THC_half2float(a)));
++//  }
++
++  __device__
++  static
++  inline
++  half ceil(half a)
++  {
++    #ifdef CUDA_HALF_INSTRUCTIONS
++      return hceil(a);
++    #else
++      float fa = __half2float(a);
++      return __float2half(ceilf(fa));
++    #endif
++  }
++  __host__
++  static
++  inline
++  half ceil(half a)
++  {
+     return THC_float2half(ceilf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half floor(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return hfloor(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(floorf(fa));
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half floor(half a)
++  {
++    #ifdef CUDA_HALF_INSTRUCTIONS
++      return hfloor(a);
++    #else
++      float fa = __half2float(a);
++      return __float2half(floorf(fa));
++    #endif
++  }
++  __host__
++  static
++  inline
++  half floor(half a)
++  {
+     return THC_float2half(floorf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half trunc(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return htrunc(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(truncf(fa));
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half trunc(half a)
++  {
++    #ifdef CUDA_HALF_INSTRUCTIONS
++      return htrunc(a);
++    #else
++      float fa = __half2float(a);
++      return __float2half(truncf(fa));
++    #endif
++  }
++  __host__
++  static
++  inline
++  half trunc(half a)
++  {
+     return THC_float2half(truncf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half neg(half a) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __hneg(a);
+-#else
+-    float fa = __half2float(a);
+-    return __float2half(-fa);
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half neg(half a)
++  {
++      return -a;
++  }
++  __host__
++  static
++  inline
++  half neg(half a)
++  {
+     return THC_float2half(-(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half acos(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half acos(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(acosf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half acos(half a)
++  {
+     return THC_float2half(acosf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half cosh(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half cosh(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(coshf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half cosh(half a)
++  {
+     return THC_float2half(coshf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half asin(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half asin(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(asinf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half asin(half a)
++  {
+     return THC_float2half(asinf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half sinh(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half sinh(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(sinhf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half sinh(half a)
++  {
+     return THC_float2half(sinhf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half tan(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half tan(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(tanf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half tan(half a)
++  {
+     return THC_float2half(tanf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half atan(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half atan(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(atanf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half atan(half a)
++  {
+     return THC_float2half(atanf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half tanh(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half tanh(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(tanhf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half tanh(half a)
++  {
+     return THC_float2half(tanhf(THC_half2float(a)));
+-#endif
+   }
+ 
+-
+-   static inline __host__ __device__ half erf(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half erf(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(erff(fa));
+-#else // __CUDA_ARCH__
+-    return THC_float2half(erff(THC_half2float(a)));
+-#endif
++  }
++  __host__
++  static
++  inline
++  half erf(half a)
++  {
++    return THC_float2half(erf(THC_half2float(a)));
+   }
+ 
+-
+-   static inline __host__ __device__ half erfinv(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half erfinv(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(erfinvf(fa));
+-#else // __CUDA_ARCH__
+-    return THC_float2half(erfinvf(THC_half2float(a)));
+-#endif
++  }
++  __host__
++  static
++  inline
++  half erfinv(half a)
++  {
++    return THC_float2half(erfinv(THC_half2float(a)));
+   }
+ 
+-  static inline __host__ __device__ half abs(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half abs(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(fabs(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half abs(half a)
++  {
+     return THC_float2half(fabs(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half round(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half round(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(roundf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half round(half a)
++  {
+     return THC_float2half(roundf(THC_half2float(a)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half frac(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half frac(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(fa - truncf(fa));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half frac(half a)
++  {
+     float fa = THC_half2float(a);
+     return THC_float2half(fa - floorf(fa));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half cinv(half a) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half cinv(half a)
++  {
+     float fa = __half2float(a);
+     return __float2half(1.0f / fa);
+-#else // __CUDA_ARCH__
+-    return THC_float2half(1.0f / THC_half2float(a));
+-#endif
+   }
+-
+-  static inline __host__ __device__ half add(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __hadd(a, b);
+-#else
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return __float2half( fa + fb );
+-#endif
+-#else // __CUDA_ARCH__
+-    return THC_float2half(THC_half2float(a) + THC_half2float(b));
+-#endif
++  __host__
++  static
++  inline
++  half cinv(half a)
++  {
++    return THC_float2half(1.0f / THC_half2float(a));
+   }
+ 
+-  static inline __host__ __device__ half div(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return __float2half( fa / fb );
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half add(half a, half b)
++  {
++      return a + b;
++  }
++  __host__
++  static
++  inline
++  half add(half a, half b)
++  {
++      return a + b;
++  }
++
++  __device__
++  static
++  inline
++  half div(half a, half b)
++  {
++      return a / b;
++  }
++  __host__
++  static
++  inline
++  half div(half a, half b)
++  {
+     return THC_float2half(THC_half2float(a) / THC_half2float(b));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half mul(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __hmul(a, b);
+-#else
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return __float2half( fa * fb );
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half mul(half a, half b)
++  {
++      return a * b;
++  }
++  __host__
++  static
++  inline
++  half mul(half a, half b)
++  {
+     return THC_float2half(THC_half2float(a) * THC_half2float(b));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half sub(half a, half b) {
+-#ifdef __CUDA_ARCH__
+-#ifdef CUDA_HALF_INSTRUCTIONS
+-    return __hsub(a, b);
+-#else
+-    float fa = __half2float(a);
+-    float fb = __half2float(b);
+-    return __float2half( fa - fb );
+-#endif
+-#else // __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half sub(half a, half b)
++  {
++      return a - b;
++  }
++  __host__
++  static
++  inline
++  half sub(half a, half b)
++  {
+     return THC_float2half(THC_half2float(a) - THC_half2float(b));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half pow(half a, half b) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half pow(half a, half b)
++  {
+     float fa = __half2float(a);
+     float fb = __half2float(b);
+     return __float2half(powf(fa, fb));
+-#else // __CUDA_ARCH__
++  }
++  __host__
++  static
++  inline
++  half pow(half a, half b)
++  {
+     return THC_float2half(powf(THC_half2float(a), THC_half2float(b)));
+-#endif
+   }
+ 
+-  static inline __host__ __device__ half atan2(half a, half b) {
+-#ifdef __CUDA_ARCH__
++  __device__
++  static
++  inline
++  half atan2(half a, half b) {
+     float fa = __half2float(a);
+     float fb = __half2float(b);
+     return __float2half(atan2f(fa, fb));
+-#else // __CUDA_ARCH__
+-    return THC_float2half(atan2f(THC_half2float(a), THC_half2float(b)));
+-#endif
+   }
+ 
++  __host__
++  static
++  inline
++  half atan2(half a, half b) {
++     return THC_float2half(atan2f(THC_half2float(a), THC_half2float(b)));
++  }
+ };
+ #endif
+ 
+ template <>
+ struct THCNumerics<float> {
+-  static inline __host__ __device__ float min() { return -FLT_MAX; }
+-  static inline __host__ __device__ float max() { return FLT_MAX; }
++  static inline __host__ __device__ float (min)() { return -FLT_MAX; }
++  static inline __host__ __device__ float (max)() { return FLT_MAX; }
+ 
+   static inline __host__ __device__ bool lt(float a, float b) { return a < b; }
+   static inline __host__ __device__ bool le(float a, float b) { return a <= b; }
+@@ -675,8 +885,8 @@ struct THCNumerics<float> {
+ 
+ template <>
+ struct THCNumerics<double> {
+-  static inline __host__ __device__ double min() { return -DBL_MAX; }
+-  static inline __host__ __device__ double max() { return DBL_MAX; }
++  static inline __host__ __device__ double (min)() { return -DBL_MAX; }
++  static inline __host__ __device__ double (max)() { return DBL_MAX; }
+ 
+   static inline __host__ __device__ bool lt(double a, double b) { return a < b; }
+   static inline __host__ __device__ bool le(double a, double b) { return a <= b; }
+@@ -701,7 +911,7 @@ struct THCNumerics<double> {
+   static inline __host__ __device__  double ceil (double a) { return  ::ceil(a); }
+   static inline __host__ __device__  double floor(double a) { return ::floor(a); }
+   static inline __host__ __device__  double trunc(double a) { return ::trunc(a); }
+-  static inline __host__ __device__  double neg  (double a) { return       -a; }
++  static inline __host__ __device__  double neg  (double a) { return         -a; }
+   static inline __host__ __device__  double acos (double a) { return  ::acos(a); }
+   static inline __host__ __device__  double cosh (double a) { return  ::cosh(a); }
+   static inline __host__ __device__  double acosh(double a) { return ::acosh(a); }
+@@ -712,7 +922,7 @@ struct THCNumerics<double> {
+   static inline __host__ __device__  double atan (double a) { return  ::atan(a); }
+   static inline __host__ __device__  double tanh (double a) { return  ::tanh(a); }
+   static inline __host__ __device__  double erf  (double a) { return   ::erf(a); }
+-  static inline __host__ __device__  double abs  (double a) { return   ::abs(a); }
++  static inline __host__ __device__  double abs  (double a) { return   ::fabs(a); }
+   static inline __host__ __device__  double round(double a) { return ::round(a); }
+   static inline __host__ __device__  double frac (double a) { return a - ::trunc(a); }
+   static inline __host__ __device__  double cinv (double a) { return 1.0 / a; }
+@@ -728,46 +938,59 @@ struct THCNumerics<double> {
+ /// is a struct without a constructor/implicit conversion constructor.
+ /// We use this to convert scalar values to the given type that the
+ /// tensor expects.
+-template <typename In, typename Out>
++template<typename In, typename Out>
+ struct ScalarConvert {
+-  static __host__ __device__ Out to(const In v) { return (Out) v; }
++  __host__ __device__
++  static
++  Out to(const In& v) { return static_cast<Out>(v); }
+ };
+ 
+ #ifdef CUDA_HALF_TENSOR
+-template <typename Out>
+-struct ScalarConvert<half, Out> {
+-  static __host__ __device__ Out to(const half v) {
+-#ifdef __CUDA_ARCH__
+-    return (Out) __half2float(v);
+-#else
+-    return (Out) THC_half2float(v);
+-#endif
+-  }
+-};
++  template<typename Out>
++  struct ScalarConvert<half, Out> {
++    __device__
++    static
++    Out to(half v)
++    {
++        return static_cast<Out>(v);
++    }
+ 
+-template <typename In>
+-struct ScalarConvert<In, half> {
+-  static __host__ __device__ half to(const In v) {
+-#ifdef __CUDA_ARCH__
+-    return __float2half((float) v);
+-#else
+-    return THC_float2half((float) v);
+-#endif
+-  }
+-};
++    __host__
++    static
++    Out to(half v)
++    {
++      return static_cast<Out>(THC_half2float(v));
++    }
++  };
++
++  template <typename In>
++  struct ScalarConvert<In, half> {
++    __device__
++    static
++    half to(In v)
++    {
++        return static_cast<half>(v);
++    }
+ 
+-template <>
+-struct ScalarConvert<half, half> {
+-  static __host__ __device__ half to(const half v) {
+-    return v;
+-  }
+-};
++    __host__
++    static
++    half to(In v)
++    {
++      return THC_float2half(static_cast<float>(v));
++    }
++  };
+ 
+-template <typename T, typename U>
+-__host__ __device__ T scalar_cast(U u) {
+-  return ScalarConvert<U, T>::to(u);
+-}
++  template <>
++  struct ScalarConvert<half, half> {
++    __device__
++    static
++    half to(half v) { return v; }
++  };
+ 
++  template <typename T, typename U>
++    __host__ __device__ T scalar_cast(U u) {
++    return ScalarConvert<U, T>::to(u);
++  }
+ #endif
+ 
+-#endif // THC_NUMERICS_INC
++#endif // THC_NUMERICS_INC
+diff --git a/aten/src/THC/THCStream.cpp b/aten/src/THC/THCStream.cpp
+index 49fe680a3..0e8c29fcf 100644
+--- a/aten/src/THC/THCStream.cpp
++++ b/aten/src/THC/THCStream.cpp
+@@ -37,7 +37,9 @@ THCStream* THCStream_newWithPriority(int flags, int priority)
+   THCStream* self = (THCStream*) malloc(sizeof(THCStream));
+   self->refcount = 1;
+   THCudaCheck(cudaGetDevice(&self->device));
+-  THCudaCheck(cudaStreamCreateWithPriority(&self->stream, flags, priority));
++#if !defined(__HIP_PLATFORM_HCC__)
++  THCudaCheck(cudaStreamCreateWithFlags(&self->stream, flags));
++#endif
+   return self;
+ }
+ 
+diff --git a/aten/src/THC/THCTensorIndex.cu b/aten/src/THC/THCTensorIndex.cu
+index ac0065afb..9ae86a8cc 100644
+--- a/aten/src/THC/THCTensorIndex.cu
++++ b/aten/src/THC/THCTensorIndex.cu
+@@ -1,3 +1,4 @@
++#include <thrust/execution_policy.h> 
+ #include "THC.h"
+ #include "THCTensorMath.h"
+ #include "THCGeneral.h"
+diff --git a/aten/src/THC/THCTensorMathPairwise.cu b/aten/src/THC/THCTensorMathPairwise.cu
+index f530d814d..ea2d2ad4e 100644
+--- a/aten/src/THC/THCTensorMathPairwise.cu
++++ b/aten/src/THC/THCTensorMathPairwise.cu
+@@ -23,14 +23,20 @@ struct TensorAddConstantOp {
+ #ifdef CUDA_HALF_TENSOR
+ template <>
+ struct TensorAddConstantOp<half> {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined (CUDA_HALF_INSTRUCTIONS)|| defined (__HIP_PLATFORM_HCC__)
++  #if defined(__HIP_PLATFORM_HCC__)
++    __host__ __device__
++    explicit
++  #endif
+   TensorAddConstantOp(half v) : val(v) {}
+ #else
+   TensorAddConstantOp(half v) : fval(THC_half2float(v)) {}
+ #endif
+ 
+   __device__ __forceinline__ void operator()(half* out, half* in) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined (__HIP_PLATFORM_HCC__)
++    *out = *in + val;
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+     *out = __hadd(*in, val);
+ #else
+     float fin = __half2float(*in);
+@@ -40,7 +46,9 @@ struct TensorAddConstantOp<half> {
+   }
+ 
+   __device__ __forceinline__ void operator()(half* v) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined (__HIP_PLATFORM_HCC__)
++    *v += val;
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+     *v = __hadd(*v, val);
+ #else
+     float fv = __half2float(*v);
+@@ -49,7 +57,7 @@ struct TensorAddConstantOp<half> {
+ #endif
+   }
+ 
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
+   const half val;
+ #else
+   const float fval;
+@@ -76,14 +84,20 @@ struct TensorSubConstantOp {
+ #ifdef CUDA_HALF_TENSOR
+ template <>
+ struct TensorSubConstantOp<half> {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++  __host__ __device__
++  explicit
++  TensorSubConstantOp(half v) : val{v} {}
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+   TensorSubConstantOp(half v): val(THC_float2half(-(THC_half2float(v)))) {}
+ #else
+   TensorSubConstantOp(half v): fval(-(THC_half2float(v))) {}
+ #endif
+ 
+   __device__ __forceinline__ void operator()(half* out, half* in) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++  *out = *in + val;
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+     *out = __hadd(*in, val);
+ #else
+     float fin = __half2float(*in);
+@@ -93,7 +107,9 @@ struct TensorSubConstantOp<half> {
+   }
+ 
+   __device__ __forceinline__ void operator()(half* v) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++    *v += val;
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+     *v = __hadd(*v, val);
+ #else
+     float fv = __half2float(*v);
+@@ -102,7 +118,7 @@ struct TensorSubConstantOp<half> {
+ #endif
+   }
+ 
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
+   const half val;
+ #else
+   const float fval;
+@@ -128,14 +144,20 @@ struct TensorMulConstantOp {
+ #ifdef CUDA_HALF_TENSOR
+ template <>
+ struct TensorMulConstantOp<half> {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++  __host__ __device__
++  explicit
++  TensorMulConstantOp(half v) : val(v) {}
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+   TensorMulConstantOp(half v) : val(v) {}
+ #else
+   TensorMulConstantOp(half v) : fval(THC_half2float(v)) {}
+ #endif
+ 
+   __device__ __forceinline__ void operator()(half* out, half* in) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++    *out = *in * val;
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+     *out = __hmul(*in, val);
+ #else
+     float fin = __half2float(*in);
+@@ -145,7 +167,9 @@ struct TensorMulConstantOp<half> {
+   }
+ 
+   __device__ __forceinline__ void operator()(half* v) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++    *v = *v * val;
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+     *v = __hmul(*v, val);
+ #else
+     float fv = __half2float(*v);
+@@ -154,7 +178,7 @@ struct TensorMulConstantOp<half> {
+ #endif
+   }
+ 
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
+   const half val;
+ #else
+   const float fval;
+@@ -176,6 +200,7 @@ struct TensorDivConstantOp {
+   const T val;
+ };
+ 
++#if !defined(__HIP_PLATFORM_HCC__)
+ template <>
+ struct TensorDivConstantOp<float> {
+   TensorDivConstantOp(float v) : val(1.f / v) {}
+@@ -203,17 +228,24 @@ struct TensorDivConstantOp<double> {
+ 
+   const double val;
+ };
++#endif
+ 
+ #ifdef CUDA_HALF_TENSOR
+ template <>
+ struct TensorDivConstantOp<half> {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++  __host__ __device__
++  explicit
++  TensorDivConstantOp(half v) : val(ScalarInv<half>::to(v)) {}
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+   TensorDivConstantOp(half v) : val(ScalarInv<half>::to(v)) {}
+ #else
+   TensorDivConstantOp(half v) : fval(1.f / THC_half2float(v)) {}
+ #endif
+   __device__ __forceinline__ void operator()(half* out, half* in) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++    *out = *in * val;
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+     *out = __hmul(*in, val);
+ #else
+     float fin = __half2float(*in);
+@@ -223,7 +255,9 @@ struct TensorDivConstantOp<half> {
+   }
+ 
+   __device__ __forceinline__ void operator()(half* v) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++    *v = *v * val;
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+     *v = __hmul(*v, val);
+ #else
+     float fv = __half2float(*v);
+@@ -232,7 +266,7 @@ struct TensorDivConstantOp<half> {
+ #endif
+   }
+ 
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
+   const half val;
+ #else
+   const float fval;
+@@ -291,15 +325,19 @@ struct TensorRemainderOp<double> {
+ #ifdef CUDA_HALF_TENSOR
+ template <>
+ struct TensorRemainderOp<half> {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(__HIP_PLATFORM_HCC__)
++  __host__ __device__
+   TensorRemainderOp(half v) : val(v) {}
++#elif defined(CUDA_HALF_INSTRUCTIONS)
+ #else
+   TensorRemainderOp(half v): fval(THC_half2float(v)) {}
+ #endif
+ 
+   __device__ __forceinline__ void operator()(half* out, half* in) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(CUDA_HALF_INSTRUCTIONS)
+     *out = __hsub(*in,  __hmul(val, hfloor(__hdiv(*in,  val))));
++#elif defined(__HIP_PLATFORM_HCC__)
++    *out = __hsub(*in,  __hmul(val, hfloor(hdiv(*in,  val))));
+ #else
+     float fin = __half2float(*in);
+     float fout = fin - fval * floorf(fin / fval);
+@@ -308,8 +346,10 @@ struct TensorRemainderOp<half> {
+   }
+ 
+   __device__ __forceinline__ void operator()(half* v) {
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(CUDA_HALF_INSTRUCTIONS)
+     *v = __hsub(*v, __hmul(val, hfloor(__hdiv(*v, val))));
++#elif defined(__HIP_PLATFORM_HCC__)
++    *v = __hsub(*v, __hmul(val, hfloor(hdiv(*v, val))));
+ #else
+     float fv = __half2float(*v);
+     fv = fv - fval * floorf(fv / fval);
+@@ -317,7 +357,7 @@ struct TensorRemainderOp<half> {
+ #endif
+   }
+ 
+-#ifdef CUDA_HALF_INSTRUCTIONS
++#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
+   const half val;
+ #else
+   const float fval;
+@@ -356,7 +396,12 @@ struct TensorFmodOp<double> {
+ #ifdef CUDA_HALF_TENSOR
+ template <>
+ struct TensorFmodOp<half> {
++#if defined(__HIP_PLATFORM_HCC__)
++  __host__ __device__
++  TensorFmodOp(half v): fval(v) {}
++#else
+   TensorFmodOp(half v): fval(THC_half2float(v)) {}
++#endif
+ 
+   __device__ __forceinline__ void operator()(half* out, half* in) {
+     *out = __float2half(fmodf(__half2float(*in), fval));
+diff --git a/aten/src/THC/THCTensorMathReduce.cuh b/aten/src/THC/THCTensorMathReduce.cuh
+index b5282d97e..79e23d355 100644
+--- a/aten/src/THC/THCTensorMathReduce.cuh
++++ b/aten/src/THC/THCTensorMathReduce.cuh
+@@ -105,14 +105,24 @@ struct SquareFunctor<ResT, half> {
+ template <typename T>
+ struct ReduceMin {
+   inline __device__ T operator()(T a, T b) const {
+-    return THCNumerics<T>::lt(a, b) ? a : b;
++    #if defined(__HIP_PLATFORM_HCC__)
++       T diff = THCNumerics<T>::sub(a, b);
++       return (diff < 0 ) ? a : b;
++    #else
++      return THCNumerics<T>::lt(a, b) ? a : b;
++    #endif
+   }
+ };
+ 
+ template <typename T>
+ struct ReduceMax {
+   inline __device__ T operator()(T a, T b) const {
+-    return THCNumerics<T>::gt(a, b) ? a : b;
++    #if defined(__HIP_PLATFORM_HCC__)
++       T diff = THCNumerics<T>::sub(a, b);
++       return (diff > 0 ) ? a : b;
++    #else
++      return THCNumerics<T>::gt(a, b) ? a : b;
++    #endif
+   }
+ };
+ 
+@@ -325,7 +335,7 @@ __global__ void THCTensor_kernel_varOuterDim(Real *tgt, Real *src_, unsigned num
+             THCNumerics<Accreal>::mul(delta, delta2));
+         src += num_irows;
+       }
+-      
++
+       if (flag) {
+         m2 = THCNumerics<Accreal>::div(m2, ScalarConvert<int, Accreal>::to(row_size));
+       } else {
+@@ -399,8 +409,8 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
+    * Each block computes the var/std of blockDim.y (32) rows at once.
+    * One can visualize the computation as a 16 (x) by 32 (y) grid.
+    * - Each of the 32 rows of the block is responsible for the computation
+-   *   of one input row. 
+-   * - Each row has 16 columns; the variance computation of one input row is 
++   *   of one input row.
++   * - Each row has 16 columns; the variance computation of one input row is
+    *   split between 16 threads.
+    * - Each of those 16 threads handles the accumulation of 1/16 of the input
+    *   row's data.
+@@ -438,11 +448,11 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
+ 
+     /*
+      * We are reducing across each row of 16 threads to find the true sum of the
+-     * entire input row. The warp shfl xor loop ultimately gives each thread the 
++     * entire input row. The warp shfl xor loop ultimately gives each thread the
+      * true sum.
+      */
+     for (unsigned lane_mask = 8; lane_mask > 0; lane_mask >>= 1) {
+-      local_sum = THCNumerics<Accreal>::add(local_sum, 
++      local_sum = THCNumerics<Accreal>::add(local_sum,
+           WARP_SHFL_XOR((row < num_rows) ? local_sum : acc_zero, lane_mask, 16));
+     }
+     Accreal true_mean = THCNumerics<Accreal>::div(local_sum, 
+@@ -468,7 +478,7 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
+      * the total sum, which is equal to the M2 for the entire input row.
+      */
+     for (unsigned s = 8; s >= 1; s >>= 1) {
+-      adjusted_M2 = THCNumerics<Accreal>::add(adjusted_M2, 
++      adjusted_M2 = THCNumerics<Accreal>::add(adjusted_M2,
+           WARP_SHFL_DOWN((row < num_rows) ? adjusted_M2 : acc_zero, s, 16));
+     }
+ 
+diff --git a/aten/src/THC/THCTensorRandom.cpp b/aten/src/THC/THCTensorRandom.cpp
+index ddccb7c5a..76cc2d74e 100644
+--- a/aten/src/THC/THCTensorRandom.cpp
++++ b/aten/src/THC/THCTensorRandom.cpp
+@@ -83,7 +83,7 @@ THCGenerator* THCRandom_getGenerator(THCState* state)
+   return gen;
+ }
+ 
+-struct curandStateMtgp32* THCRandom_generatorStates(struct THCState* state)
++curandStateMtgp32* THCRandom_generatorStates(struct THCState* state)
+ {
+   return THCRandom_getGenerator(state)->gen_states;
+ }
+diff --git a/aten/src/THC/THCTensorRandom.cu b/aten/src/THC/THCTensorRandom.cu
+index a179213cd..77cb5aaa2 100644
+--- a/aten/src/THC/THCTensorRandom.cu
++++ b/aten/src/THC/THCTensorRandom.cu
+@@ -1,3 +1,4 @@
++#include "hip/hip_runtime.h"
+ #include "THCTensorRandom.h"
+ #include "THCDeviceUtils.cuh"
+ #include "THCGeneral.h"
+@@ -6,67 +7,68 @@
+ #include "THCReduceApplyUtils.cuh"
+ #include "THCTensorRandom.cuh"
+ 
++#include <hiprng.h>
++#include <hiprng_kernel.h>
++
+ #include <thrust/functional.h>
+-#include <curand.h>
+-#include <curand_kernel.h>
+-#include <curand_mtgp32_host.h>
+-#include <curand_mtgp32dc_p_11213.h>
+ 
+-#define MAX_NUM_BLOCKS 200 
++#define MAX_NUM_BLOCKS 64
+ #define BLOCK_SIZE 256
+ 
+-
+ THCGenerator* THCRandom_getGenerator(THCState* state);
+ 
+ /* Sets up generator. Allocates but does not create the generator states. */
+-__host__ void initializeGenerator(THCState *state, THCGenerator* gen)
++void initializeGenerator(THCState *state, THCGenerator* gen)
+ {
+-  THCudaCheck(THCudaMalloc(state, (void**)&gen->gen_states, MAX_NUM_BLOCKS * sizeof(curandStateMtgp32)));
++  THCudaCheck(THCudaMalloc(state, (void**)&gen->gen_states, MAX_NUM_BLOCKS * sizeof(hiprngStateMtgp32)));
+   THCudaCheck(THCudaMalloc(state, (void**)&gen->kernel_params, sizeof(mtgp32_kernel_params)));
+ }
+ 
+ /* Creates a new generator state given the seed. */
+-__host__ void createGeneratorState(THCGenerator* gen, uint64_t seed)
++void createGeneratorState(THCGenerator* gen, uint64_t seed)
+ {
+-  if (curandMakeMTGP32Constants(mtgp32dc_params_fast_11213, gen->kernel_params) != CURAND_STATUS_SUCCESS)
++  if (hiprngMakeMTGP32Constants(mtgp32_params_fast_11213, gen->kernel_params) != HIPRNG_STATUS_SUCCESS)
+   {
+     THError("Creating MTGP constants failed.");
+   }
+-  if (curandMakeMTGP32KernelState(gen->gen_states, mtgp32dc_params_fast_11213,
+-                                  gen->kernel_params, MAX_NUM_BLOCKS, seed) != CURAND_STATUS_SUCCESS)
++  if (hiprngMakeMTGP32KernelState(gen->gen_states, mtgp32_params_fast_11213,
++                                  gen->kernel_params, MAX_NUM_BLOCKS, seed) != HIPRNG_STATUS_SUCCESS)
+   {
+     THError("Creating MTGP kernel state failed.");
+   }
+ }
+ 
+-__host__ void THCRandom_getRNGState(THCState* state, THByteTensor *rng_state)
++void THCRandom_getRNGState(THCState* state, THByteTensor *rng_state)
+ {
+   THCGenerator* gen = THCRandom_getGenerator(state);
+ 
+   // The RNG state comprises the MTPG32 states, the seed, and an offset used for Philox
+-  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(curandStateMtgp32);
++  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(hiprngStateMtgp32);
++  //static const size_t seed_size = sizeof(unsigned long);
+   static const size_t seed_size = sizeof(gen->initial_seed);
+   static const size_t offset_size = sizeof(gen->philox_seed_offset);
+   static const size_t total_size = states_size + seed_size + offset_size;
+   THByteTensor_resize1d(rng_state, total_size);
+   THArgCheck(THByteTensor_nElement(rng_state) == total_size, 1, "RNG state is wrong size");
+   THArgCheck(THByteTensor_isContiguous(rng_state), 1, "RNG state must be contiguous");
+-  THCudaCheck(cudaMemcpy(THByteTensor_data(rng_state), gen->gen_states,
+-                         states_size, cudaMemcpyDeviceToHost));
++  THCudaCheck(hipMemcpy(THByteTensor_data(rng_state), gen->gen_states,
++                         states_size, hipMemcpyDeviceToHost));
+   memcpy(THByteTensor_data(rng_state) + states_size, &gen->initial_seed, seed_size);
+   memcpy(THByteTensor_data(rng_state) + states_size + seed_size, &gen->philox_seed_offset, offset_size);
++
+ }
+ 
+-__global__ void set_rngstate_kernel(curandStateMtgp32 *state, mtgp32_kernel_params *kernel)
++__global__ void set_rngstate_kernel(hiprngStateMtgp32 *state, mtgp32_kernel_params *kernel)
+ {
+-  state[threadIdx.x].k = kernel;
++  state[hipThreadIdx_x].k = kernel;
+ }
+ 
+-__host__ void THCRandom_setRNGState(THCState* state, THByteTensor *rng_state)
++
++void THCRandom_setRNGState(THCState* state, THByteTensor *rng_state)
+ {
+   THCGenerator* gen = THCRandom_getGenerator(state);
+-
+-  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(curandStateMtgp32);
++  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(hiprngStateMtgp32);
++  //static const size_t seed_size = sizeof(unsigned long);
+   static const size_t seed_size = sizeof(gen->initial_seed);
+   static const size_t offset_size = sizeof(gen->philox_seed_offset);
+   static const size_t total_size = states_size + seed_size + offset_size;
+@@ -79,46 +81,55 @@ __host__ void THCRandom_setRNGState(THCState* state, THByteTensor *rng_state)
+   }
+   THArgCheck(THByteTensor_isContiguous(rng_state), 1, "RNG state must be contiguous");
+ 
+-  THCudaCheck(cudaMemcpy(gen->gen_states, THByteTensor_data(rng_state),
+-                         states_size, cudaMemcpyHostToDevice));
+-  set_rngstate_kernel<<<1, MAX_NUM_BLOCKS, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, gen->kernel_params);
+-  memcpy(&gen->initial_seed, THByteTensor_data(rng_state) + states_size, seed_size);
+-  if (!no_philox_seed) {
+-    memcpy(&gen->philox_seed_offset, THByteTensor_data(rng_state) + states_size + seed_size, offset_size);
+-  }
+-  else {
+-    gen->philox_seed_offset = 0;
+-  }
++  THCudaCheck(hipMemcpy(gen->gen_states, THByteTensor_data(rng_state),
++                         states_size, hipMemcpyHostToDevice));
++  hipLaunchKernelGGL(
++    set_rngstate_kernel,
++    dim3(1),
++    dim3(MAX_NUM_BLOCKS),
++    0,
++    THCState_getCurrentStream(state),
++    gen->gen_states,
++    gen->kernel_params);
++
++   memcpy(&gen->initial_seed, THByteTensor_data(rng_state) + states_size, seed_size);
++
++   if (!no_philox_seed) {
++     memcpy(&gen->philox_seed_offset, THByteTensor_data(rng_state) + states_size + seed_size, offset_size);
++   }
++   else {
++     gen->philox_seed_offset = 0;
++   }
+ }
+ 
++// CURAND_PATH
++
+ // Goes from (0, 1] to [0, 1). Note 1-x is not sufficient since for some floats
+ // eps near 0, 1-eps will round to 1.
+-template <typename T>
+-__device__ inline T reverse_bounds(T value) {
++ template <typename T>
++ __device__ inline T reverse_bounds(T value) {
+   if (THCNumerics<T>::eq(value, ScalarConvert<int, T>::to(1))) {
+     return ScalarConvert<int, T>::to(0);
+   }
+   return value;
+ }
+ 
+-
+ #ifdef CUDA_HALF_TENSOR
+ __device__ inline half half_uniform_scale_and_shift(float x, double a, double b) {
+-  half width = ScalarConvert<double, half>::to(b - a);
+-  half start = ScalarConvert<double, half>::to(a);
+-  half scaled = THCNumerics<half>::mul(reverse_bounds(ScalarConvert<float, half>::to(x)), width);
+-  return THCNumerics<half>::add(scaled, start);
++   half width = ScalarConvert<double, half>::to(b - a);
++   half start = ScalarConvert<double, half>::to(a);
++   half scaled = THCNumerics<half>::mul(reverse_bounds(ScalarConvert<float, half>::to(x)), width);
++   return THCNumerics<half>::add(scaled, start);
+ }
+ #endif
+ 
+ #define GENERATE_KERNEL1(NAME, T, ARG1, CURAND_T, CURAND_FUNC, TRANSFORM)      \
+-__global__ void NAME(curandStateMtgp32 *state, int size, T *result, ARG1)      \
++__global__ void NAME(hiprngStateMtgp32 *state, int size, T *result, ARG1)      \
+ {                                                                              \
+-  int idx = blockIdx.x * BLOCK_SIZE + threadIdx.x;                             \
++  int idx = hipBlockIdx_x * BLOCK_SIZE + hipThreadIdx_x;                             \
+   int rounded_size = THCCeilDiv(size, BLOCK_SIZE) * BLOCK_SIZE;                \
+   for (int i = idx; i < rounded_size; i += BLOCK_SIZE * MAX_NUM_BLOCKS) {      \
+-    CURAND_T x = CURAND_FUNC(&state[blockIdx.x]);                              \
++    CURAND_T x = CURAND_FUNC(&state[hipBlockIdx_x]);                              \
+     if (i < size) {                                                            \
+       T y = TRANSFORM;                                                         \
+       result[i] = y;                                                           \
+@@ -127,12 +138,12 @@ __global__ void NAME(curandStateMtgp32 *state, int size, T *result, ARG1)      \
+ }
+ 
+ #define GENERATE_KERNEL2(NAME, T, ARG1, ARG2, CURAND_T, CURAND_FUNC, TRANSFORM)      \
+-__global__ void NAME(curandStateMtgp32 *state, int size, T *result, ARG1, ARG2)      \
++__global__ void NAME(hiprngStateMtgp32 *state, int size, T *result, ARG1, ARG2)      \
+ {                                                                                    \
+-  int idx = blockIdx.x * BLOCK_SIZE + threadIdx.x;                                   \
++  int idx = hipBlockIdx_x * BLOCK_SIZE + hipThreadIdx_x;                                   \
+   int rounded_size = THCCeilDiv(size, BLOCK_SIZE) * BLOCK_SIZE;                      \
+   for (int i = idx; i < rounded_size; i += BLOCK_SIZE * MAX_NUM_BLOCKS) {            \
+-    CURAND_T x = CURAND_FUNC(&state[blockIdx.x]);                                    \
++    CURAND_T x = CURAND_FUNC(&state[hipBlockIdx_x]);                                    \
+     if (i < size) {                                                                  \
+       T y = TRANSFORM;                                                               \
+       result[i] = y;                                                                 \
+@@ -140,6 +151,7 @@ __global__ void NAME(curandStateMtgp32 *state, int size, T *result, ARG1, ARG2)
+   }                                                                                  \
+ }
+ 
++
+ template<typename T, typename U>
+ struct is_same { static const bool value = false; };
+ 
+@@ -147,44 +159,44 @@ template<typename T>
+ struct is_same<T, T> { static const bool value = true; };
+ 
+ template<typename real, typename prob_type>
+-__global__ void generate_bernoulli_tensor(curandStateMtgp32 *state, int size,
++__global__ void generate_bernoulli_tensor(hiprngStateMtgp32 *state, int size,
+         real *result, prob_type *probs)
+ {
+-  int idx = blockIdx.x * BLOCK_SIZE + threadIdx.x;
++  int idx = hipBlockIdx_x * BLOCK_SIZE + hipThreadIdx_x;
+   int rounded_size = THCCeilDiv(size, BLOCK_SIZE) * BLOCK_SIZE;
+   for (int i = idx; i < rounded_size; i += BLOCK_SIZE * MAX_NUM_BLOCKS) {
+     if (is_same<prob_type, double>::value) {
+-      double x = curand_uniform_double(&state[blockIdx.x]);
++      double x = hiprng_uniform(&state[hipBlockIdx_x]);
+       if (i < size)
+         result[i] = ScalarConvert<bool, real>::to(x <= probs[i]);
+     } else {
+-      float x = curand_uniform(&state[blockIdx.x]);
++      float x = hiprng_uniform(&state[hipBlockIdx_x]);
+       if (i < size)
+         result[i] = ScalarConvert<bool, real>::to(x <= probs[i]);
+     }
+   }
+ }
+ 
+-// NOTE: curand_uniform is (0, 1] and we want [a, b)
+-GENERATE_KERNEL2(generate_uniform, float, float a, float b, float, curand_uniform, reverse_bounds(x) * (b-a) + a)
+-GENERATE_KERNEL2(generate_uniform, double, double a, double b, double, curand_uniform_double, reverse_bounds(x) * (b-a) + a)
++GENERATE_KERNEL2(generate_uniform, float, double a, double b, float, hiprng_uniform, x * (b-a) + a)
++GENERATE_KERNEL2(generate_uniform, double, double a, double b, double, hiprng_uniform_double, x * (b-a) + a)
+ 
+-GENERATE_KERNEL2(generate_normal, float, double mean, double stdv, float, curand_normal, (x * stdv) + mean)
+-GENERATE_KERNEL2(generate_normal, double, double mean, double stdv, double, curand_normal_double, (x * stdv) + mean)
++GENERATE_KERNEL2(generate_normal, float, double mean, double stdv, float, hiprng_normal, (x * stdv) + mean)
++GENERATE_KERNEL2(generate_normal, double, double mean, double stdv, double, hiprng_normal_double, (x * stdv) + mean)
+ 
+-GENERATE_KERNEL1(generate_exponential, float, double lambda, float, curand_uniform, (float)(-1. / lambda * log(x)))
+-GENERATE_KERNEL1(generate_exponential, double, double lambda, double, curand_uniform_double, (double)(-1. / lambda * log(x)))
++GENERATE_KERNEL1(generate_exponential, float, double lambda, float, hiprng_uniform, (float)(-1. / lambda * log(1-x)))
++GENERATE_KERNEL1(generate_exponential, double, double lambda, double, hiprng_uniform_double, (double)(-1. / lambda * log(1-x)))
+ 
+-GENERATE_KERNEL2(generate_cauchy, float, double median, double sigma, float, curand_uniform, (float)(median + sigma * tan(M_PI*(x-0.5))))
+-GENERATE_KERNEL2(generate_cauchy, double, double median, double sigma, double, curand_uniform_double, (double)(median + sigma * tan(M_PI*(x-0.5))))
++GENERATE_KERNEL2(generate_cauchy, float, double median, double sigma, float, hiprng_uniform, (float)(median + sigma * tan(M_PI*(x-0.5))))
++GENERATE_KERNEL2(generate_cauchy, double, double median, double sigma, double, hiprng_uniform_double, (double)(median + sigma * tan(M_PI*(x-0.5))))
+ 
+ #ifdef CUDA_HALF_TENSOR
+-GENERATE_KERNEL2(generate_uniform, half, double a, double b, float, curand_uniform, (half_uniform_scale_and_shift(x, a, b)))
+-GENERATE_KERNEL2(generate_normal, half, double mean, double stdv, float, curand_normal, (ScalarConvert<float, half>::to((x * stdv) + mean)))
+-GENERATE_KERNEL1(generate_exponential, half, double lambda, float, curand_uniform, (ScalarConvert<float, half>::to((float)(-1. / lambda * log(x)))))
+-GENERATE_KERNEL2(generate_cauchy, half, double median, double sigma, float, curand_uniform, (ScalarConvert<float, half>::to((float)(median + sigma * tan(M_PI*(x-0.5))))))
++GENERATE_KERNEL2(generate_uniform, half, double a, double b, float, hiprng_uniform, (half_uniform_scale_and_shift(x, a, b)))
++GENERATE_KERNEL2(generate_normal, half, double mean, double stdv, float, hiprng_normal, (ScalarConvert<float, half>::to((x * stdv) + mean)))
++GENERATE_KERNEL1(generate_exponential, half, double lambda, float, hiprng_uniform, (ScalarConvert<float, half>::to((float)(-1. / lambda * log(1-x)))))
++GENERATE_KERNEL2(generate_cauchy, half, double median, double sigma, float, hiprng_uniform, (ScalarConvert<float, half>::to((float)(median + sigma * tan(M_PI*(x-0.5))))))
+ #endif // CUDA_HALF_TENSOR
+ 
++
+ #include "generic/THCTensorRandom.cu"
+ #include "THCGenerateAllTypes.h"
+ 
+diff --git a/aten/src/THC/THCTensorRandom.h b/aten/src/THC/THCTensorRandom.h
+index 21fe6d942..43baaa22c 100644
+--- a/aten/src/THC/THCTensorRandom.h
++++ b/aten/src/THC/THCTensorRandom.h
+@@ -8,7 +8,7 @@
+ 
+ /* Generator */
+ typedef struct _Generator {
+-  struct curandStateMtgp32* gen_states;
++  struct hcrngStateMtgp32* gen_states;
+   struct mtgp32_kernel_params *kernel_params;
+   int initf;
+   uint64_t initial_seed;
+@@ -33,6 +33,6 @@ THC_API uint64_t THCRandom_initialSeed(struct THCState *state);
+ THC_API void THCRandom_getRNGState(struct THCState *state, THByteTensor *rng_state);
+ THC_API void THCRandom_setRNGState(struct THCState *state, THByteTensor *rng_state);
+ 
+-THC_API struct curandStateMtgp32* THCRandom_generatorStates(struct THCState* state);
++THC_API struct hcrngStateMtgp32* THCRandom_generatorStates(struct THCState* state);
+ 
+ #endif
+diff --git a/aten/src/THC/THCTensorTypeUtils.cuh b/aten/src/THC/THCTensorTypeUtils.cuh
+index 78bea9746..70368343e 100644
+--- a/aten/src/THC/THCTensorTypeUtils.cuh
++++ b/aten/src/THC/THCTensorTypeUtils.cuh
+@@ -143,6 +143,51 @@ struct ScalarInv {
+   static __host__ __device__ T to(const T v) { return ((T) 1) / v; }
+ };
+ 
++#if defined(__HIP_PLATFORM_HCC__)
++    template <>
++    struct ScalarNegate<half> {
++      __host__
++      static
++      half to(half v)
++      {
++          return -v;
++      }
++
++      __device__
++      static
++      half to(half v)
++      {
++          return -v;
++      }
++    };
++
++    template <>
++    struct ScalarInv<half> {
++      __host__
++      static
++      half to(half v)
++      {
++        float fv = THC_half2float(v);
++        fv = 1.0f / fv;
++        return THC_float2half(fv);
++      }
++
++      __device__
++      static
++      half to(half v)
++      {
++          return static_cast<half>(1) / v;
++      }
++    };
++
++// inline bool operator==(half a, half b) {
++//   return a == b;
++// }
++// 
++// inline bool operator!=(half a, half b) {
++//   return a != b;
++// }
++#else
+ #ifdef CUDA_HALF_TENSOR
+ template <>
+ struct ScalarNegate<half> {
+@@ -201,5 +246,6 @@ inline bool operator!=(half a, half b) {
+ }
+ 
+ #endif // CUDA_HALF_TENSOR
++#endif
+ 
+ #endif // THC_TENSOR_TYPE_UTILS_INC
+diff --git a/aten/src/THC/generic/THCTensorRandom.cu b/aten/src/THC/generic/THCTensorRandom.cu
+index ce49c5c88..ecf24e28e 100644
+--- a/aten/src/THC/generic/THCTensorRandom.cu
++++ b/aten/src/THC/generic/THCTensorRandom.cu
+@@ -1,3 +1,4 @@
++#include "hip/hip_runtime.h"
+ #ifndef THC_GENERIC_FILE
+ #define THC_GENERIC_FILE "generic/THCTensorRandom.cu"
+ #else
+@@ -5,7 +6,6 @@
+ #define NUM_BLOCKS min((int)THCCeilDiv(size, (ptrdiff_t) BLOCK_SIZE), MAX_NUM_BLOCKS)
+ 
+ #if defined(THC_REAL_IS_FLOAT) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_HALF)
+-
+ THC_API void THCTensor_(uniform)(THCState* state, THCTensor *self_, double a, double b)
+ {
+   THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+@@ -15,8 +15,8 @@ THC_API void THCTensor_(uniform)(THCState* state, THCTensor *self_, double a, do
+   THCTensor *self = THCTensor_(newContiguous)(state, self_);
+   real *data = THCTensor_(data)(state, self);
+ 
+-  generate_uniform<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, a, b);
++  hipLaunchKernelGGL((generate_uniform), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, a, b);
+ 
+   THCTensor_(freeCopyTo)(state, self, self_);
+ };
+@@ -30,8 +30,8 @@ THC_API void THCTensor_(normal)(THCState* state, THCTensor *self_, double mean,
+   THCTensor *self = THCTensor_(newContiguous)(state, self_);
+   real *data = THCTensor_(data)(state, self);
+ 
+-  generate_normal<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, mean, stdv);
++  hipLaunchKernelGGL((generate_normal), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, mean, stdv);
+ 
+   THCTensor_(freeCopyTo)(state, self, self_);
+ };
+@@ -69,8 +69,8 @@ THC_API void THCTensor_(logNormal)(THCState* state, THCTensor *self_, double mea
+   THCTensor *self = THCTensor_(newContiguous)(state, self_);
+   real *data = THCTensor_(data)(state, self);
+ 
+-  generateLogNormal<real><<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, mean, stdv);
++  hipLaunchKernelGGL((generateLogNormal<real>), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, mean, stdv);
+ 
+   THCTensor_(freeCopyTo)(state, self, self_);
+ };
+@@ -85,8 +85,8 @@ THC_API void THCTensor_(exponential)(THCState* state, THCTensor *self_, double l
+   THCTensor *self = THCTensor_(newContiguous)(state, self_);
+   real *data = THCTensor_(data)(state, self);
+ 
+-  generate_exponential<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, lambda);
++  hipLaunchKernelGGL((generate_exponential), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, lambda);
+ 
+   THCTensor_(freeCopyTo)(state, self, self_);
+ };
+@@ -101,8 +101,8 @@ THC_API void THCTensor_(cauchy)(THCState* state, THCTensor *self_, double median
+   THCTensor *self = THCTensor_(newContiguous)(state, self_);
+   real *data = THCTensor_(data)(state, self);
+ 
+-  generate_cauchy<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, median, sigma);
++  hipLaunchKernelGGL((generate_cauchy), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, median, sigma);
+ 
+   THCTensor_(freeCopyTo)(state, self, self_);
+ };
+@@ -110,10 +110,10 @@ THC_API void THCTensor_(cauchy)(THCState* state, THCTensor *self_, double median
+ void THCTensor_(renormRows)(struct THCState* state,
+                              THCTensor* t) {
+   THAssert(THCTensor_(nDimension)(state, t) == 2);
+-  int64_t rows = THCTensor_(size)(state, t, 0);
+-  int64_t cols = THCTensor_(size)(state, t, 1);
++  long rows = THCTensor_(size)(state, t, 0);
++  long cols = THCTensor_(size)(state, t, 1);
+ 
+-  cudaDeviceProp* props = THCState_getCurrentDeviceProperties(state);
++  hipDeviceProp_t* props = THCState_getCurrentDeviceProperties(state);
+   THAssert(props != NULL);
+ 
+   int numSM = props->multiProcessorCount;
+@@ -122,9 +122,7 @@ void THCTensor_(renormRows)(struct THCState* state,
+   dim3 grid(rows < numSM * 4 ? rows : numSM * 4);
+   dim3 block(cols < maxThreads ? cols : maxThreads);
+ 
+-  renormRowsL1<real>
+-    <<<grid, block, block.x * sizeof(real),
+-    THCState_getCurrentStream(state)>>>(THCTensor_(data)(state, t),
++  hipLaunchKernelGGL((renormRowsL1<real>), dim3(grid), dim3(block), block.x * sizeof(real), THCState_getCurrentStream(state), THCTensor_(data)(state, t),
+                                         rows, cols);
+ }
+ 
+@@ -142,9 +140,9 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
+              "prob_dist must be 1 or 2 dim");
+ 
+   // Categories are in the innermost dimension
+-  int64_t numDist =
++  long numDist =
+     inputSize == 1 ? 1 : THCTensor_(size)(state, prob_dist, 0);
+-  int64_t numCategoriesLong =
++  long numCategoriesLong =
+     inputSize == 1 ? THCTensor_(size)(state, prob_dist, 0) :
+     THCTensor_(size)(state, prob_dist, 1);
+ 
+@@ -162,20 +160,19 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
+                "replacement");
+   }
+ 
+-  int free_prob_dist = 0;
++  // It is possible that prob_dist is non-contiguous
++  THCTensor* probDistContig =
++    THCTensor_(newContiguous)(state, prob_dist);
+ 
+   // Restructure data for 2d
+   if (inputSize == 1) {
+-    THCTensor *temp = THCTensor_(new)(state);
+-    THCTensor_(unsqueeze1d)(state, temp, prob_dist, 0);
+-    prob_dist = temp;
+-    free_prob_dist = 1;
++    THCTensor_(resize2d)(state, probDistContig, 1, numCategories);
+   }
+ 
+   THCudaLongTensor_resize2d(state, self, numDist, n_sample);
+ 
+   // get current device properties
+-  cudaDeviceProp* props = THCState_getCurrentDeviceProperties(state);
++  hipDeviceProp_t* props = THCState_getCurrentDeviceProperties(state);
+   THAssert(props != NULL);
+   int numSM = props->multiProcessorCount;
+   int maxThreads = props->maxThreadsPerBlock;
+@@ -194,18 +191,12 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
+     dim3 block(numCategories < maxThreads ? numCategories : maxThreads);
+     dim3 grid(numDist < numSM * 4 ? numDist : numSM * 4);
+ 
+-    sampleMultinomialOnce<real, accreal>
+-      <<<grid, block,
+-         requiredShared,
+-         THCState_getCurrentStream(state)>>>(
++    hipLaunchKernelGGL((sampleMultinomialOnce<real, accreal>), dim3(grid), dim3(block), requiredShared, THCState_getCurrentStream(state),
+       THCudaLongTensor_data(state, self),
+       numDist,
+       numCategories,
+       THCTensor_(data)(state, sampled),
+-      THCTensor_(data)(state, prob_dist),
+-      THCTensor_(stride)(state, prob_dist, 0),
+-      THCTensor_(stride)(state, prob_dist, 1)
+-      );
++      THCTensor_(data)(state, probDistContig));
+     THCTensor_(free)(state, sampled);
+   } else {
+     // Generic, slow implementation with memory allocations
+@@ -213,11 +204,11 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
+     // For sampling without replacement, we modify the distribution
+     // for subsequent samples in this space
+     THCTensor* origDist = THCTensor_(new)(state);
+-    THCTensor_(resizeAs)(state, origDist, prob_dist);
+-    THCTensor_(copy)(state, origDist, prob_dist);
++    THCTensor_(resizeAs)(state, origDist, probDistContig);
++    THCTensor_(copy)(state, origDist, probDistContig);
+ 
+     THCTensor* normDist = THCTensor_(new)(state);
+-    THCTensor_(resizeAs)(state, normDist, prob_dist);
++    THCTensor_(resizeAs)(state, normDist, probDistContig);
+ 
+     THCTensor* prefixSum = THCTensor_(new)(state);
+ 
+@@ -240,8 +231,7 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
+       // distribution concurrently.
+       dim3 grid(numDist < MAX_NUM_BLOCKS ? numDist : MAX_NUM_BLOCKS);
+ 
+-      sampleMultinomialWithReplacement
+-        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
++      hipLaunchKernelGGL((sampleMultinomialWithReplacement), dim3(grid), dim3(block), 0, THCState_getCurrentStream(state),
+           gen->gen_states,
+           n_sample,
+           THCudaLongTensor_data(state, self),
+@@ -257,7 +247,7 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
+ 
+       // Each warp in a block will generate a sample from a different
+       // distribution concurrently.
+-      ptrdiff_t numBlocks = THCCeilDiv(numDist, (int64_t) 4);
++      ptrdiff_t numBlocks = THCCeilDiv(numDist, 4L);
+       dim3 grid(numBlocks < MAX_NUM_BLOCKS ? numBlocks : MAX_NUM_BLOCKS);
+ 
+       for (int sample = 0; sample < n_sample; ++sample) {
+@@ -273,8 +263,7 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
+ 
+         // The kernel can only draw one sample before we have to
+         // recalculate our distribution
+-        sampleMultinomialWithoutReplacement
+-          <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
++        hipLaunchKernelGGL((sampleMultinomialWithoutReplacement), dim3(grid), dim3(block), 0, THCState_getCurrentStream(state),
+             gen->gen_states,
+             n_sample,
+             sample,
+@@ -293,17 +282,21 @@ THC_API void THCTensor_(multinomial)(struct THCState *state,
+   // Revert data restructuring based on input sizes
+   if (inputSize == 1) {
+     THCudaLongTensor_resize1d(state, self, n_sample);
++
++    // Unfortunately, if prob_dist is contiguous already,
++    // newContiguous is not a private copy, so we have to restructure
++    // this too, so as to not affect prob_dist
++    THCTensor_(resize1d)(state, probDistContig, numCategories);
+   }
+-  if (free_prob_dist) {
+-    THCTensor_(free)(state, prob_dist);
+-  }
++
++  THCTensor_(free)(state, probDistContig);
+ }
+ 
+ THC_API void THCTensor_(multinomialAliasSetup)(THCState *state, THCTensor *_probs, THCudaLongTensor *_J, THCTensor *_q){
+   THAssert(THCTensor_(isContiguous)(state, _q));
+   THAssert(THCudaLongTensor_isContiguous(state, _J));
+   THAssert(THCTensor_(isContiguous)(state, _probs));
+-  int64_t inputsize = THCTensor_(nElement)(state, _probs);
++  long inputsize = THCTensor_(nElement)(state, _probs);
+   THCudaLongTensor *smaller = THCudaLongTensor_newWithSize1d(state, inputsize);
+   THCudaLongTensor *larger = THCudaLongTensor_newWithSize1d(state, inputsize);
+   THCudaLongTensor *smaller_short = THCudaLongTensor_newWithSize1d(state, inputsize);
+@@ -312,41 +305,38 @@ THC_API void THCTensor_(multinomialAliasSetup)(THCState *state, THCTensor *_prob
+   THCudaLongTensor_resize1d(state, _J, inputsize);
+   THCTensor_(resize1d)(state, _q, inputsize);
+ 
+-  real one = ScalarConvert<int64_t, real>::to(1);
++  real one = ScalarConvert<long, real>::to(1);
+   int inputBlockDim = THCCeilDiv((int)inputsize + BLOCK_SIZE - 1, BLOCK_SIZE);
+-  aliasMultinomialFilter
+-    <<<inputBlockDim, BLOCK_SIZE, 0, THCState_getCurrentStream(state) >>>(
+-                     THCTensor_(data)(state, _q),
+-                     THCTensor_(data)(state, _probs),
+-                     THCudaLongTensor_data(state, smaller),
+-                     THCudaLongTensor_data(state, larger),
+-                     THCudaLongTensor_data(state, _J),
+-                     THCudaLongTensor_data(state, smaller_short),
+-                     THCudaLongTensor_data(state, larger_short),
+-                     one, inputsize
+-                     );
++  hipLaunchKernelGGL((aliasMultinomialFilter), dim3(inputBlockDim), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state) ,
++								     THCTensor_(data)(state, _q),
++								     THCTensor_(data)(state, _probs),
++								     THCudaLongTensor_data(state, smaller),
++								     THCudaLongTensor_data(state, larger),
++								     THCudaLongTensor_data(state, _J),
++								     THCudaLongTensor_data(state, smaller_short),
++								     THCudaLongTensor_data(state, larger_short),
++								     one, inputsize
++								     );
+ 
+   THCudaLongTensor_nonzero(state, smaller_short, smaller);
+   THCudaLongTensor_nonzero(state, larger_short, larger);
+   int h_large_c = THCudaLongTensor_nElement(state, larger_short);
+   THCudaLongTensor_resize1d(state, smaller_short, inputsize);
+   THCudaLongTensor_resize1d(state, larger_short, inputsize);
+-  aliasMultinomialSetup
+-    <<<1, 1, 0, THCState_getCurrentStream(state)>>>(
+-                THCudaLongTensor_data(state, _J),
+-                THCTensor_(data)(state, _q),
+-                inputsize,
+-                THCudaLongTensor_data(state, smaller_short),
+-                THCudaLongTensor_data(state, larger_short),
+-                inputsize - h_large_c, h_large_c
+-                );
++  hipLaunchKernelGGL((aliasMultinomialSetup), dim3(1), dim3(1), 0, THCState_getCurrentStream(state),
++						    THCudaLongTensor_data(state, _J),
++						    THCTensor_(data)(state, _q),
++						    inputsize,
++						    THCudaLongTensor_data(state, smaller_short),
++						    THCudaLongTensor_data(state, larger_short),
++						    static_cast<int>(inputsize - h_large_c), h_large_c
++						    );
+   real q_max = THCTensor_(maxall)(state, _q);
+-  condDiv<<<
+-    inputBlockDim, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-                      THCTensor_(data)(state, _q),
+-                      THCudaLongTensor_data(state, _J),
+-                      inputsize, q_max
+-                      );
++  hipLaunchKernelGGL((condDiv), dim3(inputBlockDim), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++								      THCTensor_(data)(state, _q),
++								      THCudaLongTensor_data(state, _J),
++								      inputsize, q_max
++								      );
+ 
+   THCudaLongTensor_free(state, smaller);
+   THCudaLongTensor_free(state, larger);
+@@ -358,8 +348,8 @@ THC_API void THCTensor_(multinomialAliasDraw)(THCState *state, THCudaLongTensor
+   THAssert(THCTensor_(isContiguous)(state, _q));
+   THAssert(THCudaLongTensor_isContiguous(state, _J));
+   THCGenerator* gen = THCRandom_getGenerator(state);
+-  int64_t K = THCudaLongTensor_nElement(state, _J);
+-  int64_t output_nelem = THCudaLongTensor_nElement(state, self);
++  long K = THCudaLongTensor_nElement(state, _J);
++  long output_nelem = THCudaLongTensor_nElement(state, self);
+   ptrdiff_t size = THCudaLongTensor_nElement(state, self);
+ 
+   THCTensor *uniform = THCTensor_(newWithSize1d)(state, output_nelem);
+@@ -368,16 +358,15 @@ THC_API void THCTensor_(multinomialAliasDraw)(THCState *state, THCudaLongTensor
+   THCTensor_(uniform)(state, uniform, 0, K);
+   THCTensor_(uniform)(state, bernoulli, 0, 1);
+ 
+-  multinomialAliasDrawKernel
+-    <<<THCCeilDiv((int)output_nelem+BLOCK_SIZE-1, BLOCK_SIZE), BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-          size,
+-          THCudaLongTensor_data(state, self),
+-          THCudaLongTensor_data(state, _J),
+-          THCTensor_(data)(state, _q),
+-          K,
+-          THCTensor_(data)(state, uniform),
+-          THCTensor_(data)(state, bernoulli)
+-          );
++  hipLaunchKernelGGL((multinomialAliasDrawKernel), dim3(THCCeilDiv((int)output_nelem+BLOCK_SIZE-1, BLOCK_SIZE)), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++				  static_cast<int>(size),
++				  THCudaLongTensor_data(state, self),
++				  THCudaLongTensor_data(state, _J),
++				  THCTensor_(data)(state, _q),
++				  K,
++				  THCTensor_(data)(state, uniform),
++				  THCTensor_(data)(state, bernoulli)
++				  );
+ }
+ 
+ THC_API void THCTensor_(rand)(THCState *state, THCTensor *r_, THLongStorage *size)
+@@ -397,9 +386,9 @@ void THCTensor_(randn)(THCState *state, THCTensor *r_, THLongStorage *size)
+ #endif
+ 
+ #if defined(THC_REAL_IS_DOUBLE)
+-GENERATE_KERNEL1(generate_bernoulli, double, double p, double, curand_uniform_double, x <= p)
++GENERATE_KERNEL1(generate_bernoulli, double, double p, double, hiprng_uniform_double, x <= p)
+ #else
+-GENERATE_KERNEL1(generate_bernoulli, real, double p, float, curand_uniform, (ScalarConvert<bool, real>::to(x <= p)))
++GENERATE_KERNEL1(generate_bernoulli, real, double p, float, hiprng_uniform, (ScalarConvert<bool, real>::to(x <= p)))
+ #endif
+ 
+ THC_API void THCTensor_(bernoulli)(THCState* state, THCTensor *self_, double p)
+@@ -411,65 +400,87 @@ THC_API void THCTensor_(bernoulli)(THCState* state, THCTensor *self_, double p)
+   THCTensor *self = THCTensor_(newContiguous)(state, self_);
+   real *data = THCTensor_(data)(state, self);
+ 
+-  generate_bernoulli<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, p);
++  hipLaunchKernelGGL((generate_bernoulli), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, p);
+ 
+   THCTensor_(freeCopyTo)(state, self, self_);
+ };
+ 
+ void THCTensor_(bernoulli_Tensor)(THCState *state, THCTensor *self, THCTensor* p)
+ {
+-#if defined(THC_REAL_IS_FLOAT)
+-  THCTensor_(bernoulli_FloatTensor)(state, self, p);
+-#elif defined(THC_REAL_IS_DOUBLE)
+-  THCTensor_(bernoulli_DoubleTensor)(state, self, p);
+-#endif
++ #if defined(THC_REAL_IS_FLOAT)
++   THCTensor_(bernoulli_FloatTensor)(state, self, p);
++ #elif defined(THC_REAL_IS_DOUBLE)
++   THCTensor_(bernoulli_DoubleTensor)(state, self, p);
++ #endif
+ }
+ 
+-#define DEFINE_BERNOULLI_TENSOR(NAME, PROB_TYPE, PROB_DATA_TYPE)               \
+-THC_API void THCTensor_(NAME)(THCState* state,                                 \
+-        THCTensor *self_, PROB_TYPE *probs_)                                   \
+-{                                                                              \
+-  THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, self_, probs_));             \
+-  ptrdiff_t size = THCTensor_(nElement)(state, self_);                         \
+-  if (size == 0) return;                                                       \
+-  THCGenerator* gen = THCRandom_getGenerator(state);                           \
+-  THCTensor *self = THCTensor_(newContiguous)(state, self_);                   \
+-  PROB_TYPE *probs = PROB_TYPE##_newContiguous(state, probs_);                 \
+-  ptrdiff_t prob_size = PROB_TYPE##_nElement(state, probs);                    \
+-  real *result_data = THCTensor_(data)(state, self);                           \
+-  PROB_DATA_TYPE *probs_data = PROB_TYPE##_data(state, probs);                 \
+-                                                                               \
+-  THArgCheck(size == prob_size, 3, "inconsistent tensor size");                \
+-                                                                               \
+-  generate_bernoulli_tensor<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>( \
+-      gen->gen_states, size, result_data, probs_data);                         \
+-                                                                               \
+-  PROB_TYPE##_free(state, probs);                                              \
+-  THCTensor_(freeCopyTo)(state, self, self_);                                  \
+-}
++#if defined(__HIP_PLATFORM_HCC__)
++  #define DEFINE_BERNOULLI_TENSOR(NAME, PROB_TYPE, PROB_DATA_TYPE)               \
++  THC_API void THCTensor_(NAME)(THCState* state,                                 \
++          THCTensor *self_, PROB_TYPE *probs_)                                   \
++  {                                                                              \
++    THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, self_, probs_));             \
++    ptrdiff_t size = THCTensor_(nElement)(state, self_);                         \
++    if (size == 0) return;                                                       \
++    THCGenerator* gen = THCRandom_getGenerator(state);                           \
++    THCTensor *self = THCTensor_(newContiguous)(state, self_);                   \
++    PROB_TYPE *probs = PROB_TYPE##_newContiguous(state, probs_);                 \
++    ptrdiff_t prob_size = PROB_TYPE##_nElement(state, probs);                    \
++    real *result_data = THCTensor_(data)(state, self);                           \
++    PROB_DATA_TYPE *probs_data = PROB_TYPE##_data(state, probs);                 \
++                                                                                 \
++    THArgCheck(size == prob_size, 3, "inconsistent tensor size");                \
++                                                                                 \
++    hipLaunchKernelGGL(                                                          \
++      (generate_bernoulli_tensor), NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state), \
++        gen->gen_states, static_cast<int>(size), result_data, probs_data);       \
++                                                                                 \
++    PROB_TYPE##_free(state, probs);                                              \
++    THCTensor_(freeCopyTo)(state, self, self_);                                  \
++  }
++#else
++  #define DEFINE_BERNOULLI_TENSOR(NAME, PROB_TYPE, PROB_DATA_TYPE)               \
++  THC_API void THCTensor_(NAME)(THCState* state,                                 \
++          THCTensor *self_, PROB_TYPE *probs_)                                   \
++  {                                                                              \
++    THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, self_, probs_));             \
++    ptrdiff_t size = THCTensor_(nElement)(state, self_);                         \
++    if (size == 0) return;                                                       \
++    THCGenerator* gen = THCRandom_getGenerator(state);                              \
++    THCTensor *self = THCTensor_(newContiguous)(state, self_);                   \
++    PROB_TYPE *probs = PROB_TYPE##_newContiguous(state, probs_);                 \
++    ptrdiff_t prob_size = PROB_TYPE##_nElement(state, probs);                    \
++    real *result_data = THCTensor_(data)(state, self);                           \
++    PROB_DATA_TYPE *probs_data = PROB_TYPE##_data(state, probs);                 \
++                                                                                 \
++    THArgCheck(size == prob_size, 3, "inconsistent tensor size");                \
++                                                                                 \
++    hipLaunchKernelGGL((generate_bernoulli_tensor), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),  \
++        gen->gen_states, size, result_data, probs_data);                         \
++                                                                                 \
++    PROB_TYPE##_free(state, probs);                                              \
++    THCTensor_(freeCopyTo)(state, self, self_);                                  \
++  }
++#endif
+ 
+ DEFINE_BERNOULLI_TENSOR(bernoulli_FloatTensor, THCudaTensor, float)
+ DEFINE_BERNOULLI_TENSOR(bernoulli_DoubleTensor, THCudaDoubleTensor, double)
+ 
+ #if defined(THC_REAL_IS_DOUBLE)
+-GENERATE_KERNEL1(generate_geometric, double, double p, double, curand_uniform_double, ceil(log(x) / log(1-p)))
++GENERATE_KERNEL1(generate_geometric, double, double p, double, hiprng_uniform_double, ceil(log(x) / log(1-p)))
+ #else
+-GENERATE_KERNEL1(generate_geometric, real, double p, float, curand_uniform, (ScalarConvert<float, real>::to(ceilf(logf(x) / log(1-p)))))
++GENERATE_KERNEL1(generate_geometric, real, double p, float, hiprng_uniform, (ScalarConvert<float, real>::to(ceilf(logf(x) / log(1-p)))))
+ #endif
+ 
+ #if defined(THC_REAL_IS_LONG) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_FLOAT)
+-#define CURAND64(STATE) (((uint64_t)curand(STATE)) << 32) | (uint64_t)curand(STATE)
+-GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, curand, \
+-    static_cast<real>(static_cast<int32_t>((x % range) + base)))
+-GENERATE_KERNEL2(generate_random_64, real, int64_t base, uint64_t range, uint64_t, CURAND64, \
+-    static_cast<real>(static_cast<int64_t>((x % range) + base)))
++#define CURAND64(STATE) (((uint64_t) hiprng(&state[blockIdx.x])) << 32) | (uint64_t) hiprng(&state[blockIdx.x])
++GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, hiprng, (real)(x % range + base))
++GENERATE_KERNEL2(generate_random_64, real, int64_t base, uint64_t range, uint64_t, CURAND64, (real)(x % range + base))
+ #elif defined(THC_REAL_IS_HALF)
+-GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, curand,
+-    (ScalarConvert<int32_t, real>::to(static_cast<int32_t>(x % range + base))))
++GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, hiprng, (ScalarConvert<uint32_t, real>::to(x % range + base)))
+ #else
+-GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, curand,
+-    static_cast<real>(static_cast<int32_t>(x % range + base)))
++GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, hiprng, (real)(x % range + base))
+ #endif
+ 
+ THC_API void THCTensor_(geometric)(THCState* state, THCTensor *self_, double p)
+@@ -482,8 +493,8 @@ THC_API void THCTensor_(geometric)(THCState* state, THCTensor *self_, double p)
+   THCTensor *self = THCTensor_(newContiguous)(state, self_);
+   real *data = THCTensor_(data)(state, self);
+ 
+-  generate_geometric<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, p);
++  hipLaunchKernelGGL((generate_geometric), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, p);
+ 
+   THCTensor_(freeCopyTo)(state, self, self_);
+ };
+@@ -503,12 +514,12 @@ THC_API void THCTensor_(clampedRandom)(THCState* state, THCTensor *self_, int64_
+ 
+ #if defined(THC_REAL_IS_LONG) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_FLOAT)
+   if (range > 1ULL << 32) {
+-    generate_random_64<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-        gen->gen_states, size, data, min_val, range);
++    hipLaunchKernelGGL((generate_random_64), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++        gen->gen_states, static_cast<int>(size), data, static_cast<int64_t>(min_val), static_cast<uint64_t>(range));
+   } else {
+ #endif
+-    generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-        gen->gen_states, size, data, min_val, range);
++    hipLaunchKernelGGL((generate_random), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++        gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(min_val), static_cast<uint32_t>(range));
+ #if defined(THC_REAL_IS_LONG) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_FLOAT)
+   }
+ #endif
+@@ -533,27 +544,24 @@ THC_API void THCTensor_(random)(THCState* state, THCTensor *self_)
+   real *data = THCTensor_(data)(state, self);
+ 
+ #if defined(THC_REAL_IS_HALF)
+-  generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, 0UL, (1UL << HLF_MANT_DIG) + 1);
++  hipLaunchKernelGGL((generate_random), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(0UL), static_cast<uint32_t>((1UL << HLF_MANT_DIG) + 1));
+ #elif defined(THC_REAL_IS_FLOAT)
+-  generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, 0UL, (1UL << FLT_MANT_DIG) + 1);
++  hipLaunchKernelGGL((generate_random), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(0UL), static_cast<uint32_t>((1UL << FLT_MANT_DIG) + 1));
+ #elif defined(THC_REAL_IS_DOUBLE)
+-  generate_random_64<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, 0ULL, (1ULL << DBL_MANT_DIG) + 1);
++  hipLaunchKernelGGL((generate_random_64), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, static_cast<int64_t>(0ULL), static_cast<uint64_t>((1ULL << DBL_MANT_DIG) + 1));
+ #elif defined(THC_REAL_IS_LONG)
+-  generate_random_64<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, 0ULL, static_cast<uint64_t>(std::numeric_limits<real>::max()) + 1);
++  hipLaunchKernelGGL((generate_random_64), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, static_cast<int64_t>(0ULL), static_cast<uint64_t>(std::numeric_limits<real>::max()) + 1);
+ #else
+-  generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+-      gen->gen_states, size, data, 0UL, static_cast<uint32_t>(std::numeric_limits<real>::max()) + 1);
++  hipLaunchKernelGGL((generate_random), dim3(NUM_BLOCKS), dim3(BLOCK_SIZE), 0, THCState_getCurrentStream(state),
++      gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(0UL), static_cast<uint32_t>(std::numeric_limits<real>::max()) + 1);
+ #endif
+ 
+   THCTensor_(freeCopyTo)(state, self, self_);
+ };
+-
+-#undef HLF_MANT_DIG
+-#undef CURAND64
+ #undef NUM_BLOCKS
+ 
+ #endif
+diff --git a/aten/src/THC/generic/THCTensorSort.cu b/aten/src/THC/generic/THCTensorSort.cu
+index 06ed71f82..1b3b0374d 100644
+--- a/aten/src/THC/generic/THCTensorSort.cu
++++ b/aten/src/THC/generic/THCTensorSort.cu
+@@ -223,39 +223,45 @@ void sortViaThrust(THCState* state,
+   // Fill the indices with a global index across all slices
+   thrust::counting_iterator<int64_t> countIter(0);
+ 
++#if defined (__NVCC__)
+   thrust::copy(
+ #if CUDA_VERSION >= 7000
+     thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
+ #endif
+     countIter, countIter + totalElements, indexIter);
+-
++#endif
+   // First, we sort globally (across all slices) according to key
+   // (the values we're sorting)
+   if (dir) {
++#if defined (__NVCC__)
+     thrust::stable_sort_by_key(
+ #if CUDA_VERSION >= 7000
+       thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
+ #endif
+       keyIter, keyIter + totalElements, indexIter, ThrustGTOp<real>());
++#endif
+   } else {
++#if defined (__NVCC__)
+     thrust::stable_sort_by_key(
+ #if CUDA_VERSION >= 7000
+       thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
+ #endif
+       keyIter, keyIter + totalElements, indexIter, ThrustLTOp<real>());
++#endif
+   }
+ 
+   // Then, re-sort according to slice that each index is
+   // in. This completes the segment sort in Thrust, since we're
+   // stably sorting here, preserving the relative order of values
+   // per each slice
++#if defined (__NVCC__)
+   thrust::stable_sort_by_key(
+ #if CUDA_VERSION >= 7000
+     thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
+ #endif
+     indexIter, indexIter + totalElements, keyIter,
+     SliceComp(sliceSize));
+-
++#endif
+   // Translate the global integer 0-based index to a per-slice real
+   // Lua index
+   thrust::for_each(
+diff --git a/aten/src/THCS/generic/THCSTensorMath.cu b/aten/src/THCS/generic/THCSTensorMath.cu
+index 319652fdf..181ab7634 100644
+--- a/aten/src/THCS/generic/THCSTensorMath.cu
++++ b/aten/src/THCS/generic/THCSTensorMath.cu
+@@ -173,13 +173,16 @@ void THCSTensor_(sspaddmm)(THCState *state, THCSTensor *r_, real beta, THCSTenso
+ }
+ 
+ void THCSTensor_(hspmm)(THCState *state, THCSTensor *r_, real alpha, THCSTensor *sparse_, THCTensor *dense) {
++#if defined(__HIP_PLATFORM_HCC__)
++  #define THRUST_EXEC(fn, ...) // whitespace
++#else
+ #if CUDA_VERSION >= 7000
+   THCThrustAllocator thrustAlloc(state);
+ #define THRUST_EXEC(fn, ...) fn(thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)), ##__VA_ARGS__)
+ #else
+ #define THRUST_EXEC(fn, ...) fn(##__VA_ARGS__)
+ #endif
+-
++#endif
+   THCAssertSameGPU(THCSTensor_(checkGPU)(state, 2, 3, r_, sparse_, dense));
+ 
+   THArgCheck(sparse_->nDimensionI == 2, 3,
+@@ -507,7 +510,7 @@ void THCSTensor_(pow)(THCState *state, THCSTensor *r_, THCSTensor *t_, real valu
+ #if defined(THCS_REAL_IS_FLOAT) || defined(THCS_REAL_IS_DOUBLE) || defined(THCS_REAL_IS_HALF)
+ accreal THCSTensor_(normall)(THCState *state, THCSTensor *self, real value) {
+   THCSTensor* self_coalesced = THCSTensor_(newCoalesce)(state, self);
+-  accreal result = THCTensor_(normall)(state, self_coalesced->values, value); 
++  accreal result = THCTensor_(normall)(state, self_coalesced->values, value);
+   THCSTensor_(free)(state, self_coalesced);
+   return result;
+ }
+diff --git a/aten/src/THCUNN/Abs.cu b/aten/src/THCUNN/Abs.cu
+index f3c7592e2..8a5346879 100644
+--- a/aten/src/THCUNN/Abs.cu
++++ b/aten/src/THCUNN/Abs.cu
+@@ -8,7 +8,7 @@ struct absupdateOutput_functor
+ {
+   __device__ void operator()(T* output, const T* input) const
+   {
+-    *output = abs(*input);
++    *output = fabs(*input);
+   }
+ };
+ 
+diff --git a/aten/src/THCUNN/BCECriterion.cu b/aten/src/THCUNN/BCECriterion.cu
+index ccb40008c..1051a19af 100644
+--- a/aten/src/THCUNN/BCECriterion.cu
++++ b/aten/src/THCUNN/BCECriterion.cu
+@@ -9,6 +9,14 @@
+ #include <thrust/transform.h>
+ #include <thrust/transform_reduce.h>
+ 
++#if defined(__HIP_PLATFORM_HCC__)
++template <typename T>
++inline __host__ __device__ T eps();
++template <>
++inline __host__ __device__ float eps() { return 1e-12f; }
++template <>
++inline __host__ __device__ double eps() { return 1e-12; }
++#else
+ template <typename T>
+ inline __device__ T eps();
+ 
+@@ -17,6 +25,7 @@ inline __device__ float eps() { return 1e-12f; }
+ 
+ template <>
+ inline __device__ double eps() { return 1e-12; }
++#endif
+ 
+ template <typename Dtype, typename Acctype>
+ struct bce_functor
+diff --git a/aten/src/THCUNN/BatchNormalization.cu b/aten/src/THCUNN/BatchNormalization.cu
+index 865323a16..0f20ac46b 100644
+--- a/aten/src/THCUNN/BatchNormalization.cu
++++ b/aten/src/THCUNN/BatchNormalization.cu
+@@ -6,15 +6,16 @@
+ #include "THCDeviceTensor.cuh"
+ #include "THCDeviceTensorUtils.cuh"
+ #include "THCDeviceUtils.cuh"
+-const int WARP_SIZE = 32;
++
++const int WARP_SIZE = 64;
+ 
+ // The maximum number of threads in a block
+-const int MAX_BLOCK_SIZE = 512;
++const int MAX_BLOCK_SIZE = 256;
+ 
+ // Number of threads in a block given an input size up to MAX_BLOCK_SIZE
+ static int getNumThreads(int nElem) {
+-  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };
+-  for (int i = 0; i != 5; ++i) {
++  int threadSizes[3] = { 64, 128, MAX_BLOCK_SIZE };
++  for (int i = 0; i != 3; ++i) {
+     if (nElem <= threadSizes[i]) {
+       return threadSizes[i];
+     }
+@@ -67,7 +68,7 @@ struct GradOp {
+     : mean(m), input(i), gradOutput(g) {}
+   __device__ __forceinline__ Float2<Dtype, Acctype> operator()(int batch, int plane, int n) {
+     Dtype g = gradOutput[batch][plane][n];
+-    Dtype c = ScalarConvert<Acctype, Dtype>::to(input[batch][plane][n] - mean);
++    Dtype c = ScalarConvert<Acctype, Dtype>::to((input[batch][plane][n]).template as<Acctype>() - mean);
+     return Float2<Dtype, Acctype>(g, g * c);
+   }
+   const Acctype mean;
+@@ -196,11 +197,12 @@ __global__ void BatchNormalizationUpdateOutput_kernel(
+     Acctype unbiasedVar = varN / (N - 1);
+     saveMean[plane] = ScalarConvert<Acctype, Dtype>::to(mean);
+     saveStd[plane] = ScalarConvert<Acctype, Dtype>::to(invStd);
++
+     if (runningMean.data() != NULL) {
+-      runningMean[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * runningMean[plane] + momentum * mean);
++      runningMean[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * (runningMean[plane]).template as<Acctype>() + momentum * mean);
+     }
+     if (runningVar.data() != NULL) {
+-      runningVar[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * runningVar[plane] + momentum * unbiasedVar);
++      runningVar[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * (runningVar[plane]).template as<Acctype>() + momentum * unbiasedVar);
+     }
+   }
+ 
+@@ -240,7 +242,7 @@ __global__ void BatchNormalizationBackward_kernel(
+     stdVal = ScalarConvert<Dtype, Acctype>::to(saveStd[plane]);
+   } else {
+     mean = ScalarConvert<Dtype, Acctype>::to(runningMean[plane]);
+-    stdVal = 1 / sqrt(runningVar[plane] + eps);
++    stdVal = 1 / sqrt((runningVar[plane]).template as<Acctype>() + eps);
+   }
+ 
+   Acctype weightVal = weight.numElements() > 0 ? ScalarConvert<Dtype, Acctype>::to(weight[plane]) : Acctype(1);
+@@ -275,16 +277,15 @@ __global__ void BatchNormalizationBackward_kernel(
+ 
+   if (gradWeight.numElements() > 0) {
+     if (threadIdx.x == 0) {
+-      gradWeight[plane] += ScalarConvert<Acctype, Dtype>::to(scale * dotP * stdVal);
++      (gradWeight[plane]).template as<Dtype>() += ScalarConvert<Acctype, Dtype>::to(scale * dotP * stdVal);
+     }
+   }
+ 
+   if (gradBias.numElements() > 0) {
+     if (threadIdx.x == 0) {
+-      gradBias[plane] += ScalarConvert<Acctype, Dtype>::to(scale * gradOutputSum);
++      (gradBias[plane]).template as<Dtype>() += Scala
\ No newline at end of file
diff --git a/setup.py b/setup.py
index 7e68bbb93..ea27d171b 100644
--- a/setup.py
+++ b/setup.py
@@ -126,6 +126,10 @@ IS_WINDOWS = (platform.system() == 'Windows')
 IS_DARWIN = (platform.system() == 'Darwin')
 IS_LINUX = (platform.system() == 'Linux')
 
+# If using ROCM stack disable distributed for now
+if WITH_ROCM:
+  WITH_DISTRIBUTED=False
+
 NUM_JOBS = multiprocessing.cpu_count()
 max_jobs = os.getenv("MAX_JOBS")
 if max_jobs is not None:
@@ -222,6 +226,8 @@ def build_libs(libs):
     if WITH_CUDA:
         my_env["CUDA_BIN_PATH"] = CUDA_HOME
         build_libs_cmd += ['--with-cuda']
+    if WITH_ROCM:
+        build_libs_cmd += ['--with-rocm']
     if WITH_NNPACK:
         build_libs_cmd += ['--with-nnpack']
     if WITH_CUDNN:
@@ -315,6 +321,9 @@ class build_deps(Command):
                        'torch/lib/include/pybind11')
         self.copy_file('torch/torch.h', 'torch/lib/include/torch/torch.h')
 
+        if WITH_ROCM:
+            os.environ["CC"] = 'hipcc'
+            os.environ["CXX"] = 'hipcc'
 
 build_dep_cmds = {}
 
@@ -753,6 +762,41 @@ if WITH_CUDA:
         "torch/csrc/nn/THCUNN.cpp",
     ]
 
+elif WITH_ROCM:
+    rocm_include_path = '/opt/rocm/include'
+    hcc_include_path = '/opt/rocm/hcc/include'
+    hipblas_include_path = '/opt/rocm/hipblas/include'
+    hipsparse_include_path = '/opt/rocm/hcsparse/include'
+    print(rocm_include_path)
+    print(hcc_include_path)
+    print(hipblas_include_path)
+    print(hipsparse_include_path)
+    hip_lib_path = '/opt/rocm/hip/lib'
+    hcc_lib_path = '/opt/rocm/hcc/lib'
+    include_dirs.append(rocm_include_path)
+    include_dirs.append(hcc_include_path)
+    include_dirs.append(hipblas_include_path)
+    include_dirs.append(hipsparse_include_path)
+    include_dirs.append(tmp_install_path + "/include/THCUNN")
+    extra_link_args.append('-L' + hip_lib_path)
+    extra_link_args.append('-Wl,-rpath,' + hip_lib_path)
+    extra_link_args.append('-shared')
+    extra_compile_args += ['-DWITH_ROCM']
+    extra_compile_args += ['-D__HIP_PLATFORM_HCC__']
+
+    os.environ["LDSHARED"] = 'gcc'
+
+    main_sources += [
+        "torch/csrc/cuda/Module.cpp",
+        "torch/csrc/cuda/Storage.cpp",
+        "torch/csrc/cuda/Stream.cpp",
+        "torch/csrc/cuda/utils.cpp",
+        "torch/csrc/cuda/comm.cpp",
+        "torch/csrc/cuda/python_comm.cpp",
+        "torch/csrc/cuda/serialization.cpp",
+        "torch/csrc/nn/THCUNN.cpp",
+    ]
+
 if WITH_NCCL:
     if WITH_SYSTEM_NCCL:
         main_link_args += [NCCL_SYSTEM_LIB]
@@ -822,6 +866,7 @@ if not IS_WINDOWS:
     DL = Extension("torch._dl",
                    sources=["torch/csrc/dl.c"],
                    language='c',
+                   extra_link_args=['-shared']
                    )
     extensions.append(DL)
 
diff --git a/test/run_nn_tests_amd.sh b/test/run_nn_tests_amd.sh
new file mode 100644
index 000000000..5c34bf88d
--- /dev/null
+++ b/test/run_nn_tests_amd.sh
@@ -0,0 +1,175 @@
+python test_nn.py TestNN.test_module_backcompat
+python test_nn.py TestNN.test_hooks
+python test_nn.py TestNN.test_hook_cpp
+python test_nn.py TestNN.test_hook_fail
+python test_nn.py TestNN.test_hook_writeable
+python test_nn.py TestNN.test_zero_grad
+python test_nn.py TestNN.test_volatile
+python test_nn.py TestNN._test_dropout
+python test_nn.py TestNN.test_parameters
+python test_nn.py TestNN.test_named_parameters
+python test_nn.py TestNN.test_children
+python test_nn.py TestNN.test_dir
+python test_nn.py TestNN.test_named_children
+python test_nn.py TestNN.test_modules
+python test_nn.py TestNN.test_named_modules
+python test_nn.py TestNN.test_register_buffer_raises_error_if_attr_exists
+python test_nn.py TestNN.test_register_buffer_allows_overwriting_with_same_name
+python test_nn.py TestNN.test_register_parameter_raises_error_if_attr_exists
+python test_nn.py TestNN.test_register_parameter_allows_overwriting_with_same_name
+python test_nn.py TestNN.test_add_module_raises_error_if_attr_exists
+python test_nn.py TestNN.test_Sequential_getitem
+python test_nn.py TestNN.test_ListModule
+python test_nn.py TestNN.test_ParameterList
+python test_nn.py TestNN.test_add_module
+python test_nn.py TestNN.test_type
+python test_nn.py TestNN.test_non_leaf_parameters
+python test_nn.py TestNN.test_clip_grad_norm
+python test_nn.py TestNN.test_parameters_to_vector
+python test_nn.py TestNN.test_vector_to_parameters
+python test_nn.py TestNN.test_weight_norm
+python test_nn.py TestNN.test_weight_norm_pickle
+python test_nn.py TestNN.test_embedding_padding_idx
+python test_nn.py TestNN.test_embedding_max_norm
+python test_nn.py TestNN.test_embedding_max_norm_cuda
+python test_nn.py TestNN.test_embedding_functional
+python test_nn.py TestNN._test_EmbeddingBag
+python test_nn.py TestNN.test_EmbeddingBag
+python test_nn.py TestNN.test_EmbeddingBag_cuda
+python test_nn.py TestNN.test_Dropout
+python test_nn.py TestNN.test_Dropout2d
+python test_nn.py TestNN.test_Dropout3d
+python test_nn.py TestNN.test_AlphaDropout
+python test_nn.py TestNN._test_InstanceNorm
+python test_nn.py TestNN.test_InstanceNorm2d
+python test_nn.py TestNN.test_InstanceNorm1d
+python test_nn.py TestNN.test_InstanceNorm3d
+python test_nn.py TestNN.test_pad
+python test_nn.py TestNN.test_normalize
+python test_nn.py TestNN._test_maxpool_indices
+python test_nn.py TestNN.test_Conv2d_naive_groups
+python test_nn.py TestNN.test_Conv2d_naive_groups_cuda
+python test_nn.py TestNN.test_batchnorm_eval
+python test_nn.py TestNN.test_batchnorm_eval_cuda
+python test_nn.py TestNN.test_MaxPool1d_indices
+python test_nn.py TestNN.test_MaxPool1d_indices_cuda
+python test_nn.py TestNN.test_MaxPool2d_indices
+python test_nn.py TestNN.test_MaxPool2d_indices_cuda
+python test_nn.py TestNN.test_MaxPool3d_indices
+python test_nn.py TestNN.test_MaxPool3d_indices_cuda
+python test_nn.py TestNN.test_AdaptiveMaxPool1d_indices
+python test_nn.py TestNN.test_AdaptiveMaxPool1d_indices_cuda
+python test_nn.py TestNN.test_AdaptiveMaxPool2d_indices
+python test_nn.py TestNN.test_AdaptiveMaxPool2d_indices_cuda
+python test_nn.py TestNN._test_scatter
+python test_nn.py TestNN.test_scatter_cpu
+python test_nn.py TestNN.test_scatter_gpu
+python test_nn.py TestNN._test_gather
+python test_nn.py TestNN.test_gather_cpu
+python test_nn.py TestNN.test_gather_gpu
+python test_nn.py TestNN._test_broadcast_double_backwards
+python test_nn.py TestNN.test_broadcast_double_backwards_gpu
+python test_nn.py TestNN.test_replicate
+python test_nn.py TestNN.test_replicate_buffers
+python test_nn.py TestNN.test_parallel_apply
+python test_nn.py TestNN.test_data_parallel_multiple_input
+python test_nn.py TestNN.test_data_parallel_small_back
+python test_nn.py TestNN.test_data_parallel
+python test_nn.py TestNN.test_data_parallel_nested_output
+python test_nn.py TestNN.test_data_parallel_nested_input
+python test_nn.py TestNN.test_data_parallel_module
+python test_nn.py TestNN.test_data_parallel_module_kwargs_only
+python test_nn.py TestNN.test_state_dict
+python test_nn.py TestNN.test_load_state_dict
+python test_nn.py TestNN.test_parameter_assignment
+python test_nn.py TestNN.test_assignment
+python test_nn.py TestNN.def test_assignments
+python test_nn.py TestNN.test_Conv2d_inconsistent_types
+python test_nn.py TestNN.test_Conv2d_inconsistent_types_on_GPU_without_cudnn
+python test_nn.py TestNN.test_Conv2d_inconsistent_types_on_GPU_with_cudnn
+python test_nn.py TestNN.test_Conv2d_missing_argument
+python test_nn.py TestNN.test_Conv2d_backward_twice
+python test_nn.py TestNN.test_Conv2d_large_workspace
+python test_nn.py TestNN.test_conv_modules_raise_error_on_incorrect_input_size
+python test_nn.py TestNN.test_ConvTranspose2d_output_size
+python test_nn.py TestNN._test_Conv2d_naive_groups
+python test_nn.py TestNN.test_Conv2d_groups_nobias
+python test_nn.py TestNN.test_MaxUnpool2d_output_size
+python test_nn.py TestNN.test_container_copy
+python test_nn.py TestNN.test_RNN_cell
+python test_nn.py TestNN.test_invalid_dropout_p
+python test_nn.py TestNN.test_pack_padded_sequence
+python test_nn.py TestNN._test_variable_sequence
+python test_nn.py TestNN.test_variable_sequence
+python test_nn.py TestNN.test_variable_sequence_cuda
+python test_nn.py TestNN.test_LSTM_cell
+python test_nn.py TestNN.test_cudnn_weight_format
+python test_nn.py TestNN.test_cuda_rnn_fused
+python test_nn.py TestNN.test_rnn_initial_hidden_state
+python test_nn.py TestNN._test_rnn_retain_variables
+python test_nn.py TestNN.test_rnn_retain_variables
+python test_nn.py TestNN.test_rnn_retain_variables_cuda
+python test_nn.py TestNN._test_RNN_cpu_vs_cudnn
+python test_nn.py TestNN.test_RNN_cpu_vs_cudnn_no_dropout
+python test_nn.py TestNN.test_RNN_cpu_vs_cudnn_with_dropout
+python test_nn.py TestNN.test_RNN_dropout
+python test_nn.py TestNN.test_RNN_dropout_state
+python test_nn.py TestNN.test_RNN_change_dropout
+python test_nn.py TestNN.test_inplace_thnn
+python test_nn.py TestNN.test_noncontig_conv_grad
+python test_nn.py TestNN.test_pixel_shuffle
+python test_nn.py TestNN.test_bce_with_logits_raises_if_target_and_input_are_different_size
+python test_nn.py TestNN.test_bce_with_logits_gives_same_result_as_sigmoid_and_bce_loss
+python test_nn.py TestNN.test_bce_with_logits_has_correct_grad_at_zero
+python test_nn.py TestNN.test_bce_with_logits_broadcasts_weights
+python test_nn.py TestNN.test_bce_loss_broadcasts_weights
+python test_nn.py TestNN.test_batchnorm_raises_error_if_running_mean_is_not_same_size_as_input
+python test_nn.py TestNN.test_batchnorm_raises_error_if_running_var_is_not_same_size_as_input
+python test_nn.py TestNN.test_batchnorm_raises_error_if_weight_is_not_same_size_as_input
+python test_nn.py TestNN.test_batchnorm_raises_error_if_bias_is_not_same_size_as_input
+python test_nn.py TestNN._test_batchnorm_eval
+python test_nn.py TestNN.test_pairwise_distance
+python test_nn.py TestNN.test_triplet_margin_loss
+python test_nn.py TestNN.test_triplet_margin_swap_loss
+python test_nn.py TestNN.test_cosine_similarity
+python test_nn.py TestNN.test_grid_sample
+python test_nn.py TestNN.def test_cpu_against_cuda
+python test_nn.py TestNN.test_shape
+python test_nn.py TestNN.test_affine_grid
+python test_nn.py TestNN.test_upsamplingNearest1d
+python test_nn.py TestNN.test_upsamplingLinear1d
+python test_nn.py TestNN.test_upsamplingNearest2d
+python test_nn.py TestNN.test_upsamplingBilinear2d
+python test_nn.py TestNN.test_upsamplingNearest3d
+python test_nn.py TestNN.test_upsamplingTrilinear3d
+python test_nn.py TestNN.test_linear_broadcasting
+python test_nn.py TestNN.test_bilinear
+python test_nn.py TestNN.test_conv_double_backward
+python test_nn.py TestNN.test_conv_double_backward_no_bias
+python test_nn.py TestNN.test_conv_double_backward_groups
+python test_nn.py TestNN.test_conv_double_backward_stride
+python test_nn.py TestNN.test_conv_double_backward_cuda
+python test_nn.py TestNN.test_calculate_gain_linear
+python test_nn.py TestNN.test_calculate_gain_nonlinear
+python test_nn.py TestNN.test_calculate_gain_leaky_relu
+python test_nn.py TestNN.test_calculate_gain_leaky_relu_only_accepts_numbers
+python test_nn.py TestNN.test_calculate_gain_only_accepts_valid_nonlinearities
+python test_nn.py TestNN.test_uniform
+python test_nn.py TestNN.test_normal
+python test_nn.py TestNN.test_constant
+python test_nn.py TestNN.test_eye
+python test_nn.py TestNN.test_eye_only_works_on_2d_inputs
+python test_nn.py TestNN.test_dirac_properties
+python test_nn.py TestNN.test_dirac_identity
+python test_nn.py TestNN.test_dirac_only_works_on_3_4_5d_inputs
+python test_nn.py TestNN.test_xavier_uniform_errors_on_inputs_smaller_than_2d
+python test_nn.py TestNN.test_xavier_normal_errors_on_inputs_smaller_than_2d
+python test_nn.py TestNN.test_xavier_uniform
+python test_nn.py TestNN.test_xavier_normal
+python test_nn.py TestNN.test_kaiming_uniform_errors_on_inputs_smaller_than_2d
+python test_nn.py TestNN.test_kaiming_normal_errors_on_inputs_smaller_than_2d
+python test_nn.py TestNN.test_kaiming_uniform
+python test_nn.py TestNN.test_kaiming_normal
+python test_nn.py TestNN.test_sparse_only_works_on_2d_inputs
+python test_nn.py TestNN.test_sparse_default_std
+python test_nn.py TestNN.test_orthogonal
diff --git a/tools/build_pytorch_libs.sh b/tools/build_pytorch_libs.sh
index cf856421a..cd85e05b2 100755
--- a/tools/build_pytorch_libs.sh
+++ b/tools/build_pytorch_libs.sh
@@ -12,9 +12,13 @@ set -ex
 
 # Options for building only a subset of the libraries
 WITH_CUDA=0
+WITH_ROCM=0
 if [[ "$1" == "--with-cuda" ]]; then
   WITH_CUDA=1
   shift
+elif [[ "$1" == "--with-rocm" ]]; then
+  WITH_ROCM=1
+  shift
 fi
 
 WITH_NNPACK=0
@@ -74,13 +78,13 @@ C_FLAGS="${C_FLAGS} -DOMPI_SKIP_MPICXX=1"
 LDFLAGS="-L\"$INSTALL_DIR/lib\" "
 LD_POSTFIX=".so.1"
 LD_POSTFIX_UNVERSIONED=".so"
-if [[ $(uname) == 'Darwin' ]]; then
-    LDFLAGS="$LDFLAGS -Wl,-rpath,@loader_path"
-    LD_POSTFIX=".1.dylib"
-    LD_POSTFIX_UNVERSIONED=".dylib"
-else
-    LDFLAGS="$LDFLAGS -Wl,-rpath,\$ORIGIN"
-fi
+# if [[ $(uname) == 'Darwin' ]]; then
+#     LDFLAGS="$LDFLAGS -Wl,-rpath,@loader_path"
+#     LD_POSTFIX=".1.dylib"
+#     LD_POSTFIX_UNVERSIONED=".dylib"
+# else
+#     LDFLAGS="$LDFLAGS -Wl,-rpath,\$ORIGIN"
+# fi
 CPP_FLAGS=" -std=c++11 "
 GLOO_FLAGS=""
 THD_FLAGS=""
@@ -179,6 +183,36 @@ function build() {
   fi
 }
 
+function build_rocm_aten() {
+  mkdir -p build
+  pushd build
+  ${CMAKE_VERSION} .. \
+  ${CMAKE_GENERATOR} \
+      -DCMAKE_BUILD_TYPE=$BUILD_TYPE \
+      -DNO_CUDA=$((1-$WITH_CUDA)) \
+      -DNO_NNPACK=$((1-$WITH_NNPACK)) \
+      -DCUDNN_INCLUDE_DIR=$CUDNN_INCLUDE_DIR \
+      -DCUDNN_LIB_DIR=$CUDNN_LIB_DIR \
+      -DCUDNN_LIBRARY=$CUDNN_LIBRARY \
+      -DNO_MKLDNN=$((1-$WITH_MKLDNN)) \
+      -DMKLDNN_INCLUDE_DIR=$MKLDNN_INCLUDE_DIR \
+      -DMKLDNN_LIB_DIR=$MKLDNN_LIB_DIR \
+      -DMKLDNN_LIBRARY=$MKLDNN_LIBRARY \
+      -DATEN_NO_CONTRIB=1 \
+      -DCMAKE_INSTALL_PREFIX="$INSTALL_DIR" \
+      -DCMAKE_EXPORT_COMPILE_COMMANDS=1 \
+      -DCMAKE_C_FLAGS="$USER_CFLAGS" \
+      -DCMAKE_CXX_FLAGS="$USER_CFLAGS" \
+      -DCMAKE_EXE_LINKER_FLAGS="$USER_LDFLAGS" \
+      -DWITH_ROCM=1 \
+      -DCMAKE_SHARED_LINKER_FLAGS="$USER_LDFLAGS"
+      # STOP!!! Are you trying to add a C or CXX flag?  Add it
+      # to aten/CMakeLists.txt, not here.  We need the vanilla
+      # cmake build to work.
+  ${CMAKE_INSTALL} -j"$NUM_JOBS"
+  popd
+}
+
 function build_nccl() {
   mkdir -p build/nccl
   pushd build/nccl
@@ -250,7 +284,11 @@ for arg in "$@"; do
         popd
     elif [[ "$arg" == "ATen" ]]; then
         pushd "$BASE_DIR/aten"
-        build_aten
+        if [[ $WITH_ROCM -eq 1 ]]; then
+          build_rocm_aten
+        else
+          build_aten
+        fi
         popd
     elif [[ "$arg" == "THD" ]]; then
         pushd "$TORCH_LIB_DIR"
diff --git a/torch/csrc/DynamicTypes.cpp b/torch/csrc/DynamicTypes.cpp
index 08702be83..4500e8ec6 100644
--- a/torch/csrc/DynamicTypes.cpp
+++ b/torch/csrc/DynamicTypes.cpp
@@ -10,7 +10,7 @@
 #include <unordered_map>
 #include <sstream>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #include <THCS/THCS.h>
 #endif
diff --git a/torch/csrc/Module.cpp b/torch/csrc/Module.cpp
index 167ca46b9..1183fb478 100644
--- a/torch/csrc/Module.cpp
+++ b/torch/csrc/Module.cpp
@@ -376,7 +376,7 @@ bool THCPByteStorage_init(PyObject *module);
 
 bool THCPStream_init(PyObject *module);
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 PyMethodDef* THCPModule_methods();
 namespace torch { namespace cuda {
 
@@ -388,7 +388,7 @@ void initModule(PyObject *module);
 namespace torch { namespace nn {
 
 void init__THNN(PyObject*);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 void init__THCUNN(PyObject*);
 #endif
 
@@ -435,7 +435,7 @@ static PyObject* initModule() {
   THPUtils_addPyMethodDefs(methods, TorchMethods);
   THPUtils_addPyMethodDefs(methods, DataLoaderMethods);
   THPUtils_addPyMethodDefs(methods, torch::autograd::python_functions());
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   THPUtils_addPyMethodDefs(methods, THCPModule_methods());
 #endif
 #ifdef WITH_CUDNN
@@ -471,7 +471,7 @@ static PyObject* initModule() {
   torch::jit::initJITBindings(module);
   torch::autograd::initNNFunctions(module);
   torch::autograd::init_legacy_variable(module);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   torch::cuda::initModule(module);
 #endif
   ASSERT_TRUE(THPDoubleStorage_init(module));
@@ -483,7 +483,7 @@ static PyObject* initModule() {
   ASSERT_TRUE(THPCharStorage_init(module));
   ASSERT_TRUE(THPByteStorage_init(module));
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   // This will only initialise base classes and attach them to library namespace
   // They won't be ready for real usage until importing cuda module, that will
   // complete the process (but it defines Python classes before calling back into
@@ -536,7 +536,7 @@ static PyObject* initModule() {
 #endif
 
   torch::nn::init__THNN(module);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   torch::nn::init__THCUNN(module);
 #endif
 
diff --git a/torch/csrc/THP_API.h b/torch/csrc/THP_API.h
index fbb9ce06c..f0e5ac291 100644
--- a/torch/csrc/THP_API.h
+++ b/torch/csrc/THP_API.h
@@ -6,7 +6,7 @@
     be defined only when compiling the core torch package.
 #endif
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "cuda/THCP.h"
 #include "cuda/undef_macros.h"
 #endif
diff --git a/torch/csrc/allocators.cpp b/torch/csrc/allocators.cpp
index 15e4f8712..fa1d0173f 100644
--- a/torch/csrc/allocators.cpp
+++ b/torch/csrc/allocators.cpp
@@ -59,7 +59,7 @@ THAllocator THStorageWeakRefAllocator = {
   free_wrapper<StorageWeakRefAllocator>,
 };
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 cudaError_t CudaStorageWeakRefAllocator::malloc(void** ptr, size_t size, cudaStream_t stream) {
   THError("CudaStorageWeakRefAllocator: malloc not supported");
   return cudaSuccess;
diff --git a/torch/csrc/allocators.h b/torch/csrc/allocators.h
index 25b2ac986..b6c431427 100644
--- a/torch/csrc/allocators.h
+++ b/torch/csrc/allocators.h
@@ -5,7 +5,7 @@
 #include <memory>
 
 #include <TH/TH.h>
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #endif
 
@@ -41,7 +41,7 @@ public:
   void free(void* ptr);
 };
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 class CudaStorageWeakRefAllocator {
 public:
   CudaStorageWeakRefAllocator(PyObject *wrapped_object, THCDeviceAllocator *alloc, void *ctx) {
@@ -63,6 +63,6 @@ public:
 
 extern THAllocator THObjectPtrAllocator;
 extern THAllocator THStorageWeakRefAllocator;
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 extern THCDeviceAllocator THCStorageWeakRefAllocator;
 #endif
diff --git a/torch/csrc/autograd/engine.cpp b/torch/csrc/autograd/engine.cpp
index 4eb52df3b..af8ee14d5 100644
--- a/torch/csrc/autograd/engine.cpp
+++ b/torch/csrc/autograd/engine.cpp
@@ -22,7 +22,7 @@
 #include <queue>
 #include <TH/TH.h>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda.h>
 #include <THC/THC.h>
 #endif
@@ -498,7 +498,7 @@ auto Engine::ready_queue(int device) -> ReadyQueue& {
 
 auto Engine::start_threads() -> void {
   int num_devices = 0;
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   // check for case of compiled with CUDA but no available devices
   if (cudaGetDeviceCount(&num_devices) != cudaSuccess) {
     cudaGetLastError();
diff --git a/torch/csrc/cuda/Module.cpp b/torch/csrc/cuda/Module.cpp
index ff8dea1f9..cca6e80c9 100644
--- a/torch/csrc/cuda/Module.cpp
+++ b/torch/csrc/cuda/Module.cpp
@@ -65,7 +65,6 @@ PyObject * THCPModule_getDeviceCount_wrap(PyObject *self)
   END_HANDLE_TH_ERRORS
 }
 
-
 PyObject * THCPModule_getCurrentStream_wrap(PyObject *self)
 {
   HANDLE_TH_ERRORS
@@ -109,7 +108,7 @@ PyObject * THCPModule_getDriverVersion(PyObject *self)
 
 PyObject * THCPModule_getCompiledVersion(PyObject *self)
 {
-  return PyLong_FromLong((long) CUDA_VERSION);
+  return PyLong_FromLong((long) 0);
 }
 
 PyObject * THCPModule_getRNGState(PyObject *_unused)
@@ -299,7 +298,9 @@ static void bindCudaDeviceProperties(PyObject* module) {
     .def_readonly("major", &cudaDeviceProp::major)
     .def_readonly("minor", &cudaDeviceProp::minor)
     .def_readonly("is_multi_gpu_board", &cudaDeviceProp::isMultiGpuBoard)
+#if defined(__NVCC__)
     .def_readonly("is_integrated", &cudaDeviceProp::integrated)
+#endif
     .def_readonly("multi_processor_count", &cudaDeviceProp::multiProcessorCount)
     .def_readonly("total_memory", &cudaDeviceProp::totalGlobalMem)
     .def("__repr__", [](const cudaDeviceProp &prop) {
diff --git a/torch/csrc/distributed/Module.cpp b/torch/csrc/distributed/Module.cpp
index a04fdf1bc..2c8b71d49 100644
--- a/torch/csrc/distributed/Module.cpp
+++ b/torch/csrc/distributed/Module.cpp
@@ -9,7 +9,7 @@
 #include "torch/csrc/PythonTypes.h"
 #include "torch/csrc/autograd/python_variable.h"
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "torch/csrc/cuda/Stream.h"
 #endif
 
@@ -82,7 +82,7 @@ static bool THDPModule_assignStateless(PyObject *self)
 static std::unordered_map<PyObject*, THDReduceOp> obj2reduceop;
 static std::unordered_map<PyObject*, THDGroup> obj2group;
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 extern THCState* state;
 #endif
 
@@ -109,7 +109,7 @@ PyObject* THDPModule_initProcessGroup(PyObject *_unused, PyObject *args)
     AutoNoGIL nogil;
     THDProcessGroupInit(channel_type, init_method, world_size, group_name, rank);
   }
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   THDSetCudaStatePtr(&state);
 #endif
   Py_RETURN_NONE;
@@ -149,14 +149,14 @@ PyObject* THDPModule_initMasterWorker(PyObject *_unused, PyObject *args)
     AutoNoGIL nogil;
     THDMasterWorkerInit(channel_type, init_method, world_size, group_name, rank);
   }
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   THDSetCudaStatePtr(&state);
 #endif
   Py_RETURN_NONE;
   END_HANDLE_TH_ERRORS
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 PyObject* THDPModule_registerStream(PyObject *_unused, PyObject *_stream)
 {
   HANDLE_TH_ERRORS
@@ -183,7 +183,7 @@ PyObject* THDPModule_getNumProcesses(PyObject *_unused)
   END_HANDLE_TH_ERRORS
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 extern PyObject* THCPDoubleTensorClass;
 extern PyObject* THCPFloatTensorClass;
 extern PyObject* THCPHalfTensorClass;
@@ -981,7 +981,7 @@ static struct PyMethodDef _THDPModule_methods[] = {
   {"_dist_destroy_process_group", (PyCFunction)THDPModule_destroyProcessGroup, METH_NOARGS, NULL},
   {"_dist_clear_group_cache", (PyCFunction)THDPModule_clearGroupCache, METH_VARARGS, NULL},
   {"_dist_init_master_worker", (PyCFunction)THDPModule_initMasterWorker, METH_VARARGS, NULL},
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   {"_dist_register_stream", (PyCFunction)THDPModule_registerStream, METH_O, NULL},
 #endif
   {"_dist_get_rank", (PyCFunction)THDPModule_getRank, METH_NOARGS, NULL},
diff --git a/torch/csrc/generic/StorageMethods.cpp b/torch/csrc/generic/StorageMethods.cpp
index 00c8c063c..99b34f792 100644
--- a/torch/csrc/generic/StorageMethods.cpp
+++ b/torch/csrc/generic/StorageMethods.cpp
@@ -1,4 +1,4 @@
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda_runtime.h>
 #endif
 
@@ -29,7 +29,7 @@ static PyObject * THPStorage_(copy_)(PyObject *self, PyObject *args, PyObject *k
 static PyObject * THPStorage_(isPinned)(THPStorage *self)
 {
   HANDLE_TH_ERRORS
-#if defined(WITH_CUDA)
+#if defined(WITH_ROCM)
   cudaPointerAttributes attr;
   cudaError_t err = cudaPointerGetAttributes(&attr, self->cdata->data);
   if (err != cudaSuccess) {
diff --git a/torch/csrc/generic/StorageSharing.cpp b/torch/csrc/generic/StorageSharing.cpp
index 5de9e7e4d..333bc803a 100644
--- a/torch/csrc/generic/StorageSharing.cpp
+++ b/torch/csrc/generic/StorageSharing.cpp
@@ -1,8 +1,14 @@
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda.h>
 #include <cuda_runtime.h>
 #endif
 
+#if defined(__HIP_PLATFORM_HCC__)
+  #undef PyBytes_AS_STRING(op)
+  #undef PyBytes_GET_SIZE(op)
+  #define PyBytes_AS_STRING(op) (((PyBytesObject *)(op))->ob_sval)
+  #define PyBytes_GET_SIZE(op)  Py_SIZE(op)
+#endif
 
 static PyObject * THPStorage_(sharedDecref)(THPStorage *self)
 {
@@ -253,6 +259,7 @@ static PyObject * THPStorage_(shareCuda)(THPStorage *self)
     THCudaCheck(cudaIpcGetMemHandle(&handle, base_ptr));
 
     _handle = PyBytes_FromStringAndSize((char *)&handle, CUDA_IPC_HANDLE_SIZE);
+
     _offset = PyLong_FromSsize_t((Py_ssize_t)offset);
     size = PyLong_FromSize_t(base_size / sizeof(real));
   }
diff --git a/torch/csrc/nn/type_checks.h b/torch/csrc/nn/type_checks.h
index b0463c309..a975342ef 100644
--- a/torch/csrc/nn/type_checks.h
+++ b/torch/csrc/nn/type_checks.h
@@ -68,7 +68,7 @@ static inline THIntTensor* THNN_IntTensor_Unpack(PyObject* obj) {
   return torch::nn::unpack<THIntTensor>(obj);
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 
 static inline bool THNN_CudaHalfTensor_Check(PyObject* obj) {
   return torch::nn::check_type(obj, at::TypeID::CUDAHalf);
@@ -102,4 +102,4 @@ static inline THCudaLongTensor* THNN_CudaLongTensor_Unpack(PyObject* obj) {
   return torch::nn::unpack<THCudaLongTensor>(obj);
 }
 
-#endif  // WITH_CUDA
+#endif  // WITH_ROCM
diff --git a/torch/csrc/utils.cpp b/torch/csrc/utils.cpp
index 48d04ac14..3517ecab1 100644
--- a/torch/csrc/utils.cpp
+++ b/torch/csrc/utils.cpp
@@ -17,7 +17,7 @@
 #include "generic/utils.cpp"
 #include <TH/THGenerateHalfType.h>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "torch/csrc/cuda/THCP.h"
 #endif
 
@@ -232,7 +232,7 @@ bool maybeThrowBackCompatKeepdimWarn(char *func) {
   return true;
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 std::vector <THCStream*> THPUtils_PySequence_to_THCStreamList(PyObject *obj) {
   if (!PySequence_Check(obj)) {
     throw std::runtime_error("Expected a sequence in THPUtils_PySequence_to_THCStreamList");
diff --git a/torch/csrc/utils.h b/torch/csrc/utils.h
index 5756954bc..b3d3b0cd6 100644
--- a/torch/csrc/utils.h
+++ b/torch/csrc/utils.h
@@ -9,7 +9,7 @@
 #include "torch/csrc/utils/python_numbers.h"
 #include "torch/csrc/utils/python_compat.h"
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #endif
 
@@ -177,7 +177,7 @@ void setBackCompatKeepdimWarn(bool warn);
 bool getBackCompatKeepdimWarn();
 bool maybeThrowBackCompatKeepdimWarn(char *func);
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 std::vector <THCStream*> THPUtils_PySequence_to_THCStreamList(PyObject *obj);
 #endif
 
diff --git a/torch/csrc/utils/auto_gpu.h b/torch/csrc/utils/auto_gpu.h
index 093709745..fa53c2f1b 100644
--- a/torch/csrc/utils/auto_gpu.h
+++ b/torch/csrc/utils/auto_gpu.h
@@ -7,7 +7,7 @@
 
 #include <ATen/ATen.h>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda.h>
 #include <cuda_runtime.h>
 #endif
@@ -29,7 +29,7 @@ struct AutoGPU {
   }
 
   ~AutoGPU() {
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     if (original_device != -1) {
       cudaSetDevice(original_device);
     }
@@ -37,7 +37,7 @@ struct AutoGPU {
   }
 
   inline void setDevice(int device) {
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     if (device == -1) {
       return;
     }
@@ -55,7 +55,7 @@ struct AutoGPU {
   int original_device = -1;
 
 private:
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   static void cudaCheck(cudaError_t err) {
     if (err != cudaSuccess) {
       std::string msg = "CUDA error (";
diff --git a/torch/csrc/utils/auto_stream.h b/torch/csrc/utils/auto_stream.h
index b18f7edcb..616313d20 100644
--- a/torch/csrc/utils/auto_stream.h
+++ b/torch/csrc/utils/auto_stream.h
@@ -2,13 +2,13 @@
 
 // RAII structs to set CUDA stream
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 extern THCState* state;
 #endif
 
 struct AutoStream {
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   explicit AutoStream(THCStream* stream)
     : original_stream(THCState_getStream(state))
   {
diff --git a/torch/csrc/utils/cuda_enabled.h b/torch/csrc/utils/cuda_enabled.h
index 5ec2aba3b..880b74c08 100644
--- a/torch/csrc/utils/cuda_enabled.h
+++ b/torch/csrc/utils/cuda_enabled.h
@@ -4,7 +4,7 @@ namespace torch {
 namespace utils {
 
 static inline bool cuda_enabled() {
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   return true;
 #else
   return false;
diff --git a/torch/csrc/utils/python_strings.h b/torch/csrc/utils/python_strings.h
index 36dce84e0..55737944d 100644
--- a/torch/csrc/utils/python_strings.h
+++ b/torch/csrc/utils/python_strings.h
@@ -5,6 +5,13 @@
 #include <string>
 #include "object_ptr.h"
 
+#if defined(__HIP_PLATFORM_HCC__)
+  #undef PyBytes_AS_STRING(op)
+  #undef PyBytes_GET_SIZE(op)
+  #define PyBytes_AS_STRING(op) (((PyBytesObject *)(op))->ob_sval)
+  #define PyBytes_GET_SIZE(op)  Py_SIZE(op)
+#endif
+
 // Utilities for handling Python strings. Note that PyString, when defined, is
 // the same as PyBytes.
 
diff --git a/torch/cuda/__init__.py b/torch/cuda/__init__.py
index f52ab04f1..4e3f63c4b 100644
--- a/torch/cuda/__init__.py
+++ b/torch/cuda/__init__.py
@@ -123,7 +123,7 @@ def _lazy_call(callable):
         # Don't store the actual traceback to avoid memory cycle
         _queued_calls.append((callable, traceback.format_stack()))
 
-_lazy_call(_check_capability)
+#_lazy_call(_check_capability)
 
 
 class DeferredCudaCallError(Exception):
@@ -159,9 +159,9 @@ def _lazy_init():
             "Cannot re-initialize CUDA in forked subprocess. " + msg)
     _check_driver()
     torch._C._cuda_init()
-    _cudart = _load_cudart()
-    _cudart.cudaGetErrorName.restype = ctypes.c_char_p
-    _cudart.cudaGetErrorString.restype = ctypes.c_char_p
+    # _cudart = _load_cudart()
+    #_cudart.cudaGetErrorName.restype = ctypes.c_char_p
+    #_cudart.cudaGetErrorString.restype = ctypes.c_char_p
     _original_pid = os.getpid()
     _initialized = True
     # Important to do this after _initialized, since some queued calls
diff --git a/torch/lib/THD/base/Cuda.cpp b/torch/lib/THD/base/Cuda.cpp
index 5e83aaffb..879240896 100644
--- a/torch/lib/THD/base/Cuda.cpp
+++ b/torch/lib/THD/base/Cuda.cpp
@@ -1,7 +1,7 @@
 #include "Cuda.hpp"
 #include <unordered_map>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 THCState** _THDCudaState;
 
 void THDSetCudaStatePtr(THCState **state) {
diff --git a/torch/lib/THD/base/Cuda.h b/torch/lib/THD/base/Cuda.h
index 1cc3fd5c3..3bd5469c2 100644
--- a/torch/lib/THD/base/Cuda.h
+++ b/torch/lib/THD/base/Cuda.h
@@ -1,6 +1,6 @@
 #pragma once
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "../THD.h"
 
 #include <THC/THC.h>
diff --git a/torch/lib/THD/base/Cuda.hpp b/torch/lib/THD/base/Cuda.hpp
index 260eea69f..3c7516ff8 100644
--- a/torch/lib/THD/base/Cuda.hpp
+++ b/torch/lib/THD/base/Cuda.hpp
@@ -1,6 +1,6 @@
 #pragma once
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #include "Cuda.h"
 
diff --git a/torch/lib/THD/base/DataChannel.cpp b/torch/lib/THD/base/DataChannel.cpp
index 2a6f783e1..993d91e34 100644
--- a/torch/lib/THD/base/DataChannel.cpp
+++ b/torch/lib/THD/base/DataChannel.cpp
@@ -5,7 +5,7 @@
 #ifdef WITH_MPI
 #include "data_channels/DataChannelMPI.hpp"
 #endif // WITH_MPI
-#if defined(WITH_CUDA) && defined(WITH_DISTRIBUTED_NCCL)
+#if defined(WITH_ROCM) && defined(WITH_DISTRIBUTED_NCCL)
 #include "data_channels/DataChannelNccl.hpp"
 #endif // WITH_DISTRIBUTED_NCCL
 #include "data_channels/DataChannelTCP.hpp"
@@ -43,7 +43,7 @@ DataChannel* DataChannel::newChannel(THDChannelType type, std::string init_metho
       );
 
     case THDChannelNccl:
-#if defined(WITH_CUDA) && defined(WITH_DISTRIBUTED_NCCL)
+#if defined(WITH_ROCM) && defined(WITH_DISTRIBUTED_NCCL)
       return new DataChannelNccl(GET_CONFIG);
 #endif
       throw std::runtime_error(
diff --git a/torch/lib/THD/base/TensorDescriptor.cpp b/torch/lib/THD/base/TensorDescriptor.cpp
index d87a65242..7be7937ff 100644
--- a/torch/lib/THD/base/TensorDescriptor.cpp
+++ b/torch/lib/THD/base/TensorDescriptor.cpp
@@ -29,7 +29,7 @@ THDTensorDescriptor THDTensorDescriptor_newFromTHByteTensor(THByteTensor *tensor
   return at::getType(at::Backend::CPU, at::ScalarType::Byte).unsafeTensorFromTH((void*)tensor, true);
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 
 THDTensorDescriptor THDTensorDescriptor_newFromTHCudaDoubleTensor(THCudaDoubleTensor *tensor) {
   return at::getType(at::Backend::CUDA, at::ScalarType::Double).unsafeTensorFromTH((void*)tensor, true);
diff --git a/torch/lib/THD/base/TensorDescriptor.h b/torch/lib/THD/base/TensorDescriptor.h
index 66ef02ca3..46b6badbd 100644
--- a/torch/lib/THD/base/TensorDescriptor.h
+++ b/torch/lib/THD/base/TensorDescriptor.h
@@ -2,7 +2,7 @@
 
 #include "../THD.h"
 #include <TH/TH.h>
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #endif
 
@@ -18,7 +18,7 @@ THDTensorDescriptor THDTensorDescriptor_newFromTHIntTensor(THIntTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHShortTensor(THShortTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHCharTensor(THCharTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHByteTensor(THByteTensor *tensor);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 THDTensorDescriptor THDTensorDescriptor_newFromTHCudaDoubleTensor(THCudaDoubleTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHCudaFloatTensor(THCudaTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHCudaHalfTensor(THCudaHalfTensor *tensor);
diff --git a/torch/lib/THD/base/data_channels/DataChannelMPI.cpp b/torch/lib/THD/base/data_channels/DataChannelMPI.cpp
index 56e300eb1..62fa6e7c7 100644
--- a/torch/lib/THD/base/data_channels/DataChannelMPI.cpp
+++ b/torch/lib/THD/base/data_channels/DataChannelMPI.cpp
@@ -10,7 +10,7 @@
 #include <unordered_map>
 #include <iostream>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda_runtime.h>
 #endif
 
@@ -140,7 +140,7 @@ rank_type DataChannelMPI::getNumProcesses() {
 struct AutoGPU {
   AutoGPU(int new_device) {
     if (new_device == -1) return;
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     cudaGetDevice(&device_);
     cudaSetDevice(new_device);
 #endif
@@ -148,7 +148,7 @@ struct AutoGPU {
 
   ~AutoGPU() {
     if (device_ == -1) return;
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     cudaSetDevice(device_);
 #endif
   }
diff --git a/torch/lib/THD/base/data_channels/GlooCache.hpp b/torch/lib/THD/base/data_channels/GlooCache.hpp
index eaed54aff..0f0577e68 100644
--- a/torch/lib/THD/base/data_channels/GlooCache.hpp
+++ b/torch/lib/THD/base/data_channels/GlooCache.hpp
@@ -9,7 +9,7 @@
 #include "gloo/allreduce_ring.h"
 #include "gloo/barrier_all_to_all.h"
 #include "gloo/broadcast_one_to_all.h"
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "gloo/cuda_allreduce_ring.h"
 #include "gloo/cuda_allreduce_halving_doubling.h"
 #include "gloo/cuda_allreduce_halving_doubling_pipelined.h"
@@ -19,7 +19,7 @@
 #include "gloo/rendezvous/store.h"
 #include "gloo/rendezvous/prefix_store.h"
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda.h>
 #include <THC/THC.h>
 #endif
@@ -141,7 +141,7 @@ struct GlooCache {
     if (device == DeviceType::CPU) {
       return std::shared_ptr<buffer_type>(new char[bytes],
                                           std::default_delete<char[]>());
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (device == DeviceType::CUDA) {
       buffer_type *buf;
       THCudaCheck(THCudaMalloc(THDGetCudaState(), (void**)&buf, bytes));
@@ -184,7 +184,7 @@ struct GlooCache {
 
     if (t_dev == DeviceType::CPU) {
       std::memcpy(input_buffer, t.data_ptr(), tensor_bytes);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (t_dev == DeviceType::CUDA) {
       auto stream = THCState_getCurrentStream(THDGetCudaState());
       THCudaCheck(cudaMemcpyAsync(input_buffer, t.data_ptr(), tensor_bytes,
@@ -202,7 +202,7 @@ struct GlooCache {
 
     if (t_dev == DeviceType::CPU) {
       std::memcpy(t.data_ptr(), output_buffer, tensor_bytes);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (t_dev == DeviceType::CUDA) {
       auto stream = THCState_getCurrentStream(THDGetCudaState());
       THCudaCheck(cudaMemcpyAsync(t.data_ptr(), output_buffer, tensor_bytes,
@@ -318,7 +318,7 @@ struct algorithm_spec<CollectiveType::ALL_REDUCE, T> {
         std::initializer_list<T*>{reinterpret_cast<T*>(input_buffer.get())},
         count,
         THDToGlooReduceOp<T>(op));
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (device == DeviceType::CUDA) {
       if (op != THDReduceSUM) {
         throw std::runtime_error("Gloo backend only supports sum op for CUDA all reduce");
@@ -388,7 +388,7 @@ struct algorithm_spec<CollectiveType::BROADCAST, T> {
         std::initializer_list<T*>{reinterpret_cast<T*>(input_buffer.get())},
         count,
         src_rank);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (device == DeviceType::CUDA) {
       auto stream = THCState_getCurrentStream(THDGetCudaState());
 
