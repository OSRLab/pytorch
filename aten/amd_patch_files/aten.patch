diff --git a/CMakeLists.txt.hip b/CMakeLists.txt.hip
new file mode 100644
index 000000000..474ed003c
--- /dev/null
+++ b/CMakeLists.txt.hip
@@ -0,0 +1,542 @@
+cmake_minimum_required(VERSION 3.0)
+# HIP_PATH
+IF(NOT DEFINED $ENV{HIP_PATH})
+  SET(HIP_PATH /opt/rocm/hip)
+ELSE()
+  SET(HIP_PATH $ENV{HIP_PATH})
+ENDIF()
+
+# HCC_PATH
+IF(NOT DEFINED $ENV{HCC_PATH})
+  SET(HCC_PATH /opt/rocm/hcc)
+ELSE()
+  SET(HCC_PATH $ENV{HCC_PATH})
+ENDIF()
+
+# HIPBLAS_PATH
+IF(NOT DEFINED $ENV{HIPBLAS_PATH})
+  SET(HIPBLAS_PATH /opt/rocm/hipblas)
+ELSE()
+  SET(HIPBLAS_PATH $ENV{HIPBLAS_PATH})
+ENDIF()
+
+# HIPRNG_PATH
+IF(NOT DEFINED $ENV{HIPRNG_PATH})
+  SET(HIPRNG_PATH /opt/rocm/hcrng)
+ELSE()
+  SET(HIPRNG_PATH $ENV{HIPRNG_PATH})
+ENDIF()
+
+# HIPSPARSE_PATH
+IF(NOT DEFINED $ENV{HIPSPARSE_PATH})
+  SET(HIPSPARSE_PATH /opt/rocm/hcsparse)
+ELSE()
+  SET(HIPSPARSE_PATH $ENV{HIPSPARSE_PATH})
+ENDIF()
+
+SET(THRUST_PATH "/root/Thrust")
+
+set(CMAKE_MODULE_PATH
+  ${CMAKE_CURRENT_SOURCE_DIR}/cmake
+  ${CMAKE_CURRENT_SOURCE_DIR}/cmake/FindCUDA
+  /usr/lib/x86_64-linux-gnu/
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/TH/cmake
+  ${HIP_PATH}/cmake
+  ${CMAKE_MODULE_PATH})
+SET(CMAKE_LIBRARY_PATH /usr/lib/x86_64-linux-gnu/ ${CMAKE_LIBRARY_PATH})
+project(ATen)
+
+cmake_policy(SET CMP0012 NEW)
+
+# ADD_DEFINITIONS(-D__HIP_PLATFORM_HCC__)
+
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+
+# RPATH stuff
+# see https://cmake.org/Wiki/CMake_RPATH_handling
+if(APPLE)
+  set(CMAKE_MACOSX_RPATH ON)
+endif()
+set(CMAKE_SKIP_BUILD_RPATH  FALSE)
+set(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE)
+set(CMAKE_INSTALL_RPATH "${CMAKE_INSTALL_PREFIX}/lib")
+set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
+set(CMAKE_POSITION_INDEPENDENT_CODE TRUE)
+list(FIND CMAKE_PLATFORM_IMPLICIT_LINK_DIRECTORIES "${CMAKE_INSTALL_PREFIX}/lib" isSystemDir)
+if("${isSystemDir}" STREQUAL "-1")
+  set(CMAKE_INSTALL_RPATH "${CMAKE_INSTALL_PREFIX}/lib")
+endif()
+
+IF(NOT MSVC)
+  set(CMAKE_CXX_FLAGS "--std=c++11 -Wall -Wno-unknown-pragmas -Wno-vla -fexceptions ${CMAKE_CXX_FLAGS}")
+  set(CMAKE_C_FLAGS "-fexceptions ${CMAKE_C_FLAGS}")
+ENDIF(NOT MSVC)
+
+INCLUDE(CheckCXXSourceCompiles)
+
+# windef.h will define max/min macros if NOMINMAX is not defined
+IF(MSVC)
+  add_definitions(/DNOMINMAX)
+ENDIF(MSVC)
+
+#Check if certain std functions are supported. Sometimes
+#_GLIBCXX_USE_C99 macro is not defined and some functions are missing.
+CHECK_CXX_SOURCE_COMPILES("
+#include <cmath>
+#include <string>
+
+int main() {
+  int a = std::isinf(3.0);
+  int b = std::isnan(0.0);
+  std::string s = std::to_string(1);
+
+  return 0;
+  }" SUPPORT_GLIBCXX_USE_C99)
+
+if(NOT SUPPORT_GLIBCXX_USE_C99)
+  message(FATAL_ERROR
+          "The C++ compiler does not support required functions. "
+          "This is very likely due to a known bug in GCC 5 "
+          "(and maybe other versions) on Ubuntu 17.10 and newer. "
+          "For more information, see: "
+          "https://github.com/pytorch/pytorch/issues/5229"
+         )
+endif()
+
+# Top-level build config
+############################################
+# Flags
+# When using MSVC
+
+#Â Detect CUDA architecture and get best NVCC flags
+# finding cuda must be first because other things depend on the result
+IF(NOT CUDA_FOUND)
+  FIND_PACKAGE(CUDA 5.5)
+ENDIF()
+
+IF(MSVC)
+  # we want to respect the standard, and we are bored of those **** .
+  ADD_DEFINITIONS(-D_CRT_SECURE_NO_DEPRECATE=1)
+  LIST(APPEND CUDA_NVCC_FLAGS "-Xcompiler /wd4819 -Xcompiler /wd4503")
+  ADD_DEFINITIONS(-DTH_EXPORTS)
+  IF (WITH_ROCM)
+    ADD_DEFINITIONS(-DTHC_EXPORTS)
+  ENDIF()
+ENDIF(MSVC)
+
+IF (NOT MSVC)
+  IF (CMAKE_VERSION VERSION_LESS "3.1")
+    #SET(CMAKE_C_FLAGS "-std=c11 ${CMAKE_C_FLAGS}")
+  ELSE ()
+    #SET(CMAKE_C_STANDARD 11)
+  ENDIF ()
+ENDIF(NOT MSVC)
+
+if(CMAKE_CXX_COMPILER_ID STREQUAL "GNU")
+  if(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER "4.9")
+    if(CUDA_VERSION VERSION_LESS "8.0")
+      MESSAGE(STATUS "Found gcc >=5 and CUDA <= 7.5, adding workaround C++ flags")
+      set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -D_FORCE_INLINES -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__")
+    endif(CUDA_VERSION VERSION_LESS "8.0")
+  endif(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER "4.9")
+endif(CMAKE_CXX_COMPILER_ID STREQUAL "GNU")
+
+LIST(APPEND CUDA_NVCC_FLAGS -Wno-deprecated-gpu-targets)
+LIST(APPEND CUDA_NVCC_FLAGS --expt-extended-lambda)
+
+if(NOT CMAKE_CXX_COMPILER_ID STREQUAL "GNU")
+  SET(CMAKE_CXX_STANDARD 11)
+endif()
+
+IF(NOT COMMAND CUDA_SELECT_NVCC_ARCH_FLAGS)
+  INCLUDE(${CMAKE_CURRENT_SOURCE_DIR}/cmake/select_compute_arch.cmake)
+ENDIF()
+LIST(APPEND CUDA_NVCC_FLAGS $ENV{TORCH_NVCC_FLAGS})
+CUDA_SELECT_NVCC_ARCH_FLAGS(NVCC_FLAGS_EXTRA $ENV{TORCH_CUDA_ARCH_LIST})
+LIST(APPEND CUDA_NVCC_FLAGS ${NVCC_FLAGS_EXTRA})
+IF(CMAKE_POSITION_INDEPENDENT_CODE AND NOT MSVC)
+  LIST(APPEND CUDA_NVCC_FLAGS "-Xcompiler -fPIC")
+ENDIF()
+
+IF(CUDA_HAS_FP16 OR NOT ${CUDA_VERSION} LESS 7.5)
+  MESSAGE(STATUS "Found CUDA with FP16 support, compiling with torch.CudaHalfTensor")
+  LIST(APPEND CUDA_NVCC_FLAGS "-DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__")
+  SET(CMAKE_C_FLAGS "-DCUDA_HAS_FP16=1 ${CMAKE_C_FLAGS}")
+ELSE(CUDA_HAS_FP16 OR NOT ${CUDA_VERSION} LESS 7.5)
+  MESSAGE(STATUS "Could not find CUDA with FP16 support, compiling without torch.CudaHalfTensor")
+ENDIF(CUDA_HAS_FP16 OR NOT ${CUDA_VERSION} LESS 7.5)
+
+OPTION(NDEBUG "disable asserts (WARNING: this may result in silent UB e.g. with out-of-bound indices)")
+IF(NOT NDEBUG)
+  MESSAGE(STATUS "Removing -DNDEBUG from compile flags")
+  STRING(REPLACE "-DNDEBUG" "" CMAKE_C_FLAGS "" ${CMAKE_C_FLAGS})
+  STRING(REPLACE "-DNDEBUG" "" CMAKE_C_FLAGS_DEBUG "" ${CMAKE_C_FLAGS_DEBUG})
+  STRING(REPLACE "-DNDEBUG" "" CMAKE_C_FLAGS_RELEASE "" ${CMAKE_C_FLAGS_RELEASE})
+  STRING(REPLACE "-DNDEBUG" "" CMAKE_CXX_FLAGS "" ${CMAKE_CXX_FLAGS})
+  STRING(REPLACE "-DNDEBUG" "" CMAKE_CXX_FLAGS_DEBUG "" ${CMAKE_CXX_FLAGS_DEBUG})
+  STRING(REPLACE "-DNDEBUG" "" CMAKE_CXX_FLAGS_RELEASE "" ${CMAKE_CXX_FLAGS_RELEASE})
+ENDIF()
+
+# OpenMP support?
+SET(WITH_OPENMP ON CACHE BOOL "OpenMP support if available?")
+IF (APPLE AND CMAKE_COMPILER_IS_GNUCC)
+  EXEC_PROGRAM (uname ARGS -v  OUTPUT_VARIABLE DARWIN_VERSION)
+  STRING (REGEX MATCH "[0-9]+" DARWIN_VERSION ${DARWIN_VERSION})
+  MESSAGE (STATUS "MAC OS Darwin Version: ${DARWIN_VERSION}")
+  IF (DARWIN_VERSION GREATER 9)
+    SET(APPLE_OPENMP_SUCKS 1)
+  ENDIF (DARWIN_VERSION GREATER 9)
+  EXECUTE_PROCESS (COMMAND ${CMAKE_C_COMPILER} -dumpversion
+    OUTPUT_VARIABLE GCC_VERSION)
+  IF (APPLE_OPENMP_SUCKS AND GCC_VERSION VERSION_LESS 4.6.2)
+    MESSAGE(STATUS "Warning: Disabling OpenMP (unstable with this version of GCC)")
+    MESSAGE(STATUS " Install GCC >= 4.6.2 or change your OS to enable OpenMP")
+    SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -Wno-unknown-pragmas")
+    SET(WITH_OPENMP OFF CACHE BOOL "OpenMP support if available?" FORCE)
+  ENDIF ()
+ENDIF ()
+
+IF (WITH_OPENMP AND NOT CHECKED_OPENMP)
+  FIND_PACKAGE(OpenMP)
+  SET(CHECKED_OPENMP ON CACHE BOOL "already checked for OpenMP")
+
+  # OPENMP_FOUND is not cached in FindOpenMP.cmake (all other variables are cached)
+  # see https://github.com/Kitware/CMake/blob/master/Modules/FindOpenMP.cmake
+  SET(OPENMP_FOUND ${OPENMP_FOUND} CACHE BOOL "OpenMP Support found")
+ENDIF (WITH_OPENMP AND NOT CHECKED_OPENMP)
+
+IF(OPENMP_FOUND)
+  MESSAGE(STATUS "Compiling with OpenMP support")
+  SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}")
+  SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}")
+ENDIF(OPENMP_FOUND)
+
+
+SET(CUDA_ATTACH_VS_BUILD_RULE_TO_CUDA_FILE OFF)
+
+FIND_PACKAGE(MAGMA)
+IF(CUDA_FOUND AND MAGMA_FOUND)
+  INCLUDE_DIRECTORIES("${MAGMA_INCLUDE_DIR}")
+  SET(CMAKE_REQUIRED_INCLUDES "${MAGMA_INCLUDE_DIR};${CUDA_INCLUDE_DIRS}")
+  INCLUDE(CheckPrototypeDefinition)
+  check_prototype_definition(magma_get_sgeqrf_nb
+   "magma_int_t magma_get_sgeqrf_nb( magma_int_t m, magma_int_t n );"
+   "0"
+   "magma.h"
+    MAGMA_V2)
+  IF (MAGMA_V2)
+    add_definitions(-DMAGMA_V2)
+  ENDIF (MAGMA_V2)
+
+  SET(USE_MAGMA 1)
+  MESSAGE(STATUS "Compiling with MAGMA support")
+  MESSAGE(STATUS "MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}")
+  MESSAGE(STATUS "MAGMA LIBRARIES: ${MAGMA_LIBRARIES}")
+  MESSAGE(STATUS "MAGMA V2 check: ${MAGMA_V2}")
+ELSE()
+  MESSAGE(STATUS "MAGMA not found. Compiling without MAGMA support")
+ENDIF()
+
+# ARM specific flags
+FIND_PACKAGE(ARM)
+IF (ASIMD_FOUND)
+  MESSAGE(STATUS "asimd/Neon found with compiler flag : -D__NEON__")
+  SET(CMAKE_C_FLAGS "-D__NEON__ ${CMAKE_C_FLAGS}")
+ELSEIF (NEON_FOUND)
+  MESSAGE(STATUS "Neon found with compiler flag : -mfpu=neon -D__NEON__")
+  SET(CMAKE_C_FLAGS "-mfpu=neon -D__NEON__ ${CMAKE_C_FLAGS}")
+ENDIF (ASIMD_FOUND)
+IF (CORTEXA8_FOUND)
+  MESSAGE(STATUS "Cortex-A8 Found with compiler flag : -mcpu=cortex-a8")
+  SET(CMAKE_C_FLAGS "-mcpu=cortex-a8 -fprefetch-loop-arrays ${CMAKE_C_FLAGS}")
+ENDIF (CORTEXA8_FOUND)
+IF (CORTEXA9_FOUND)
+  MESSAGE(STATUS "Cortex-A9 Found with compiler flag : -mcpu=cortex-a9")
+  SET(CMAKE_C_FLAGS "-mcpu=cortex-a9 ${CMAKE_C_FLAGS}")
+ENDIF (CORTEXA9_FOUND)
+
+IF(UNIX)
+  # prevent Unknown CMake command "check_function_exists".
+  INCLUDE(CheckFunctionExists)
+ENDIF(UNIX)
+
+INCLUDE (CheckIncludeFile)
+INCLUDE (CheckCSourceCompiles)
+INCLUDE (CheckCSourceRuns)
+
+# Check that our programs run.  This is different from the native CMake compiler
+# check, which just tests if the program compiles and links.  This is important
+# because with ASAN you might need to help the compiled library find some
+# dynamic libraries.
+CHECK_C_SOURCE_RUNS("
+int main() { return 0; }
+" COMPILER_WORKS)
+IF(NOT COMPILER_WORKS)
+  # Force cmake to retest next time around
+  unset(COMPILER_WORKS CACHE)
+  MESSAGE(FATAL_ERROR
+      "Could not run a simple program built with your compiler. "
+      "If you are trying to use -fsanitize=address, make sure "
+      "libasan is properly installed on your system (you can confirm "
+      "if the problem is this by attempting to build and run a "
+      "small program.)")
+ENDIF()
+
+CHECK_INCLUDE_FILE(cpuid.h HAVE_CPUID_H)
+# Check for a cpuid intrinsic
+IF(HAVE_CPUID_H)
+    CHECK_C_SOURCE_COMPILES("#include <cpuid.h>
+        int main()
+        {
+            unsigned int eax, ebx, ecx, edx;
+            return __get_cpuid(0, &eax, &ebx, &ecx, &edx);
+        }" HAVE_GCC_GET_CPUID)
+ENDIF()
+IF(HAVE_GCC_GET_CPUID)
+  SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -DHAVE_GCC_GET_CPUID")
+ENDIF(HAVE_GCC_GET_CPUID)
+
+CHECK_C_SOURCE_COMPILES("#include <stdint.h>
+    static inline void cpuid(uint32_t *eax, uint32_t *ebx,
+    			 uint32_t *ecx, uint32_t *edx)
+    {
+      uint32_t a = *eax, b, c = *ecx, d;
+      asm volatile ( \"cpuid\" : \"+a\"(a), \"=b\"(b), \"+c\"(c), \"=d\"(d) );
+      *eax = a; *ebx = b; *ecx = c; *edx = d;
+    }
+    int main() {
+      uint32_t a,b,c,d;
+      cpuid(&a, &b, &c, &d);
+      return 0;
+    }" NO_GCC_EBX_FPIC_BUG)
+
+IF(NOT NO_GCC_EBX_FPIC_BUG)
+  SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -DUSE_GCC_GET_CPUID")
+ENDIF(NOT NO_GCC_EBX_FPIC_BUG)
+
+FIND_PACKAGE(SSE) # checks SSE, AVX and AVX2
+IF(C_SSE2_FOUND)
+  MESSAGE(STATUS "SSE2 Found")
+  SET(CMAKE_C_FLAGS "${C_SSE2_FLAGS} -DUSE_SSE2 ${CMAKE_C_FLAGS}")
+ENDIF(C_SSE2_FOUND)
+IF(C_SSE4_1_FOUND AND C_SSE4_2_FOUND)
+  SET(CMAKE_C_FLAGS "${C_SSE4_1_FLAGS} -DUSE_SSE4_1 ${C_SSE4_2_FLAGS} -DUSE_SSE4_2 ${CMAKE_C_FLAGS}")
+ENDIF()
+IF(C_SSE3_FOUND)
+  MESSAGE(STATUS "SSE3 Found")
+  #SET(CMAKE_C_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_C_FLAGS}")
+  #SET(CMAKE_CXX_FLAGS "${C_SSE3_FLAGS} -DUSE_SSE3 ${CMAKE_CXX_FLAGS}")
+ENDIF(C_SSE3_FOUND)
+
+# we don't set -mavx and -mavx2 flags globally, but only for specific files
+# however, we want to enable the AVX codepaths, so we still need to
+# add USE_AVX and USE_AVX2 macro defines
+IF(C_AVX_FOUND)
+  MESSAGE(STATUS "AVX Found")
+  SET(CMAKE_C_FLAGS "-DUSE_AVX ${CMAKE_C_FLAGS}")
+ENDIF(C_AVX_FOUND)
+IF(C_AVX2_FOUND)
+  MESSAGE(STATUS "AVX2 Found")
+  SET(CMAKE_C_FLAGS "-DUSE_AVX2 ${CMAKE_C_FLAGS}")
+  SET(CMAKE_CXX_FLAGS "-DUSE_AVX2 ${CMAKE_CXX_FLAGS}")
+ENDIF(C_AVX2_FOUND)
+
+CHECK_C_SOURCE_RUNS("
+#include <stdatomic.h>
+// ATOMIC_INT_LOCK_FREE is flaky on some older gcc versions
+// so if this define is not usable a preprocessor definition
+// we fail this check and fall back to GCC atomics
+#if ATOMIC_INT_LOCK_FREE == 2
+#define TH_ATOMIC_IPC_REFCOUNT 1
+#endif
+int main()
+{
+  int a;
+  int oa;
+  atomic_store(&a, 1);
+  atomic_fetch_add(&a, 1);
+  oa = atomic_load(&a);
+  if(!atomic_compare_exchange_strong(&a, &oa, 3))
+    return -1;
+  return 0;
+}
+" HAS_C11_ATOMICS)
+
+# For ROCm stack, the hcc/hipcc compiler cannot use C11 atomics. Prefer GCC.
+SET(HAS_C11_ATOMICS OFF)
+
+IF(NOT HAS_C11_ATOMICS)
+  CHECK_C_SOURCE_RUNS("
+#include <intrin.h>
+int main()
+{
+  long a;
+  _InterlockedExchange(&a, 1);
+  _InterlockedExchangeAdd(&a, 1);
+  if(_InterlockedCompareExchange(&a, 3, 2) != 2)
+    return -1;
+  return 0;
+}
+" HAS_MSC_ATOMICS)
+
+  CHECK_C_SOURCE_RUNS("
+int main()
+{
+  int a;
+  __sync_lock_test_and_set(&a, 1);
+  __sync_fetch_and_add(&a, 1);
+  if(!__sync_bool_compare_and_swap(&a, 2, 3))
+    return -1;
+  return 0;
+}
+" HAS_GCC_ATOMICS)
+ENDIF()
+
+IF(HAS_C11_ATOMICS)
+  ADD_DEFINITIONS(-DUSE_C11_ATOMICS=1)
+  MESSAGE(STATUS "Atomics: using C11 intrinsics")
+ELSEIF(HAS_MSC_ATOMICS)
+  ADD_DEFINITIONS(-DUSE_MSC_ATOMICS=1)
+  MESSAGE(STATUS "Atomics: using MSVC intrinsics")
+ELSEIF(HAS_GCC_ATOMICS)
+  ADD_DEFINITIONS(-DUSE_GCC_ATOMICS=1)
+    MESSAGE(STATUS "Atomics: using GCC intrinsics")
+ELSE()
+  SET(CMAKE_THREAD_PREFER_PTHREAD TRUE)
+  FIND_PACKAGE(Threads)
+  IF(THREADS_FOUND)
+    ADD_DEFINITIONS(-DUSE_PTHREAD_ATOMICS=1)
+    TARGET_LINK_LIBRARIES(TH ${CMAKE_THREAD_LIBS_INIT})
+    MESSAGE(STATUS "Atomics: using pthread")
+  ENDIF()
+ENDIF()
+
+IF (WIN32 AND NOT CYGWIN)
+  SET(BLAS_INSTALL_LIBRARIES "OFF"
+    CACHE BOOL "Copy the required BLAS DLLs into the TH install dirs")
+ENDIF (WIN32 AND NOT CYGWIN)
+
+MACRO(Install_Required_Library ln)
+    get_filename_component(libpath ${ln} PATH)
+    get_filename_component(libname ${ln} NAME_WE)
+    file(GLOB libdlls "${libpath}/${libname}*.dll")
+    install(PROGRAMS ${libdlls}
+      DESTINATION "${TH_INSTALL_BIN_SUBDIR}")
+ENDMACRO(Install_Required_Library libname)
+
+FIND_PACKAGE(BLAS)
+SET(AT_MKL_ENABLED 0)
+IF(BLAS_FOUND)
+  SET(USE_BLAS 1)
+  IF(BLAS_INFO STREQUAL "mkl")
+    ADD_DEFINITIONS(-DTH_BLAS_MKL)
+    INCLUDE_DIRECTORIES(${BLAS_INCLUDE_DIR})  # include MKL headers
+    SET(AT_MKL_ENABLED 1)
+  ENDIF()
+ENDIF(BLAS_FOUND)
+
+FIND_PACKAGE(LAPACK)
+IF(LAPACK_FOUND)
+  SET(USE_LAPACK 1)
+ENDIF(LAPACK_FOUND)
+
+#############################################
+
+set(ATen_CPU_SRCS)
+set(ATen_CPU_INCLUDE)
+set(ATen_CUDA_SRCS)
+set(ATen_CUDA_INCLUDE)
+SET(ATEN_INSTALL_BIN_SUBDIR "bin" CACHE PATH "ATen install binary subdirectory")
+SET(ATEN_INSTALL_LIB_SUBDIR "lib" CACHE PATH "ATen install library subdirectory")
+SET(ATEN_INSTALL_INCLUDE_SUBDIR "include" CACHE PATH "ATen install include subdirectory")
+
+add_definitions(-DTH_INDEX_BASE=0)
+set(TH_LINK_STYLE STATIC)
+add_subdirectory(hip-src/TH)
+include_directories(
+  # dense
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/TH
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/THC
+  ${CMAKE_CURRENT_BINARY_DIR}/hip-src/TH
+  ${CMAKE_CURRENT_BINARY_DIR}/hip-src/THC
+  # sparse
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/THS
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/THCS
+  ${CMAKE_CURRENT_BINARY_DIR}/hip-src/THS
+  ${CMAKE_CURRENT_BINARY_DIR}/hip-src/THCS
+
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src
+  ${CMAKE_CURRENT_BINARY_DIR}/hip-src)
+add_subdirectory(hip-src/THNN)
+add_subdirectory(hip-src/THS)
+
+if (WITH_ROCM)
+  SET(AT_CUDA_ENABLED 1)
+  add_subdirectory(hip-src/THC)
+  add_subdirectory(hip-src/THCUNN)
+  add_subdirectory(hip-src/THCS)
+elseif(NOT NO_CUDA)
+  SET(AT_CUDA_ENABLED 1)
+  INCLUDE_DIRECTORIES(${CUDA_INCLUDE_DIRS})
+  find_package(CUDA 5.5 REQUIRED)
+  add_subdirectory(hip-src/THC)
+  add_subdirectory(hip-src/THCUNN)
+  add_subdirectory(hip-src/THCS)
+else()
+  message("disabling CUDA because NO_CUDA is set")
+  SET(CUDA_FLAG -n)
+  SET(AT_CUDA_ENABLED 0)
+endif()
+
+find_package(CuDNN)
+IF(NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND)
+  MESSAGE(STATUS "CuDNN not found. Compiling without CuDNN support")
+  set(AT_CUDNN_ENABLED 0)
+ELSE()
+  INCLUDE_DIRECTORIES(BEFORE ${CUDNN_INCLUDE_DIRS})
+  set(AT_CUDNN_ENABLED 1)
+ENDIF()
+
+if(NO_NNPACK)
+  message("disabling NNPACK because NO_NNPACK is set")
+  set(AT_NNPACK_ENABLED 0)
+else()
+  find_package(NNPACK)
+  if(NOT NNPACK_FOUND)
+    MESSAGE(STATUS "NNPACK not found. Compiling without NNPACK support")
+    set(AT_NNPACK_ENABLED 0)
+  ELSE()
+    INCLUDE_DIRECTORIES(${NNPACK_INCLUDE_DIRS})
+    set(AT_NNPACK_ENABLED 1)
+  ENDIF()
+endif()
+
+set(cwrap_files
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/ATen/Declarations.cwrap
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/THNN/generic/THNN.h
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/THCUNN/generic/THCUNN.h
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/ATen/nn.yaml
+  ${CMAKE_CURRENT_SOURCE_DIR}/hip-src/ATen/native/native_functions.yaml
+)
+
+include_directories(
+${CMAKE_CURRENT_SOURCE_DIR}/hip-src/THNN
+${CMAKE_CURRENT_SOURCE_DIR}/hip-src/THCUNN)
+
+add_subdirectory(hip-src/ATen)
+include_directories(
+${CMAKE_CURRENT_SOURCE_DIR}/hip-src
+${CMAKE_CURRENT_SOURCE_DIR}/hip-src/ATen/utils/catch/single_include
+${CMAKE_CURRENT_BINARY_DIR}/hip-src/ATen)
+if(NOT NO_CUDA AND NOT WITH_ROCM)
+  include_directories(${CUDA_INCLUDE_DIRS})
+endif()
+
+#add_subdirectory(hip-src/ATen/test)
+
+if(ATEN_NO_CONTRIB)
+  message("disable contrib because ATEN_NO_CONTRIB is set")
+else()
+  add_subdirectory(contrib/data)
+  add_subdirectory(contrib/meter)
+endif()
diff --git a/src/ATen/ATen.h b/src/ATen/ATen.h
index e41c2d956..84b0d3f57 100644
--- a/src/ATen/ATen.h
+++ b/src/ATen/ATen.h
@@ -1,5 +1,9 @@
 #pragma once
 
+#if defined(__HCC__)
+#include "hip/hip_runtime.h"
+#endif
+
 #include "ATen/ATenGeneral.h"
 #include "ATen/CPUGeneral.h"
 #include "ATen/Allocator.h"
diff --git a/src/ATen/CMakeLists.txt.hip b/src/ATen/CMakeLists.txt.hip
new file mode 100644
index 000000000..3824b77b7
--- /dev/null
+++ b/src/ATen/CMakeLists.txt.hip
@@ -0,0 +1,374 @@
+CMAKE_MINIMUM_REQUIRED(VERSION 2.8)
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+# HIP_PATH
+IF(NOT DEFINED $ENV{HIP_PATH})
+  SET(HIP_PATH /opt/rocm/hip)
+ELSE()
+  SET(HIP_PATH $ENV{HIP_PATH})
+ENDIF()
+
+# HCC_PATH
+IF(NOT DEFINED $ENV{HCC_PATH})
+  SET(HCC_PATH /opt/rocm/hcc)
+ELSE()
+  SET(HCC_PATH $ENV{HCC_PATH})
+ENDIF()
+
+# HIPBLAS_PATH
+IF(NOT DEFINED $ENV{HIPBLAS_PATH})
+  SET(HIPBLAS_PATH /opt/rocm/hipblas)
+ELSE()
+  SET(HIPBLAS_PATH $ENV{HIPBLAS_PATH})
+ENDIF()
+
+# HIPRNG_PATH
+IF(NOT DEFINED $ENV{HIPRNG_PATH})
+  SET(HIPRNG_PATH /opt/rocm/hcrng)
+ELSE()
+  SET(HIPRNG_PATH $ENV{HIPRNG_PATH})
+ENDIF()
+
+# HIPSPARSE_PATH
+IF(NOT DEFINED $ENV{HIPSPARSE_PATH})
+
+  SET(HIPSPARSE_PATH /opt/rocm/hcsparse)
+ELSE()
+  SET(HIPSPARSE_PATH $ENV{HIPSPARSE_PATH})
+ENDIF()
+
+SET(THRUST_PATH "~/Thrust")
+INCLUDE_DIRECTORIES(${HIP_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPBLAS_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPSPARSE_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPRNG_PATH}/include)
+INCLUDE_DIRECTORIES(${THRUST_PATH})
+
+# load HIP cmake module and load platform id
+# SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH} "${HIP_PATH}/cmake")
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig -P OUTPUT_VARIABLE PLATFORM)
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig --cpp_config OUTPUT_VARIABLE HIP_CXX_FLAGS)
+
+SET(CMAKE_C_COMPILER gcc)
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+
+# SET(CMAKE_C_FLAGS "-std=c99 -Werror=implicit-function-declaration ${CMAKE_C_FLAGS}")
+# SET(CMAKE_C_FLAGS "${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+SET(CMAKE_CXX_FLAGS  "-std=c++11 ${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+
+# Set the Flags for hcc & hipcc
+
+# Show message that we're using ROCm.
+MESSAGE(STATUS "ROCM TRUE:")
+MESSAGE(STATUS "CMAKE_CXX_COMPILER: " ${CMAKE_CXX_COMPILER})
+
+# avoid some cmake warnings
+IF(POLICY CMP0026)
+ CMAKE_POLICY(SET CMP0026 OLD)
+ENDIF()
+
+IF(MSVC AND NOT "${CMAKE_BUILD_TYPE}" MATCHES "Debug")
+  SET(MSVC_OPT_FLAG "/Ox ")
+ELSE()
+  SET(MSVC_OPT_FLAG "")
+ENDIF()
+
+########################
+# SET_SOURCE_FILES_PROPERTIES must be in the same CMakeLists.txt file as the target that includes the file
+# so we need to set these commands here rather than in src/TH
+IF(C_SSE4_1_FOUND AND C_SSE4_2_FOUND)
+  IF(MSVC)
+    SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/generic/simd/convolve5x5_sse.c PROPERTIES COMPILE_FLAGS "${MSVC_OPT_FLAG}/fp:fast")
+  ELSE(MSVC)
+    SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/generic/simd/convolve5x5_sse.c PROPERTIES COMPILE_FLAGS "-O3 -ffast-math")
+  ENDIF(MSVC)
+ENDIF(C_SSE4_1_FOUND AND C_SSE4_2_FOUND)
+IF(C_AVX_FOUND)
+  IF(MSVC)
+    SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/generic/simd/convolve5x5_avx.c PROPERTIES COMPILE_FLAGS "${MSVC_OPT_FLAG}/fp:fast ${C_AVX_FLAGS}")
+    SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/vector/AVX.c PROPERTIES COMPILE_FLAGS "${MSVC_OPT_FLAG}/arch:AVX ${C_AVX_FLAGS}")
+  ELSE(MSVC)
+    SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/generic/simd/convolve5x5_avx.c PROPERTIES COMPILE_FLAGS "-O3 -ffast-math ${C_AVX_FLAGS}")
+    SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/vector/AVX.c PROPERTIES COMPILE_FLAGS "-O3 ${C_AVX_FLAGS}")
+  ENDIF(MSVC)
+ENDIF(C_AVX_FOUND)
+
+IF(C_AVX2_FOUND)
+  IF(MSVC)
+    SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/vector/AVX2.cpp PROPERTIES COMPILE_FLAGS "${MSVC_OPT_FLAG}/arch:AVX2 ${C_AVX2_FLAGS}")
+  ELSE(MSVC)
+    SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/vector/AVX2.cpp PROPERTIES COMPILE_FLAGS "-O3 ${C_AVX2_FLAGS}")
+  ENDIF(MSVC)
+ENDIF(C_AVX2_FOUND)
+
+IF(NOT MSVC AND NOT "${CMAKE_C_COMPILER_ID}" MATCHES "Clang")
+  SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/THAtomic.c PROPERTIES COMPILE_FLAGS "-fno-openmp")
+  SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/THAllocator.c PROPERTIES COMPILE_FLAGS "-fno-openmp")
+ENDIF()
+########################
+
+SET_SOURCE_FILES_PROPERTIES(${PROJECT_SOURCE_DIR}/hip-src/TH/THTensor.cpp PROPERTIES COMPILE_FLAGS "")
+
+################################################################################
+# Helper functions
+################################################################################
+
+FUNCTION(EXCLUDE_DIR list_name dir_name)
+  # A helper that excludes all files that contain dir_name in their file path
+  SET(local_list ${${list_name}})
+  FOREACH(source ${local_list})
+    IF(${source} MATCHES ${dir_name})
+      MESSAGE(STATUS "Excluding " ${source} " from the build")
+      LIST(REMOVE_ITEM local_list ${source})
+    ENDIF()
+  ENDFOREACH()
+  SET(${list_name} ${local_list} PARENT_SCOPE)
+ENDFUNCTION()
+
+function(filter_list output input)
+    unset(result)
+    foreach(filename ${${input}})
+        foreach(pattern ${ARGN})
+            if("${filename}" MATCHES "${pattern}")
+                list(APPEND result "${filename}")
+            endif()
+        endforeach()
+    endforeach()
+    set(${output} ${result} PARENT_SCOPE)
+endfunction()
+
+
+IF ($ENV{TH_BINARY_BUILD})
+  MESSAGE(STATUS "TH_BINARY_BUILD detected. Statically linking libstdc++")
+  SET(CMAKE_CXX_FLAGS "-static-libstdc++ ${CMAKE_CXX_FLAGS}")
+  IF (UNIX AND NOT APPLE)
+    # hiding statically linked library symbols, this flag is not available for the linker under macOS
+    SET(CMAKE_CXX_FLAGS "-Wl,--exclude-libs,libstdc++.a ${CMAKE_CXX_FLAGS}")
+  ENDIF(UNIX AND NOT APPLE)
+ENDIF()
+
+# Can be compiled standalone
+IF(NOT AT_INSTALL_BIN_DIR OR NOT AT_INSTALL_LIB_DIR OR NOT AT_INSTALL_INCLUDE_DIR OR NOT AT_INSTALL_SHARE_DIR)
+  SET(AT_INSTALL_BIN_DIR "bin" CACHE PATH "AT install binary subdirectory")
+  SET(AT_INSTALL_LIB_DIR "lib" CACHE PATH "AT install library subdirectory")
+  SET(AT_INSTALL_INCLUDE_DIR "include" CACHE PATH "AT install include subdirectory")
+  SET(AT_INSTALL_SHARE_DIR "share" CACHE PATH "AT install include subdirectory")
+ENDIF()
+
+# TODO: Maybe put this in the generated files directory
+CONFIGURE_FILE(Config.h.in "${CMAKE_CURRENT_SOURCE_DIR}/Config.h")
+
+FILE(GLOB base_h RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "*.h")
+FILE(GLOB base_cpp RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "*.cpp")
+FILE(GLOB native_cpp RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "native/*.cpp")
+FILE(GLOB native_cudnn_cpp RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "native/cudnn/*.cpp")
+FILE(GLOB native_cuda_cu RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "native/cuda/*.cu")
+
+FILE(GLOB cudnn_cpp RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "cudnn/*.cpp")
+
+FILE(GLOB all_python RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "*.py")
+
+FILE(GLOB_RECURSE aten_cuda_cu RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "cuda/*.cu")
+
+IF(DEFINED ENV{PYTORCH_PYTHON})
+  message(STATUS "Using python found in $ENV{PYTORCH_PYTHON}")
+  SET(PYCMD "$ENV{PYTORCH_PYTHON}")
+ELSE()
+  SET(PYCMD "python")
+ENDIF()
+
+SET(GEN_COMMAND
+    ${PYCMD} ${CMAKE_CURRENT_SOURCE_DIR}/gen.py ${CUDA_FLAG}
+    -s ${CMAKE_CURRENT_SOURCE_DIR}
+    ${cwrap_files}
+)
+
+EXECUTE_PROCESS(
+    COMMAND ${GEN_COMMAND}
+      --output-dependencies ${CMAKE_CURRENT_BINARY_DIR}/generated_cpp.txt
+    RESULT_VARIABLE RETURN_VALUE
+)
+if (NOT RETURN_VALUE EQUAL 0)
+    message(STATUS ${generated_cpp})
+    message(FATAL_ERROR "Failed to get generated_cpp list")
+endif()
+file(READ ${CMAKE_CURRENT_BINARY_DIR}/generated_cpp.txt generated_cpp)
+
+FILE(GLOB_RECURSE all_templates "templates/*")
+
+FILE(MAKE_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/ATen)
+
+ADD_CUSTOM_COMMAND(OUTPUT ${generated_cpp}
+COMMAND ${GEN_COMMAND}
+DEPENDS ${all_python} ${all_templates} ${cwrap_files})
+
+# Generated headers used from a cuda (.cu) file are
+# not tracked correctly in cmake . We make the libATen.so depend explicitly
+# on building the generated aten files to workaround.
+ADD_CUSTOM_TARGET(aten_files_are_generated
+  DEPENDS ${generated_cpp}
+)
+
+
+SET(all_cpp ${base_cpp} ${native_cpp} ${native_cudnn_cpp} ${generated_cpp} ${ATen_CPU_SRCS})
+
+INCLUDE_DIRECTORIES(${ATen_CPU_INCLUDE})
+IF(WITH_ROCM)
+  INCLUDE_DIRECTORIES(${ATen_CUDA_INCLUDE})
+  INCLUDE_DIRECTORIES("${CUDA_SDK_ROOT_DIR}/common/inc")
+  INCLUDE_DIRECTORIES("${CMAKE_CURRENT_SOURCE_DIR}/cuda")
+  SET(ATen_CUDA_SRCS ${ATen_CUDA_SRCS} ${aten_cuda_cu} ${native_cuda_cu})
+  SET(all_cpp ${all_cpp} ${ATen_CUDA_SRCS})
+  IF(CUDNN_FOUND)
+    SET(all_cpp ${all_cpp} ${cudnn_cpp})
+  ENDIF()
+endif()
+
+filter_list(generated_h generated_cpp "\\.h$")
+
+INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/..)
+# so the build can find the generated header files
+INCLUDE_DIRECTORIES(${CMAKE_CURRENT_BINARY_DIR})
+IF(NOT AT_LINK_STYLE)
+  SET(AT_LINK_STYLE SHARED)
+ENDIF()
+IF(WITH_ROCM)
+  HIP_ADD_LIBRARY(ATen ${AT_LINK_STYLE} ${all_cpp})
+ELSE()
+  ADD_LIBRARY(ATen ${AT_LINK_STYLE} ${all_cpp})
+ENDIF()
+ADD_DEPENDENCIES(ATen aten_files_are_generated)
+
+SET_TARGET_PROPERTIES(ATen PROPERTIES VERSION 1 SOVERSION 1)
+
+if(NOT ${CMAKE_VERSION} VERSION_LESS "3.1" AND NOT WITH_ROCM)
+    SET_PROPERTY(TARGET ATen PROPERTY CXX_STANDARD 11)
+endif(NOT ${CMAKE_VERSION} VERSION_LESS "3.1" AND NOT WITH_ROCM)
+
+IF(BLAS_FOUND)
+  IF ($ENV{TH_BINARY_BUILD})
+    MESSAGE(STATUS "TH_BINARY_BUILD detected. Enabling special linkage.")
+    TARGET_LINK_LIBRARIES(ATen "${BLAS_LIBRARIES};${BLAS_LIBRARIES};${BLAS_LIBRARIES}")
+  ELSE ($ENV{TH_BINARY_BUILD})
+    TARGET_LINK_LIBRARIES(ATen ${BLAS_LIBRARIES})
+  ENDIF ($ENV{TH_BINARY_BUILD})
+ENDIF(BLAS_FOUND)
+
+IF(LAPACK_FOUND)
+  TARGET_LINK_LIBRARIES(ATen ${LAPACK_LIBRARIES})
+ENDIF(LAPACK_FOUND)
+
+IF (UNIX AND NOT APPLE)
+   INCLUDE(CheckLibraryExists)
+   # https://github.com/libgit2/libgit2/issues/2128#issuecomment-35649830
+   CHECK_LIBRARY_EXISTS(rt clock_gettime "time.h" NEED_LIBRT)
+   IF(NEED_LIBRT)
+     TARGET_LINK_LIBRARIES(ATen rt)
+     SET(CMAKE_REQUIRED_LIBRARIES ${CMAKE_REQUIRED_LIBRARIES} rt)
+   ENDIF(NEED_LIBRT)
+ENDIF(UNIX AND NOT APPLE)
+
+IF(UNIX)
+  SET(CMAKE_EXTRA_INCLUDE_FILES "sys/mman.h")
+  CHECK_FUNCTION_EXISTS(mmap HAVE_MMAP)
+  IF(HAVE_MMAP)
+    ADD_DEFINITIONS(-DHAVE_MMAP=1)
+  ENDIF(HAVE_MMAP)
+  # done for lseek: https://www.gnu.org/software/libc/manual/html_node/File-Position-Primitive.html
+  ADD_DEFINITIONS(-D_FILE_OFFSET_BITS=64)
+  CHECK_FUNCTION_EXISTS(shm_open HAVE_SHM_OPEN)
+  IF(HAVE_SHM_OPEN)
+    ADD_DEFINITIONS(-DHAVE_SHM_OPEN=1)
+  ENDIF(HAVE_SHM_OPEN)
+  CHECK_FUNCTION_EXISTS(shm_unlink HAVE_SHM_UNLINK)
+  IF(HAVE_SHM_UNLINK)
+    ADD_DEFINITIONS(-DHAVE_SHM_UNLINK=1)
+  ENDIF(HAVE_SHM_UNLINK)
+  CHECK_FUNCTION_EXISTS(malloc_usable_size HAVE_MALLOC_USABLE_SIZE)
+  IF(HAVE_MALLOC_USABLE_SIZE)
+    ADD_DEFINITIONS(-DHAVE_MALLOC_USABLE_SIZE=1)
+  ENDIF(HAVE_MALLOC_USABLE_SIZE)
+ENDIF(UNIX)
+
+IF(NOT MSVC)
+  TARGET_LINK_LIBRARIES(ATen m)
+ENDIF(NOT MSVC)
+
+# Is __thread supported?
+IF(NOT MSVC)
+  CHECK_C_SOURCE_COMPILES("static __thread int x = 1; int main() { return x; }" C_HAS_THREAD)
+ELSE(NOT MSVC)
+  CHECK_C_SOURCE_COMPILES("static __declspec( thread ) int x = 1; int main() { return x; }" C_HAS_THREAD)
+ENDIF(NOT MSVC)
+IF(NOT C_HAS_THREAD)
+  MESSAGE(STATUS "Warning: __thread is not supported, generating thread-unsafe code")
+ELSE(NOT C_HAS_THREAD)
+  SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -DTH_HAVE_THREAD")
+ENDIF(NOT C_HAS_THREAD)
+
+if (NNPACK_FOUND)
+  target_link_libraries(ATen ${NNPACK_LIBRARIES})
+endif(NNPACK_FOUND)
+
+IF(CUDA_FOUND)
+  TARGET_LINK_LIBRARIES(ATen
+    ${CUDA_LIBRARIES}
+    ${CUDA_cusparse_LIBRARY}
+    ${CUDA_curand_LIBRARY})
+  CUDA_ADD_CUBLAS_TO_TARGET(ATen)
+
+  if(CUDNN_FOUND)
+    target_link_libraries(ATen ${CUDNN_LIBRARIES})
+  endif(CUDNN_FOUND)
+
+  IF(USE_MAGMA)
+    TARGET_LINK_LIBRARIES(ATen ${MAGMA_LIBRARIES})
+    IF ($ENV{TH_BINARY_BUILD})
+      # because magma is linked statically and it wants a BLAS,
+      # we need to link the BLAS lib against THC. Usually TH will
+      # load a BLAS library and it's all fine, but in the binary builds,
+      # TH uses static linkage to MKL, so it doesn't have all symbols that
+      # magma needs. So in this case, explicitly find a BLAS and link against it
+      # just like in TH
+      SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/../TH/cmake ${CMAKE_MODULE_PATH})
+      FIND_PACKAGE(BLAS)
+      IF(BLAS_FOUND)
+        TARGET_LINK_LIBRARIES(ATen "${BLAS_LIBRARIES};${BLAS_LIBRARIES};${BLAS_LIBRARIES}")
+      ELSE(BLAS_FOUND)
+        MESSAGE(FATAL_ERROR "Binary build needs blas to be found here")
+      ENDIF(BLAS_FOUND)
+    ENDIF($ENV{TH_BINARY_BUILD})
+  ENDIF(USE_MAGMA)
+ENDIF()
+
+### Link in the ROCm stuff ###
+FIND_LIBRARY(HIPBLAS_LIBRARY hipblas HINTS ${HIPBLAS_PATH}/lib)
+FIND_LIBRARY(HIPRNG_LIBRARY hiprng HINTS ${HIPRNG_PATH}/lib)
+
+IF(WITH_ROCM)
+  TARGET_LINK_LIBRARIES(ATen
+    ${HIPBLAS_LIBRARY}
+    ${HIPRNG_LIBRARY})
+ENDIF()
+
+INSTALL(TARGETS ATen
+  RUNTIME DESTINATION "${AT_INSTALL_BIN_DIR}"
+  LIBRARY DESTINATION "${AT_INSTALL_LIB_DIR}"
+  ARCHIVE DESTINATION "${AT_INSTALL_LIB_DIR}")
+
+GET_TARGET_PROPERTY(ATEN_OUTPUT_NAME ATen LOCATION)
+GET_FILENAME_COMPONENT(ATEN_OUTPUT_NAME ${ATEN_OUTPUT_NAME} NAME)
+SET(ATEN_LIBRARIES "${CMAKE_INSTALL_PREFIX}/${AT_INSTALL_LIB_DIR}/${ATEN_OUTPUT_NAME}")
+SET(ATEN_INCLUDE_DIR "${CMAKE_INSTALL_PREFIX}/${AT_INSTALL_INCLUDE_DIR}")
+CONFIGURE_FILE(ATenConfig.cmake.in "${CMAKE_CURRENT_BINARY_DIR}/cmake-exports/ATenConfig.cmake")
+INSTALL(FILES "${CMAKE_CURRENT_BINARY_DIR}/cmake-exports/ATenConfig.cmake"
+  DESTINATION "${AT_INSTALL_SHARE_DIR}/cmake/ATen")
+
+FOREACH(HEADER ${base_h})
+  INSTALL(FILES ${HEADER} DESTINATION ${AT_INSTALL_INCLUDE_DIR}/ATen)
+ENDFOREACH()
+FOREACH(HEADER ${generated_h})
+  INSTALL(FILES ${CMAKE_CURRENT_BINARY_DIR}/${HEADER}
+  DESTINATION ${AT_INSTALL_INCLUDE_DIR}/ATen)
+ENDFOREACH()
+INSTALL(FILES ${CMAKE_CURRENT_BINARY_DIR}/ATen/Declarations.yaml
+  DESTINATION ${AT_INSTALL_SHARE_DIR}/ATen)
diff --git a/src/ATen/Context.cpp b/src/ATen/Context.cpp
index ac07e2fa3..5bab39fcb 100644
--- a/src/ATen/Context.cpp
+++ b/src/ATen/Context.cpp
@@ -44,7 +44,11 @@ void Context::doInitCUDA() {
 #if AT_CUDA_ENABLED()
   thc_state = THCState_alloc();
   THCState_setDeviceAllocator(thc_state, THCCachingAllocator_get());
+#if defined(__HIP_PLATFORM_HCC__)
+  thc_state->hipHostAllocator = &THCCachingHostAllocator;
+#else
   thc_state->cudaHostAllocator = &THCCachingHostAllocator;
+#endif
   THCudaInit(thc_state);
   generator_registry[static_cast<int>(Backend::CUDA)]
     .reset(new CUDAGenerator(this));
diff --git a/src/ATen/Context.h b/src/ATen/Context.h
index 95c227b66..f535fa359 100644
--- a/src/ATen/Context.h
+++ b/src/ATen/Context.h
@@ -13,9 +13,15 @@
 // Forwarde declare these CUDA types here to avoid including CUDA headers in
 // ATen headers, which would make ATen always require CUDA to build.
 struct THCState;
+#if defined(__HIP_PLATFORM_HCC__)
+struct ihipStream_t;
+typedef struct ihipStream_t *hipStream_t;
+struct hipDeviceProp_t;
+#else
 struct CUstream_st;
 typedef struct CUstream_st *cudaStream_t;
 struct cudaDeviceProp;
+#endif
 
 namespace at {
 
diff --git a/src/ATen/Half.cpp b/src/ATen/Half.cpp
index 465799194..9761a9718 100644
--- a/src/ATen/Half.cpp
+++ b/src/ATen/Half.cpp
@@ -25,6 +25,11 @@ template<> AT_API double convert(Half f) {
   return convert<float, Half>(f);
 }
 
+template<> AT_API __fp16 convert(Half f) {
+  __fp16 h = reinterpret_cast<__fp16&>(f.x);
+  return h;
+}
+
 template<> AT_API Half convert(int64_t f) {
   return convert<Half,double>(static_cast<double>(f));
 }
diff --git a/src/ATen/Half.h b/src/ATen/Half.h
index da2326cec..d5ea9a549 100644
--- a/src/ATen/Half.h
+++ b/src/ATen/Half.h
@@ -48,6 +48,7 @@ template<typename To, typename From> To checked_convert(From f, const char* name
 struct alignas(2) Half {
   unsigned short x;
   operator double();
+  operator __fp16();
 };
 
 template<> AT_API Half convert(float f);
@@ -56,11 +57,16 @@ template<> AT_API Half convert(double f);
 template<> AT_API double convert(Half f);
 template<> AT_API Half convert(int64_t f);
 template<> AT_API int64_t convert(Half f);
+template<> AT_API __fp16 convert(Half f);
 
 inline Half::operator double() {
   return convert<double, Half>(*this);
 }
 
+inline Half::operator __fp16() {
+  return convert<__fp16, Half>(*this);
+}
+
 template<> bool overflows<Half, double>(double f);
 template<> bool overflows<Half, int64_t>(int64_t f);
 
@@ -68,4 +74,15 @@ template<typename To, typename From>
 To HalfFix(From h) {
   return To { h.x };
 }
+
+template <>
+  inline __fp16 HalfFix<__fp16, Half>(Half h) {
+  return reinterpret_cast<__fp16&>(h);
+}
+template<>
+  inline Half HalfFix<Half, __fp16>(__fp16 h) {
+  unsigned short s = reinterpret_cast<unsigned short&>(h);
+  return Half { s };
+}
+
 } // namespace at
diff --git a/src/ATen/cuda/CUDAApplyUtils.cuh b/src/ATen/cuda/CUDAApplyUtils.cuh
index 431cad5b2..5a57291dd 100644
--- a/src/ATen/cuda/CUDAApplyUtils.cuh
+++ b/src/ATen/cuda/CUDAApplyUtils.cuh
@@ -322,13 +322,25 @@ bool CUDA_tensor_apply2(at::Tensor a,
   // dimension, and the loop to translate the linear index to the array
   // index can be similarly collapsed. That is what this unrolling is for.
 
-#define HANDLE_CASE(TYPE, A, B)                                         \
-  kernelPointwiseApply2<Op,                                             \
-                        scalar1,                                        \
-                        scalar2,                                        \
-                        TYPE, A, B>                                     \
-   <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(    \
-       aInfo, bInfo, (TYPE) totalElements, op);
+  #if defined(__HIP_PLATFORM_HCC__)
+    #define HANDLE_CASE(TYPE, A, B)                                         \
+      hipLaunchKernelGGL(                                                   \
+          (kernelPointwiseApply2<Op,                                        \
+                            scalar1,                                        \
+                            scalar2,                                        \
+                            TYPE, A, B>),                                   \
+           grid, block, 0, at::globalContext().getCurrentCUDAStream(),      \
+           aInfo, bInfo, (TYPE) totalElements, op);
+  #else
+    #define HANDLE_CASE(TYPE, A, B)                                         \
+      kernelPointwiseApply2<Op,                                             \
+                            scalar1,                                        \
+                            scalar2,                                        \
+                            TYPE, A, B>                                     \
+       <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(    \
+           aInfo, bInfo, (TYPE) totalElements, op);
+  #endif
+
 
 #define HANDLE_B_CASE(TYPE, A, B)               \
   {                                             \
@@ -398,22 +410,42 @@ bool CUDA_tensor_apply2(at::Tensor a,
     // version and the completely generic version, to reduce
     // compilation time.
     if (aInfo.isContiguous() && bInfo.isContiguous()) {
+#if defined(__HIP_PLATFORM_HCC__)
+      hipLaunchKernelGGL(
+        (kernelPointwiseApply2<Op,
+                            scalar1,
+                            scalar2,
+                          uint64_t, -2, -2>),
+           grid, block, 0, at::globalContext().getCurrentCUDAStream(),
+           aInfo, bInfo, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply2<Op,
                             scalar1,
                             scalar2,
                           uint64_t, -2, -2>
         <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(
            aInfo, bInfo, (uint64_t) totalElements, op);
+#endif
     } else {
 #if CUDA_VERSION < 9000
       grid.x = std::min((unsigned int)at::globalContext().getCurrentDeviceProperties()->multiProcessorCount * AT_APPLY_BLOCKS_PER_SM , grid.x);
 #endif
+#if defined(__HIP_PLATFORM_HCC__)
+    hipLaunchKernelGGL(
+      (kernelPointwiseApply2<Op,
+                            scalar1,
+                            scalar2,
+                            uint64_t, -1, -1>),
+           grid, block, 0, at::globalContext().getCurrentCUDAStream(),
+           aInfo, bInfo, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply2<Op,
                             scalar1,
                             scalar2,
                             uint64_t, -1, -1>
         <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(
            aInfo, bInfo, (uint64_t) totalElements, op);
+#endif
     }
   }
 #undef HANDLE_CASE
@@ -512,15 +544,26 @@ bool CUDA_tensor_apply3(at::Tensor a,
     c = c.contiguous();
   }
 
-#define HANDLE_CASE(TYPE, A, B, C)                                      \
-  kernelPointwiseApply3<Op,                                             \
-                        scalar1,                                        \
-                        scalar2,                                        \
-                        scalar3,                                        \
-                        TYPE, A, B, C>                                  \
-    <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(   \
-      aInfo, bInfo, cInfo, (TYPE) totalElements, op);
-
+#if defined(__HIP_PLATFORM_HCC__)
+  #define HANDLE_CASE(TYPE, A, B, C)                                      \
+    hipLaunchKernelGGL(                                                   \
+        (kernelPointwiseApply3<Op,                                        \
+                          scalar1,                                        \
+                          scalar2,                                        \
+                          scalar3,                                        \
+                          TYPE, A, B, C>),                                \
+        grid, block, 0, at::globalContext().getCurrentCUDAStream(),       \
+        aInfo, bInfo, cInfo, (TYPE) totalElements, op);
+#else
+  #define HANDLE_CASE(TYPE, A, B, C)                                      \
+    kernelPointwiseApply3<Op,                                             \
+                          scalar1,                                        \
+                          scalar2,                                        \
+                          scalar3,                                        \
+                          TYPE, A, B, C>                                  \
+      <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(   \
+        aInfo, bInfo, cInfo, (TYPE) totalElements, op);
+#endif
 #define HANDLE_C_CASE(TYPE, A, B, C)            \
   {                                             \
     if (cInfo.isContiguous()) {                 \
@@ -619,6 +662,16 @@ bool CUDA_tensor_apply3(at::Tensor a,
     // version and the completely generic version, to reduce
     // compilation time.
     if (aInfo.isContiguous() && bInfo.isContiguous() && cInfo.isContiguous()) {
+#if defined(__HIP_PLATFORM_HCC__)
+      hipLaunchKernelGGL(
+        (kernelPointwiseApply3<Op,
+                              scalar1,
+                              scalar2,
+                              scalar3,
+                              uint64_t, -2, -2, -2>),
+          grid, block, 0, at::globalContext().getCurrentCUDAStream(),
+          aInfo, bInfo, cInfo, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply3<Op,
                             scalar1,
                             scalar2,
@@ -626,11 +679,21 @@ bool CUDA_tensor_apply3(at::Tensor a,
                             uint64_t, -2, -2, -2>
         <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(
           aInfo, bInfo, cInfo, (uint64_t) totalElements, op);
+#endif
     } else {
 #if CUDA_VERSION < 9000
   grid.x = std::min((unsigned int)at::globalContext().getCurrentDeviceProperties()->multiProcessorCount * AT_APPLY_BLOCKS_PER_SM , grid.x);
 #endif
-
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL(
+  	(kernelPointwiseApply3<Op,
+                          scalar1,
+                          scalar2,
+                          scalar3,
+                          uint64_t, -1, -1, -1>),
+            grid, block, 0, at::globalContext().getCurrentCUDAStream(),
+            aInfo, bInfo, cInfo, (uint64_t) totalElements, op);
+#else
 	kernelPointwiseApply3<Op,
                         scalar1,
                         scalar2,
@@ -638,6 +701,7 @@ bool CUDA_tensor_apply3(at::Tensor a,
                         uint64_t, -1, -1, -1>
         <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(
           aInfo, bInfo, cInfo, (uint64_t) totalElements, op);
+#endif
     }
   }
 #undef HANDLE_CASE
@@ -755,16 +819,28 @@ bool CUDA_tensor_apply4(at::Tensor a,
     d = d.contiguous();
   }
 
-#define HANDLE_CASE(TYPE, A, B, C, D)                                   \
-  kernelPointwiseApply4<Op,                                             \
-                        scalar1,                                        \
-                        scalar2,                                        \
-                        scalar3,                                        \
-                        scalar4,                                        \
-                        TYPE, A, B, C, D>                               \
-    <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(   \
-    aInfo, bInfo, cInfo, dInfo, (TYPE) totalElements, op);
-
+#if defined(__HIP_PLATFORM_HCC__)
+  #define HANDLE_CASE(TYPE, A, B, C, D)                                   \
+    hipLaunchKernelGGL(                                                   \
+    (kernelPointwiseApply4<Op,                                            \
+                          scalar1,                                        \
+                          scalar2,                                        \
+                          scalar3,                                        \
+                          scalar4,                                        \
+                          TYPE, A, B, C, D>),                             \
+      grid, block, 0, at::globalContext().getCurrentCUDAStream(),         \
+      aInfo, bInfo, cInfo, dInfo, (TYPE) totalElements, op);
+#else
+  #define HANDLE_CASE(TYPE, A, B, C, D)                                   \
+    kernelPointwiseApply4<Op,                                             \
+                          scalar1,                                        \
+                          scalar2,                                        \
+                          scalar3,                                        \
+                          scalar4,                                        \
+                          TYPE, A, B, C, D>                               \
+      <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(   \
+      aInfo, bInfo, cInfo, dInfo, (TYPE) totalElements, op);
+#endif
 #define HANDLE_D_CASE(TYPE, A, B, C, D)         \
   {                                             \
     if (dInfo.isContiguous()) {                 \
@@ -891,6 +967,17 @@ bool CUDA_tensor_apply4(at::Tensor a,
     // version and the completely generic version, to reduce
     // compilation time.
     if (aInfo.isContiguous() && bInfo.isContiguous() && cInfo.isContiguous() && dInfo.isContiguous()) {
+#if defined(__HIP_PLATFORM_HCC__)
+      hipLaunchKernelGGL(
+        (kernelPointwiseApply4<Op,
+                              scalar1,
+                              scalar2,
+                              scalar3,
+                              scalar4,
+                              uint64_t, -2, -2, -2, -2>),
+            grid, block, 0, at::globalContext().getCurrentCUDAStream(),
+            aInfo, bInfo, cInfo, dInfo, (uint64_t) totalElements, op);
+#else
       kernelPointwiseApply4<Op,
                             scalar1,
                             scalar2,
@@ -899,11 +986,22 @@ bool CUDA_tensor_apply4(at::Tensor a,
                             uint64_t, -2, -2, -2, -2>
         <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(
           aInfo, bInfo, cInfo, dInfo, (uint64_t) totalElements, op);
+#endif
     } else {
 #if CUDA_VERSION < 9000
   grid.x = std::min((unsigned int)at::globalContext().getCurrentDeviceProperties()->multiProcessorCount * AT_APPLY_BLOCKS_PER_SM , grid.x);
 #endif
-
+#if defined(__HIP_PLATFORM_HCC__)
+    hipLaunchKernelGGL(
+    	(kernelPointwiseApply4<Op,
+                            scalar1,
+                            scalar2,
+                            scalar3,
+                            scalar4,
+                            uint64_t, -1, -1, -1, -1>),
+              grid, block, 0, at::globalContext().getCurrentCUDAStream(),
+              aInfo, bInfo, cInfo, dInfo, (uint64_t) totalElements, op);
+#else
 	kernelPointwiseApply4<Op,
                         scalar1,
                         scalar2,
@@ -912,6 +1010,7 @@ bool CUDA_tensor_apply4(at::Tensor a,
                         uint64_t, -1, -1, -1, -1>
         <<<grid, block, 0, at::globalContext().getCurrentCUDAStream()>>>(
           aInfo, bInfo, cInfo, dInfo, (uint64_t) totalElements, op);
+#endif
     }
   }
 #undef HANDLE_CASE
diff --git a/src/ATen/cuda/CUDAHalf.cu b/src/ATen/cuda/CUDAHalf.cu
index dbb18fc3f..625560c4b 100644
--- a/src/ATen/cuda/CUDAHalf.cu
+++ b/src/ATen/cuda/CUDAHalf.cu
@@ -5,6 +5,7 @@
 #include <cuda_runtime.h>
 #include <cuda_fp16.h>
 
+#if !defined(__HIP_PLATFORM_HCC__)
 namespace at {
 #if CUDA_VERSION < 9000
 template <> AT_API
@@ -54,3 +55,4 @@ template <> Half HalfFix(__half h) {
 }
 #endif
 } // namespace at
+#endif
diff --git a/src/ATen/cuda/CUDAHalf.cuh b/src/ATen/cuda/CUDAHalf.cuh
index b5d9520a0..9aa383506 100644
--- a/src/ATen/cuda/CUDAHalf.cuh
+++ b/src/ATen/cuda/CUDAHalf.cuh
@@ -6,7 +6,7 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 #include <cuda_fp16.h>
-
+#if !defined(__HIP_PLATFORM_HCC__)
 namespace at {
 template <> AT_API half convert(Half aten_half);
 template <> AT_API Half convert(half cuda_half);
@@ -16,3 +16,4 @@ template <> __half HalfFix(Half h);
 template <> Half HalfFix(__half h);
 #endif
 } // namespace at
+#endif
diff --git a/src/ATen/cuda/detail/IndexUtils.cu b/src/ATen/cuda/detail/IndexUtils.cu
index 94e5dd134..2889bfe56 100644
--- a/src/ATen/cuda/detail/IndexUtils.cu
+++ b/src/ATen/cuda/detail/IndexUtils.cu
@@ -16,6 +16,7 @@ int compareSizeAndStride(const void* a, const void* b) {
   return aS->stride < bS->stride;
 }
 
+#if !defined(__HIP_DEVICE_COMPILE__)
 bool overlappingIndices(const Tensor& t) {
   // In this function, we don't care about permutations of the
   // size/stride arrays (transpositions).
@@ -68,6 +69,8 @@ bool overlappingIndices(const Tensor& t) {
   /* Tensor has holes or is contiguous */
   return false;
 }
+#endif
+
 
 bool canUse32BitIndexMath(const Tensor& t, int64_t max_elem) {
   int64_t elements = t.numel();
diff --git a/src/ATen/cuda/detail/TensorInfo.cuh b/src/ATen/cuda/detail/TensorInfo.cuh
index 9429a465b..5d0e61613 100644
--- a/src/ATen/cuda/detail/TensorInfo.cuh
+++ b/src/ATen/cuda/detail/TensorInfo.cuh
@@ -1,7 +1,6 @@
 #pragma once
 
 #include "ATen/ATen.h"
-
 namespace at {
 namespace cuda {
 namespace detail {
diff --git a/src/ATen/native/cuda/Distributions.cu b/src/ATen/native/cuda/Distributions.cu
index 50d162f14..9e04df62d 100644
--- a/src/ATen/native/cuda/Distributions.cu
+++ b/src/ATen/native/cuda/Distributions.cu
@@ -2,10 +2,11 @@
 #include "ATen/NativeFunctions.h"
 #include "ATen/cuda/CUDAApplyUtils.cuh"
 
+#if !defined(__HIP_PLATFORM_HCC__)
 #include <curand.h>
 #include <curand_kernel.h>
 #include <curand_philox4x32_x.h>
-
+#endif
 #include <TH/THAtomic.h>
 
 #include <THC/THCGeneral.h>
@@ -15,7 +16,7 @@
 
 #include <cstdint>
 #include <utility>
-
+#if !defined(__HIP_PLATFORM_HCC__)
 THCGenerator* THCRandom_getGenerator(THCState* state);
 
 namespace {
@@ -49,11 +50,15 @@ void poisson_cuda_kernel(
 namespace at { namespace native {
 Tensor _s_poisson_cuda(const Tensor& lambda, Generator* gen) {
   Tensor ret = lambda.type().tensor(lambda.sizes());
+
+  #if !defined(__HIP_PLATFORM_HCC__)
   auto lambda_ = lambda.toType(ScalarType::Float);
   AT_DISPATCH_FLOATING_TYPES(ret.type(), "poisson", [&] {
      poisson_cuda_kernel<scalar_t>(ret, lambda_, next_philox_seed(gen));
    });
+  #endif
   return ret;
 }
 
 }} // namespace at::native
+#endif
diff --git a/src/ATen/native/cuda/Embedding.cu b/src/ATen/native/cuda/Embedding.cu
index 2dfa00ddb..34b1ddcf2 100644
--- a/src/ATen/native/cuda/Embedding.cu
+++ b/src/ATen/native/cuda/Embedding.cu
@@ -213,6 +213,16 @@ Tensor embedding_backward_cuda(const Tensor & grad_, const Tensor & indices,
 
    AT_DISPATCH_FLOATING_TYPES_AND_HALF(grad.type(), "embedding_backward", [&] {
      using cuda_scalar_t = cuda::type<scalar_t>;
+#if defined(__HIP_PLATFORM_HCC__)
+     hipLaunchKernelGGL(
+       embedding_backward_feature_kernel, grid, block, 0, stream,
+         indices.data<int64_t>(),
+         grad.data<cuda_scalar_t>(), // might need to use type scalar_t
+         grad_weight.data<cuda_scalar_t>(), // might need to use type scalar_t
+         static_cast<int64_t>(num_indices),
+         static_cast<int64_t>(stride),
+         static_cast<int>(padding_idx));
+#else
      embedding_backward_feature_kernel<<<grid, block, 0, stream>>>(
        indices.data<int64_t>(),
        grad.data<cuda_scalar_t>(),
@@ -220,8 +230,8 @@ Tensor embedding_backward_cuda(const Tensor & grad_, const Tensor & indices,
        num_indices,
        stride,
        padding_idx);
+#endif
    });
-
    THCudaCheck(cudaGetLastError());
    return grad_weight;
   }
@@ -289,15 +299,28 @@ Tensor embedding_backward_cuda(const Tensor & grad_, const Tensor & indices,
 
   AT_DISPATCH_FLOATING_TYPES_AND_HALF(grad.type(), "embedding_backward", [&] {
     using cuda_scalar_t = cuda::type<scalar_t>;
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL(
+    embedding_backward_kernel, grid, block, 0, stream,
+      sorted_indices.data<int64_t>(),
+      orig_indices.data<int64_t>(),
+      grad.data<cuda_scalar_t>(), // might need to use scalar_t
+      grad_weight.data<cuda_scalar_t>(), // might need to use scalar_t
+      count.defined() ? count.data<int64_t>() : nullptr,
+      static_cast<int64_t>(num_indices),
+      static_cast<int64_t>(stride),
+      static_cast<int>(padding_idx));
+#else
     embedding_backward_kernel<<<grid, block, 0, stream>>>(
       sorted_indices.data<int64_t>(),
       orig_indices.data<int64_t>(),
       grad.data<cuda_scalar_t>(),
       grad_weight.data<cuda_scalar_t>(),
       count.defined() ? count.data<int64_t>() : nullptr,
-      num_indices,
-      stride,
-      padding_idx);
+      static_cast<int64_t>(num_indices),
+      static_cast<int64_t>(stride),
+      static_cast<int>(padding_idx));
+#endif
   });
   THCudaCheck(cudaGetLastError());
 
@@ -337,12 +360,13 @@ Tensor & embedding_renorm_cuda_(Tensor & self, const Tensor & indices,
   AT_DISPATCH_FLOATING_TYPES_AND_HALF(self.type(), "embedding_backward", [&] {
     using cuda_scalar_t = cuda::type<scalar_t>;
     using accscalar_t = cuda::acc_type<cuda_scalar_t>;
+
     renorm_kernel<<<grid, block, 128 * sizeof(accscalar_t), stream>>>(
       self.data<cuda_scalar_t>(),
       unique_indices.data<int64_t>(),
       scalar_cast<accscalar_t>(max_norm),
       scalar_cast<accscalar_t>(norm_type),
-      dim);
+      scalar_cast<int>(dim));
   });
   THCudaCheck(cudaGetLastError());
 
diff --git a/src/ATen/native/cuda/EmbeddingBag.cu b/src/ATen/native/cuda/EmbeddingBag.cu
index 7d331859e..95157f81a 100644
--- a/src/ATen/native/cuda/EmbeddingBag.cu
+++ b/src/ATen/native/cuda/EmbeddingBag.cu
@@ -46,7 +46,9 @@ __global__ void EmbeddingBag_updateOutputKernel(
       scalar_t *weightFeat = weight + featureDim;
       int64_t begin = offsets[bag];
       int64_t end = (bag < numBags - 1) ? (offsets[bag + 1]) : numIndices;
+#if defined(__NVCC__)
       assert(end >= begin);
+#endif
       accscalar_t weightFeatSum = scalar_cast<accscalar_t>(0);
       int64_t bag_size_ = 0;
       for (int64_t emb = begin; emb < end; emb++) {
@@ -172,13 +174,25 @@ embedding_bag_cuda(const Tensor &weight, const Tensor &indices,
 
   dim3 block = dim3(32, 8);
   int grid = 1024;
+
   AT_DISPATCH_FLOATING_TYPES_AND_HALF(weight.type(), "embedding_bag_cuda", [&] {
     using cuda_scalar_t = cuda::type<scalar_t>;
+#if defined(__HIP_PLATFORM_HCC__)
+    hipLaunchKernelGGL(
+      EmbeddingBag_updateOutputKernel<cuda_scalar_t>,grid, block, 0, stream, //might need to use scalar_t
+          indices.data<int64_t>(), offsets.data<int64_t>(),
+          weight.data<cuda_scalar_t>(), output.data<cuda_scalar_t>(),  //might need to use scalar_t
+          offset2bag.data<int64_t>(), static_cast<int64_t>(numIndices),
+          static_cast<int64_t>(numBags), static_cast<int64_t>(stride),
+          static_cast<int>(mode),
+          bag_size.data<int64_t>());
+#else
     EmbeddingBag_updateOutputKernel<cuda_scalar_t><<<grid, block, 0, stream>>>(
         indices.data<int64_t>(), offsets.data<int64_t>(),
         weight.data<cuda_scalar_t>(), output.data<cuda_scalar_t>(),
         offset2bag.data<int64_t>(), numIndices, numBags, stride, mode,
         bag_size.data<int64_t>());
+#endif
   });
 
   THCudaCheck(cudaGetLastError());
@@ -269,6 +283,17 @@ Tensor embedding_bag_backward_cuda(const Tensor &grad_, const Tensor &indices,
   AT_DISPATCH_FLOATING_TYPES_AND_HALF(
       grad.type(), "embedding_bag_backward_cuda", [&] {
         using cuda_scalar_t = cuda::type<scalar_t>;
+#if defined(__HIP_PLATFORM_HCC__)
+        hipLaunchKernelGGL(
+          EmbeddingBag_accGradParametersKernel<
+              cuda_scalar_t>, grid, block, 0, stream, // might need to use scalar_t
+              sorted_indices.data<int64_t>(), orig_indices.data<int64_t>(),
+              grad.data<cuda_scalar_t>(), grad_weight.data<cuda_scalar_t>(),  // might need to use scalar_t
+              offset2bag.data<int64_t>(),
+              count.defined() ? count.data<int64_t>() : nullptr,
+              static_cast<ptrdiff_t>(numel), static_cast<int64_t>(stride),
+              static_cast<int>(mode), bag_size.data<int64_t>());
+#else
         EmbeddingBag_accGradParametersKernel<
             cuda_scalar_t><<<grid, block, 0, stream>>>(
             sorted_indices.data<int64_t>(), orig_indices.data<int64_t>(),
@@ -276,6 +301,7 @@ Tensor embedding_bag_backward_cuda(const Tensor &grad_, const Tensor &indices,
             offset2bag.data<int64_t>(),
             count.defined() ? count.data<int64_t>() : nullptr, numel, stride,
             mode, bag_size.data<int64_t>());
+#endif
       });
 
   THCudaCheck(cudaGetLastError());
diff --git a/src/ATen/native/cuda/RoiPooling.cu b/src/ATen/native/cuda/RoiPooling.cu
index ef5ff049f..8c9342626 100644
--- a/src/ATen/native/cuda/RoiPooling.cu
+++ b/src/ATen/native/cuda/RoiPooling.cu
@@ -130,9 +130,10 @@ std::tuple<Tensor, Tensor> RoiPooling2d_forward_cuda(
 
   dim3 block(512);
   dim3 grid((output.numel() + 512 - 1) / 512);
-  RoiPooling2d_forward_kernel<<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
-    output.numel(), input.data<float>(), rois.data<float>(), static_cast<float>(spatialScale), inputChannels,
-    inputHeight, inputWidth, pooledHeight, pooledWidth, output.data<float>(), argmaxes.data<int>());
+  RoiPooling2d_forward_kernel<float><<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
+    static_cast<int>(output.numel()), input.data<float>(), rois.data<float>(), static_cast<float>(spatialScale), static_cast<int>(inputChannels),
+    static_cast<int>(inputHeight), static_cast<int>(inputWidth), static_cast<int>(pooledHeight), static_cast<int>(pooledWidth), output.data<float>(), argmaxes.data<int>());
+
   AT_ASSERT(cudaGetLastError() == cudaSuccess, "RoiPooling2d_forward_kernel failed");
 
   return std::make_tuple(output, argmaxes);
@@ -197,10 +198,11 @@ Tensor RoiPooling2d_backward_cuda(
 
   dim3 block(512);
   dim3 grid((gradInput.numel() + 512 - 1) / 512);
-  RoiPooling2d_backward_kernel<<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
-    gradOutput.numel(), gradOutput.data<float>(), argmaxes.data<int>(), proposals,
-    static_cast<float>(spatialScale), inputChannels, inputHeight, inputWidth,
-    pooledHeight, pooledWidth, gradInput.data<float>(), rois.data<float>());
+  RoiPooling2d_backward_kernel<float><<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
+    static_cast<int>(gradOutput.numel()), gradOutput.data<float>(), argmaxes.data<int>(), static_cast<int>(proposals),
+    static_cast<float>(spatialScale), static_cast<int>(inputChannels), static_cast<int>(inputHeight), static_cast<int>(inputWidth),
+    static_cast<int>(pooledHeight), static_cast<int>(pooledWidth), gradInput.data<float>(), rois.data<float>());
+
   AT_ASSERT(cudaGetLastError() == cudaSuccess, "RoiPooling2d_forward_kernel failed");
 
   return gradInput;
diff --git a/src/ATen/preprocess_declarations.py b/src/ATen/preprocess_declarations.py
index 1bc33e533..f9a91947d 100644
--- a/src/ATen/preprocess_declarations.py
+++ b/src/ATen/preprocess_declarations.py
@@ -57,9 +57,11 @@ def process_types_and_backends(option):
     pairs = set(p for pair in pairs for p in expand(pair))
 
     # disable CUDA Half if there is a Sparse argument
+    # for arg in option.get('arguments', []):
+    #     if arg['type'] == 'THSTensor*':
+    #         pairs.discard(('CUDA', 'Half'))
     for arg in option.get('arguments', []):
-        if arg['type'] == 'THSTensor*':
-            pairs.discard(('CUDA', 'Half'))
+        pairs.discard(('CUDA', 'Half'))
 
     # special case remove Half for cpu unless it is explicitly enabled,
     if not option.get('cpu_half', False):
diff --git a/src/ATen/templates/StorageDerived.cpp b/src/ATen/templates/StorageDerived.cpp
index e20de4f5c..21597e556 100644
--- a/src/ATen/templates/StorageDerived.cpp
+++ b/src/ATen/templates/StorageDerived.cpp
@@ -19,11 +19,19 @@ ${Storage}::${Storage}(Context* context, std::size_t storage_size)
   : storage(${THStorage}_newWithSize(${state,} storage_size)), context(context) {}
 
 #if ${isCUDA}
+#if defined(__HIP_PLATFORM_HCC__)
+static hipError_t call_deleter(void * ctx, void * data) {
+#else
 static cudaError_t call_deleter(void * ctx, void * data) {
+#endif
   auto fnptr = (std::function<void(void*)>*) ctx;
   (*fnptr)(data);
   delete fnptr;
+#if defined(__HIP_PLATFORM_HCC__)
+  return hipSuccess;
+#else
   return cudaSuccess;
+#endif
 }
 static THCDeviceAllocator storage_deleter = {
   nullptr,
diff --git a/src/ATen/test/CMakeLists.txt b/src/ATen/test/CMakeLists.txt
index fa99b87ae..d859ce193 100644
--- a/src/ATen/test/CMakeLists.txt
+++ b/src/ATen/test/CMakeLists.txt
@@ -1,4 +1,9 @@
-ADD_EXECUTABLE(scalar_test scalar_test.cpp)
+if(WITH_ROCM)
+SET(CMAKE_C_COMPILER ${HIP_HIPCC_EXECUTABLE})
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+endif()
+
+ADD_EXECUTABLE(scalar_test scalar_test.cpp)-
 target_link_libraries(scalar_test ATen)
 
 ADD_EXECUTABLE(basic basic.cpp)
@@ -32,7 +37,11 @@ add_executable(tbb_init_test tbb_init_test.cpp)
 target_link_libraries(tbb_init_test ATen)
 
 if(NOT NO_CUDA)
+  if(WITH_ROCM)
+  hip_add_executable(integer_divider_test integer_divider_test.cu)
+  else()
   cuda_add_executable(integer_divider_test integer_divider_test.cu)
+  endif()
   target_link_libraries(integer_divider_test ATen)
 endif()
 
diff --git a/src/ATen/test/scalar_test.cpp b/src/ATen/test/scalar_test.cpp
index ca3c865f4..83da70e0b 100644
--- a/src/ATen/test/scalar_test.cpp
+++ b/src/ATen/test/scalar_test.cpp
@@ -146,5 +146,7 @@ TEST_CASE( "scalar test", "[]" ) {
   auto float_one = ones(T, {});
   REQUIRE(float_one.toCFloat() == 1);
   REQUIRE(float_one.toCInt() == 1);
+#if !defined(__HIP_PLATFORM_HCC__)
   REQUIRE((float_one.toCHalf() == 1));
+#endif
 }
diff --git a/src/TH/THAllocator.c b/src/TH/THAllocator.c
index 9badbc462..9c2606bc2 100644
--- a/src/TH/THAllocator.c
+++ b/src/TH/THAllocator.c
@@ -68,7 +68,7 @@ char * unknown_filename = "filename not specified";
 
 THMapAllocatorContext *THMapAllocatorContext_new(const char *filename, int flags)
 {
-  THMapAllocatorContext *ctx = THAlloc(sizeof(THMapAllocatorContext));
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) THAlloc(sizeof(THMapAllocatorContext));
 
   if (!(flags & TH_ALLOCATOR_MAPPED_SHARED) && !(flags & TH_ALLOCATOR_MAPPED_SHAREDMEM))
     flags &= ~TH_ALLOCATOR_MAPPED_NOCREATE;
@@ -77,7 +77,7 @@ THMapAllocatorContext *THMapAllocatorContext_new(const char *filename, int flags
         "in shared mode");
 
   if (filename) {
-    ctx->filename = THAlloc(strlen(filename)+1);
+    ctx->filename = (char*) THAlloc(strlen(filename)+1);
     strcpy(ctx->filename, filename);
   } else {
     ctx->filename = unknown_filename;
@@ -136,7 +136,7 @@ static void *_map_alloc(void* ctx_, ptrdiff_t size)
   if (size == 0)
     return NULL;
 
-  THMapAllocatorContext *ctx = ctx_;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
   void *data = NULL;
 
 #ifdef _WIN32
@@ -399,7 +399,7 @@ static void THMapAllocator_free(void* ctx_, void* data) {
   if (data == NULL)
     return;
 
-  THMapAllocatorContext *ctx = ctx_;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
 
 #ifdef _WIN32
   if ((ctx->flags & TH_ALLOCATOR_MAPPED_KEEPFD) || (ctx->flags & TH_ALLOCATOR_MAPPED_SHAREDMEM))
@@ -462,7 +462,7 @@ static void THMapAllocator_free(void* ctx, void* data) {
 #if (defined(_WIN32) || defined(HAVE_MMAP)) && defined(TH_ATOMIC_IPC_REFCOUNT)
 
 static void * THRefcountedMapAllocator_alloc(void *_ctx, ptrdiff_t size) {
-  THMapAllocatorContext *ctx = _ctx;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) _ctx;
 
   if (ctx->flags & TH_ALLOCATOR_MAPPED_FROMFD)
     THError("THRefcountedMapAllocator doesn't support TH_ALLOCATOR_MAPPED_FROMFD flag");
@@ -492,7 +492,7 @@ static void *THRefcountedMapAllocator_realloc(void* ctx, void* ptr, ptrdiff_t si
 }
 
 static void THRefcountedMapAllocator_free(void* ctx_, void* data) {
-  THMapAllocatorContext *ctx = ctx_;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
 
 #ifdef _WIN32
   THMapInfo *info = (THMapInfo*)(((char*)data) - TH_ALLOC_ALIGNMENT);
diff --git a/src/TH/THDiskFile.c b/src/TH/THDiskFile.c
index 41cc254f6..0e2dbb3f4 100644
--- a/src/TH/THDiskFile.c
+++ b/src/TH/THDiskFile.c
@@ -105,7 +105,7 @@ size_t fread__(void *ptr, size_t size, size_t nitems, FILE *stream)
       {                                                                 \
         if(sizeof(TYPE) > 1)                                            \
         {                                                               \
-          char *buffer = THAlloc(sizeof(TYPE)*n);                       \
+          char *buffer = (char*) THAlloc(sizeof(TYPE)*n);               \
           THDiskFile_reverseMemory(buffer, data, sizeof(TYPE), n);      \
           nwrite = fwrite(buffer, sizeof(TYPE), n, dfself->handle);     \
           THFree(buffer);                                               \
@@ -396,7 +396,7 @@ static size_t THDiskFile_readLong(THFile *self, int64_t *data, size_t n)
     else /* if(dfself->longSize == 8) */
     {
       int big_endian = !THDiskFile_isLittleEndianCPU();
-      int32_t *buffer = THAlloc(8*n);
+      int32_t *buffer = (int32_t*) THAlloc(8*n);
       nread = fread__(buffer, 8, n, dfself->handle);
       size_t i;
       for(i = nread; i > 0; i--)
@@ -449,14 +449,14 @@ static size_t THDiskFile_writeLong(THFile *self, int64_t *data, size_t n)
       }
       else
       {
-        char *buffer = THAlloc(sizeof(int64_t)*n);
+        char *buffer = (char*) THAlloc(sizeof(int64_t)*n);
         THDiskFile_reverseMemory(buffer, data, sizeof(int64_t), n);
         nwrite = fwrite(buffer, sizeof(int64_t), n, dfself->handle);
         THFree(buffer);
       }
     } else if(dfself->longSize == 4)
     {
-      int32_t *buffer = THAlloc(4*n);
+      int32_t *buffer = (int32_t*) THAlloc(4*n);
       size_t i;
       for(i = 0; i < n; i++)
         buffer[i] = (int32_t) data[i];
@@ -468,7 +468,7 @@ static size_t THDiskFile_writeLong(THFile *self, int64_t *data, size_t n)
     else /* if(dfself->longSize == 8) */
     {
       int big_endian = !THDiskFile_isLittleEndianCPU();
-      int32_t *buffer = THAlloc(8*n);
+      int32_t *buffer = (int32_t*) THAlloc(8*n);
       size_t i;
       for(i = 0; i < n; i++)
       {
@@ -517,7 +517,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
 
   if(format[1] == 'a')
   {
-    char *p = THAlloc(TBRS_BSZ);
+    char *p = (char*) THAlloc(TBRS_BSZ);
     size_t total = TBRS_BSZ;
     size_t pos = 0;
 
@@ -526,7 +526,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
       if(total-pos == 0) /* we need more space! */
       {
         total += TBRS_BSZ;
-        p = THRealloc(p, total);
+        p = (char*) THRealloc(p, total);
       }
       pos += fread(p+pos, 1, total-pos, dfself->handle);
       if (pos < total) /* eof? */
@@ -548,7 +548,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
   }
   else
   {
-    char *p = THAlloc(TBRS_BSZ);
+    char *p = (char*) THAlloc(TBRS_BSZ);
     size_t total = TBRS_BSZ;
     size_t pos = 0;
     size_t size;
@@ -558,7 +558,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
       if(total-pos <= 1) /* we can only write '\0' in there! */
       {
         total += TBRS_BSZ;
-        p = THRealloc(p, total);
+        p = (char*) THRealloc(p, total);
       }
       if (fgets(p+pos, (int) (total-pos), dfself->handle) == NULL) /* eof? */
       {
@@ -677,10 +677,10 @@ THFile *THDiskFile_new(const char *name, const char *mode, int isQuiet)
       THError("cannot open <%s> in mode %c%c", name, (isReadable ? 'r' : ' '), (isWritable ? 'w' : ' '));
   }
 
-  self = THAlloc(sizeof(THDiskFile));
+  self = (THDiskFile*) THAlloc(sizeof(THDiskFile));
 
   self->handle = handle;
-  self->name = THAlloc(strlen(name)+1);
+  self->name = (char*) THAlloc(strlen(name)+1);
   strcpy(self->name, name);
   self->isNativeEncoding = 1;
   self->longSize = 0;
@@ -781,10 +781,10 @@ THFile *THPipeFile_new(const char *name, const char *mode, int isQuiet)
       THError("cannot open <%s> in mode %c%c.  This might be because eg the executable doesn't exist, but it could also be because you are out of memory.", name, (isReadable ? 'r' : ' '), (isWritable ? 'w' : ' '));
   }
 
-  self = THAlloc(sizeof(THDiskFile));
+  self = (THDiskFile*) THAlloc(sizeof(THDiskFile));
 
   self->handle = handle;
-  self->name = THAlloc(strlen(name)+1);
+  self->name = (char*) THAlloc(strlen(name)+1);
   strcpy(self->name, name);
   self->isNativeEncoding = 1;
   self->longSize = 0;
diff --git a/src/TH/THMemoryFile.c b/src/TH/THMemoryFile.c
index cbbcfc1f5..206ce821a 100644
--- a/src/TH/THMemoryFile.c
+++ b/src/TH/THMemoryFile.c
@@ -527,7 +527,7 @@ static size_t THMemoryFile_writeLong(THFile *self, int64_t *data, size_t n)
 
 static int8_t* THMemoryFile_cloneString(const int8_t *str, ptrdiff_t size)
 {
-  int8_t *cstr = THAlloc(size);
+  int8_t *cstr = (int8_t*) THAlloc(size);
   memcpy(cstr, str, size);
   return cstr;
 }
@@ -665,7 +665,7 @@ THFile *THMemoryFile_newWithStorage(THCharStorage *storage, const char *mode)
     storage->data[0] = '\0';
   }
 
-  mfself = THAlloc(sizeof(THMemoryFile));
+  mfself = (THMemoryFile*) THAlloc(sizeof(THMemoryFile));
 
   mfself->storage = storage;
   mfself->size = (storage ? storage->size-1 : 0);
diff --git a/src/TH/THStorage.c b/src/TH/THStorage.c
index 37df9888e..40000322b 100644
--- a/src/TH/THStorage.c
+++ b/src/TH/THStorage.c
@@ -56,7 +56,7 @@ int THLongStorage_inferSize2(THLongStorage *output, int64_t *sizesA, int64_t dim
   THArgCheck(dimsB, 1, "Can't expand empty tensor b");
   ptrdiff_t ndim = dimsA > dimsB ? dimsA : dimsB;
 
-  int64_t *expandedSizes = THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedSizes = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
 
   for (int64_t i = ndim - 1; i >= 0; --i) {
     int64_t offset = ndim - 1 - i;
@@ -92,7 +92,7 @@ int THLongStorage_inferSizeN(THLongStorage *output, int n, int64_t **sizes, int6
     ndim = dims[ j ] > ndim ? dims[ j ] : ndim;
   }
 
-  int64_t *expandedSizes = THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedSizes = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
 
   for (int64_t i = ndim - 1; i >= 0; --i) {
     expandedSizes[ i ] = 1;
@@ -121,8 +121,8 @@ int THLongStorage_inferExpandGeometry(int64_t *tensorSizes, int64_t *tensorStrid
                                         char *error_buffer, int buffer_len) {
   ptrdiff_t ndim = THLongStorage_size(sizes);
 
-  int64_t *expandedSizesCalc = THAlloc(sizeof(int64_t)*ndim);
-  int64_t *expandedStridesCalc = THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedSizesCalc = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedStridesCalc = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
 
   // create a new geometry for the tensors
   for (int64_t i = ndim - 1; i >= 0; --i) {
diff --git a/src/TH/generic/THStorage.c b/src/TH/generic/THStorage.c
index 70c596e63..389f894e9 100644
--- a/src/TH/generic/THStorage.c
+++ b/src/TH/generic/THStorage.c
@@ -31,8 +31,8 @@ THStorage* THStorage_(newWithAllocator)(ptrdiff_t size,
                                         THAllocator *allocator,
                                         void *allocatorContext)
 {
-  THStorage *storage = THAlloc(sizeof(THStorage));
-  storage->data = allocator->malloc(allocatorContext, sizeof(real)*size);
+  THStorage *storage = (THStorage*) THAlloc(sizeof(THStorage));
+  storage->data = (real*) allocator->malloc(allocatorContext, sizeof(real)*size);
   storage->size = size;
   storage->refcount = 1;
   storage->flag = TH_STORAGE_REFCOUNTED | TH_STORAGE_RESIZABLE | TH_STORAGE_FREEMEM;
@@ -136,7 +136,7 @@ THStorage* THStorage_(newWithData)(real *data, ptrdiff_t size)
 THStorage* THStorage_(newWithDataAndAllocator)(real* data, ptrdiff_t size,
                                                THAllocator* allocator,
                                                void* allocatorContext) {
-  THStorage *storage = THAlloc(sizeof(THStorage));
+  THStorage *storage = (THStorage*) THAlloc(sizeof(THStorage));
   storage->data = data;
   storage->size = size;
   storage->refcount = 1;
@@ -157,7 +157,7 @@ void THStorage_(resize)(THStorage *storage, ptrdiff_t size)
       if (size == 0) {
         storage->data = NULL;
       } else {
-        storage->data = storage->allocator->malloc(
+        storage->data = (real*) storage->allocator->malloc(
             storage->allocatorContext,
             sizeof(real)*size);
       }
@@ -173,7 +173,7 @@ void THStorage_(resize)(THStorage *storage, ptrdiff_t size)
         storage->allocator->free(storage->allocatorContext, old_data);
       }
     } else {
-      storage->data = storage->allocator->realloc(
+      storage->data = (real*) storage->allocator->realloc(
               storage->allocatorContext,
               storage->data,
               sizeof(real)*size);
diff --git a/src/THC/CMakeLists.txt.hip b/src/THC/CMakeLists.txt.hip
new file mode 100644
index 000000000..f5eb2929e
--- /dev/null
+++ b/src/THC/CMakeLists.txt.hip
@@ -0,0 +1,227 @@
+# HIP_PATH
+IF(NOT DEFINED $ENV{HIP_PATH})
+  SET(HIP_PATH /opt/rocm/hip)
+ELSE()
+  SET(HIP_PATH $ENV{HIP_PATH})
+ENDIF()
+
+# HCC_PATH
+IF(NOT DEFINED $ENV{HCC_PATH})
+  SET(HCC_PATH /opt/rocm/hcc)
+ELSE()
+  SET(HCC_PATH $ENV{HCC_PATH})
+ENDIF()
+
+# HIPBLAS_PATH
+IF(NOT DEFINED $ENV{HIPBLAS_PATH})
+  SET(HIPBLAS_PATH /opt/rocm/hipblas)
+ELSE()
+  SET(HIPBLAS_PATH $ENV{HIPBLAS_PATH})
+ENDIF()
+
+# HIPRNG_PATH
+IF(NOT DEFINED $ENV{HIPRNG_PATH})
+  SET(HIPRNG_PATH /opt/rocm/hcrng)
+ELSE()
+  SET(HIPRNG_PATH $ENV{HIPRNG_PATH})
+ENDIF()
+
+# HIPSPARSE_PATH
+IF(NOT DEFINED $ENV{HIPSPARSE_PATH})
+  SET(HIPSPARSE_PATH /opt/rocm/hcsparse)
+ELSE()
+  SET(HIPSPARSE_PATH $ENV{HIPSPARSE_PATH})
+ENDIF()
+
+SET(THRUST_PATH "/root/Thrust")
+
+# load HIP cmake module and load platform id
+SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH} "${HIP_PATH}/cmake")
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig -P OUTPUT_VARIABLE PLATFORM)
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig --cpp_config OUTPUT_VARIABLE HIP_CXX_FLAGS)
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+
+SET(HIP_CXX_FLAGS "-D__HIP_PLATFORM_HCC__ -I/opt/rocm/hip/include -I/opt/rocm/hcc/include" ${HIP_CXX_FLAGS})
+
+SET(HIP_HIPCC_FLAGS "-DGENERIC_GRID_LAUNCH=1 ${CMAKE_CXX_FLAGS}")
+SET(HIP_HIPCC_FLAGS "-DGENERIC_GRID_LAUNCH=1 ${HIP_HIPCC_FLAGS}")
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+SET(CMAKE_C_COMPILER ${HIP_HIPCC_EXECUTABLE})
+SET(CMAKE_C_FLAGS "-std=c99 -Werror=implicit-function-declaration ${CMAKE_C_FLAGS}")
+SET(CMAKE_C_FLAGS "${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+SET(CMAKE_CXX_FLAGS  "-std=c++11 ${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+
+SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH})
+
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+
+MESSAGE(STATUS "ROCM TRUE:")
+MESSAGE(STATUS "CMAKE_CXX_COMPILER: " ${CMAKE_CXX_COMPILER})
+
+INCLUDE_DIRECTORIES(${HIPBLAS_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPSPARSE_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPRNG_PATH}/include)
+INCLUDE_DIRECTORIES(${THRUST_PATH})
+
+set(ATen_CUDA_INCLUDE ${ATen_CUDA_INCLUDE}
+  "${CMAKE_CURRENT_BINARY_DIR}"
+  "${CMAKE_CURRENT_SOURCE_DIR}"
+PARENT_SCOPE)
+
+CONFIGURE_FILE(THCGeneral.h.in "${CMAKE_CURRENT_BINARY_DIR}/THCGeneral.h")
+
+set(extra_src)
+# loop over all types
+foreach(THC_TYPE Byte Char Short Int Long Half Float Double)
+   # loop over files which need to be split between types (because of long compile times)
+   foreach(THC_FILE TensorSort TensorMathCompareT TensorMathPointwise TensorMathCompare TensorMathReduce TensorMasked)
+      if(NOT EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/generated/THC${THC_FILE}${THC_TYPE}.cu")
+         FILE(WRITE "${CMAKE_CURRENT_SOURCE_DIR}/generated/THC${THC_FILE}${THC_TYPE}.cu"
+              "#include \"../THC${THC_FILE}.cuh\"\n#include \"../generic/THC${THC_FILE}.cu\"\n#include \"../THCGenerate${THC_TYPE}Type.h\"\n")
+      endif()
+      LIST(APPEND extra_src "${CMAKE_CURRENT_SOURCE_DIR}/generated/THC${THC_FILE}${THC_TYPE}.cu")
+   endforeach()
+endforeach()
+
+IF(CUDA_HAS_FP16 OR NOT ${CUDA_VERSION} LESS 7.5)
+  LIST(APPEND extra_src ${CMAKE_CURRENT_SOURCE_DIR}/THCHalf.cu)
+ENDIF()
+
+set(ATen_CUDA_SRCS ${ATen_CUDA_SRCS}
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCCachingAllocator.cpp
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCCachingHostAllocator.cpp
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCGeneral.cpp
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCStorageCopy.cpp
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCStream.cpp
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensor.cpp
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorCopy.cpp
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorRandom.cpp
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCThreadLocal.cpp
+
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCReduceApplyUtils.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCBlas.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCSleep.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCStorage.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCStorageCopy.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensor.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorCopy.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorMath.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorMathBlas.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorMathMagma.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorMathPairwise.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorMathReduce.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorMathScan.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorIndex.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorConv.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorRandom.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorScatterGather.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorTopK.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorSort.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorTypeUtils.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCSortUtils.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCTensorMode.cu
+  ${extra_src}
+  PARENT_SCOPE)
+
+INSTALL(FILES
+          THC.h
+          ${CMAKE_CURRENT_BINARY_DIR}/THCGeneral.h
+          THCBlas.h
+          THCSleep.h
+          THCStorage.h
+          THCStorageCopy.h
+          THCStream.h
+          THCThreadLocal.h
+          THCTensor.h
+          THCTensorCopy.h
+          THCTensorRandom.h
+          THCTensorMath.h
+          THCTensorConv.h
+          THCApply.cuh
+          THCReduce.cuh
+          THCReduceAll.cuh
+          THCReduceApplyUtils.cuh
+          THCTensorMathReduce.cuh
+          THCAsmUtils.cuh
+          THCAtomics.cuh
+          THCScanUtils.cuh
+          THCSortUtils.cuh
+          THCAllocator.h
+          THCCachingAllocator.h
+          THCCachingHostAllocator.h
+          THCDeviceUtils.cuh
+          THCDeviceTensor.cuh
+          THCDeviceTensor-inl.cuh
+          THCDeviceTensorUtils.cuh
+          THCDeviceTensorUtils-inl.cuh
+          THCGenerateAllTypes.h
+          THCGenerateByteType.h
+          THCGenerateCharType.h
+          THCGenerateShortType.h
+          THCGenerateIntType.h
+          THCGenerateLongType.h
+          THCGenerateHalfType.h
+          THCGenerateFloatType.h
+          THCGenerateFloatTypes.h
+          THCGenerateDoubleType.h
+          THCHalf.h
+          THCNumerics.cuh
+          THCTensorSort.cuh
+          THCTensorInfo.cuh
+          THCTensorMathPointwise.cuh
+          THCTensorTypeUtils.cuh
+          THCTensorRandom.cuh
+          THCTensorMathMagma.cuh
+          THCThrustAllocator.cuh
+          THCTensorMode.cuh
+          THCTensorTopK.cuh
+          DESTINATION "${ATEN_INSTALL_INCLUDE_SUBDIR}/THC")
+
+INSTALL(FILES
+          generic/THCStorage.c
+          generic/THCStorage.cu
+          generic/THCStorage.h
+          generic/THCTensor.c
+          generic/THCTensor.cu
+          generic/THCTensor.h
+          generic/THCStorageCopy.c
+          generic/THCStorageCopy.cu
+          generic/THCStorageCopy.h
+          generic/THCTensorCopy.c
+          generic/THCTensorCopy.cu
+          generic/THCTensorCopy.h
+          generic/THCTensorMasked.h
+          generic/THCTensorMasked.cu
+          generic/THCTensorMath.h
+          generic/THCTensorMath.cu
+          generic/THCTensorMathBlas.cu
+          generic/THCTensorMathBlas.h
+          generic/THCTensorMathCompare.h
+          generic/THCTensorMathCompare.cu
+          generic/THCTensorMathCompareT.h
+          generic/THCTensorMathCompareT.cu
+          generic/THCTensorMathMagma.h
+          generic/THCTensorMathMagma.cu
+          generic/THCTensorMathPairwise.h
+          generic/THCTensorMathPairwise.cu
+          generic/THCTensorMathPointwise.h
+          generic/THCTensorMathPointwise.cu
+          generic/THCTensorMathReduce.h
+          generic/THCTensorMathReduce.cu
+          generic/THCTensorMathScan.h
+          generic/THCTensorMathScan.cu
+          generic/THCTensorScatterGather.h
+          generic/THCTensorScatterGather.cu
+          generic/THCTensorIndex.h
+          generic/THCTensorIndex.cu
+          generic/THCTensorSort.h
+          generic/THCTensorSort.cu
+          generic/THCDeviceTensorUtils.cu
+          generic/THCTensorRandom.h
+          generic/THCTensorRandom.cu
+          generic/THCTensorMode.h
+          generic/THCTensorMode.cu
+          generic/THCTensorTopK.h
+          generic/THCTensorTopK.cu
+          DESTINATION "${ATEN_INSTALL_INCLUDE_SUBDIR}/THC/generic")
diff --git a/src/THC/THCAllocator.c.hip b/src/THC/THCAllocator.c.hip
new file mode 100644
index 000000000..27a1c77bc
--- /dev/null
+++ b/src/THC/THCAllocator.c.hip
@@ -0,0 +1,68 @@
+#include "THCAllocator.h"
+
+static void *THCudaHostAllocator_malloc(void* ctx, ptrdiff_t size) {
+  void* ptr;
+
+  if (size < 0) THError("Invalid memory size: %ld", size);
+
+  if (size == 0) return NULL;
+
+  THCudaCheck(hipHostMalloc(&ptr, size));
+
+  return ptr;
+}
+
+static void THCudaHostAllocator_free(void* ctx, void* ptr) {
+  if (!ptr) return;
+
+  THCudaCheck(hipHostFree(ptr));
+}
+
+THAllocator THCudaHostAllocator = {
+  &THCudaHostAllocator_malloc,
+  NULL,
+  &THCudaHostAllocator_free
+};
+
+static hipError_t THCIpcAllocator_malloc(void* ctx, void** devPtr, size_t size, hipStream_t stream)
+{
+  THError("THCIpcAllocator.malloc() not supported");
+  return hipSuccess;
+}
+
+static hipError_t THCIpcAllocator_free(void* ctx, void* devPtr)
+{
+  return hipIpcCloseMemHandle(devPtr);
+}
+
+THCDeviceAllocator THCIpcAllocator = {
+  &THCIpcAllocator_malloc,
+  NULL,
+  &THCIpcAllocator_free,
+  NULL,
+  NULL
+};
+
+static void *THCUVAAllocator_alloc(void* ctx, ptrdiff_t size) {
+  if (size < 0) THError("Invalid memory size: %ld", size);
+
+  if (size == 0) return NULL;
+
+  // See J.1.1 of the CUDA_C_Programming_Guide.pdf for UVA and coherence rules
+  // on various compute capabilities.
+  void* ptr;
+  // TODO: HIP_EQUIVALENT
+  //THCudaCheck(hipMallocManaged(&ptr, size, hipMemAttachGlobal));
+  return ptr;
+}
+
+static void THCUVAAllocator_free(void* ctx, void* ptr) {
+  if (!ptr) return;
+  THCudaCheck(hipFree(ptr));
+}
+
+THAllocator THCUVAAllocator = {
+  &THCUVAAllocator_alloc,
+  NULL,
+  &THCUVAAllocator_free
+};
diff --git a/src/THC/THCApply.cuh.hip b/src/THC/THCApply.cuh.hip
new file mode 100644
index 000000000..54f46e90e
--- /dev/null
+++ b/src/THC/THCApply.cuh.hip
@@ -0,0 +1,878 @@
+#ifndef THC_APPLY_INC
+#define THC_APPLY_INC
+
+#include "THCTensorCopy.h"
+#include "THCReduceApplyUtils.cuh"
+#include "THCTensorTypeUtils.cuh"
+
+//
+// This file contains pointwise operation functions and kernels that
+// work on both contiguous and non-contiguous tensor arguments of
+// arbitrary (up to MAX_CUTORCH_DIMS) dimensioned arguments without
+// copying or temporary storage.
+//
+
+// Rearrange dimensions for pointwise operations so that strides are in
+// decreasing order as much as possible, so that kernels have better memory
+// access patterns.
+//
+// For example, consider a binary operation on two "transposed" 2-dim tensors:
+//    sizes:          256 512
+//    aInfo->strides:   1 256
+//    bInfo->strides:   1 256
+//
+// Given this, each concurrent memory access inside kernelPointwiseApply2() is
+// exactly 256 elements apart, resulting in poor performance.
+//
+// This function exchanges dimensions so that memory access is contiguous:
+//    sizes:          512 256
+//    aInfo->strides: 256   1
+//    bInfo->strides: 256   1
+//
+// (Actually, it becomes even better because now collapseDims() can turn each
+// input into one contiguous array.)
+//
+// In general, given M (<=3) TensorInfo's with N dimensions, we can view each
+// strides[i] (0 <= i < N) as an M-tuple.  Given each pair i < j, we exchange
+// strides[i] and [j] if
+//    (1) strides[i][k] < strides[j][k] for some k (0 <= k < M)
+//        (exchanging them will benefit input #k), and
+//    (2) strides[i][k] <= strieds[j][k] for all k
+//        (exchanging them will not make any input worse).
+template <typename T1, typename IndexType,
+          typename T2 = void, typename T3 = void>
+void rearrangeDims(TensorInfo<T1, IndexType>* aInfo,
+                   TensorInfo<T2, IndexType>* bInfo = nullptr,
+                   TensorInfo<T3, IndexType>* cInfo = nullptr) {
+  int numInfos = 1;
+  int dims = aInfo->dims;
+  IndexType *sizes[3] = { aInfo->sizes, };
+  IndexType *strides[3] = { aInfo->strides, };
+
+  if (bInfo != nullptr) {
+    ++numInfos;
+    if (bInfo->dims != dims) return;
+    sizes[1] = bInfo->sizes;
+    strides[1] = bInfo->strides;
+  }
+
+  if (cInfo != nullptr) {
+    ++numInfos;
+    if (cInfo->dims != dims) return;
+    sizes[2] = cInfo->sizes;
+    strides[2] = cInfo->strides;
+  }
+
+  // Bail out if sizes do not match: we are using "deprecated pointwise
+  // behavior" among tensors of different shapes but same number of elements.
+  for (int i = 1; i < numInfos; ++i) {
+    for (int j = 0; j < dims; ++j) {
+      if (sizes[i][j] != sizes[0][j]) return;
+    }
+  }
+
+  for (int i = 0; i < dims - 1; ++i) {
+    // No need to consider dimensions of size 1.
+    if (sizes[0][i] == 1) continue;
+
+    for (int j = i + 1; j < dims; ++j) {
+      if (sizes[0][j] == 1) continue;
+
+      // Compare the relative sizes of strides between dim #i and dim #j.
+      bool hasIncreasingStrides = false;
+      bool hasDecreasingStrides = false;
+
+      for (int k = 0; k < numInfos; k++) {
+        IndexType stride_i = strides[k][i];
+        IndexType stride_j = strides[k][j];
+        if (stride_i < stride_j) {
+          hasIncreasingStrides = true;
+        } else if (stride_i > stride_j) {
+          hasDecreasingStrides = true;
+        }
+      }
+
+      if (hasIncreasingStrides && !hasDecreasingStrides) {
+        for (int k = 0; k < numInfos; k++) {
+          IndexType size = sizes[k][i];
+          sizes[k][i] = sizes[k][j];
+          sizes[k][j] = size;
+
+          IndexType stride = strides[k][i];
+          strides[k][i] = strides[k][j];
+          strides[k][j] = stride;
+        }
+      }
+    }
+  }
+}
+
+// Threads per block for our apply kernel
+// FIXME: use occupancy calculator instead
+#define THC_APPLY_THREADS_PER_BLOCK (32 * 16)
+#define THC_APPLY_BLOCKS_PER_SM 4
+template <typename Op,
+          typename Ta,
+          typename IndexType,
+          int ADims>
+__global__ void
+kernelPointwiseApply1(const OffsetInfo<Ta, IndexType, ADims> a,
+                      IndexType totalElements,
+                      Op op) {
+  // NOTE: The two typecasts below are essential when IndexType is 64-bit;
+  //       without them, results are silently truncated to 32 bits!
+  for (IndexType linearIndex = (IndexType) blockIdx.x * blockDim.x + threadIdx.x;
+       linearIndex < totalElements;
+       linearIndex += (IndexType) gridDim.x * blockDim.x) {
+    op(a.get(linearIndex));
+  }
+}
+
+template <typename Op,
+          typename Ta, typename Tb,
+          typename IndexType,
+          int ADims, int BDims>
+__global__ void
+kernelPointwiseApply2(const OffsetInfo<Ta, IndexType, ADims> a,
+                      const OffsetInfo<Tb, IndexType, BDims> b,
+                      IndexType totalElements,
+                      Op op) {
+  for (IndexType linearIndex = (IndexType) blockIdx.x * blockDim.x + threadIdx.x;
+       linearIndex < totalElements;
+       linearIndex += (IndexType) gridDim.x * blockDim.x) {
+    op(a.get(linearIndex), b.get(linearIndex));
+  }
+}
+
+template <typename Op,
+          typename Ta, typename Tb, typename Tc,
+          typename IndexType,
+          int ADims, int BDims, int CDims>
+__global__ void
+kernelPointwiseApply3(const OffsetInfo<Ta, IndexType, ADims> a,
+                      const OffsetInfo<Tb, IndexType, BDims> b,
+                      const OffsetInfo<Tc, IndexType, CDims> c,
+                      IndexType totalElements,
+                      Op op) {
+  for (IndexType linearIndex = (IndexType) blockIdx.x * blockDim.x + threadIdx.x;
+       linearIndex < totalElements;
+       linearIndex += (IndexType) gridDim.x * blockDim.x) {
+    op(a.get(linearIndex), b.get(linearIndex), c.get(linearIndex));
+  }
+}
+
+inline dim3 getApplyBlock() {
+  return dim3(THC_APPLY_THREADS_PER_BLOCK);
+}
+
+inline bool getApplyGrid(THCState* state, uint64_t totalElements, dim3& grid) {
+  int curDevice = -1;
+  cudaGetDevice(&curDevice);
+  if (curDevice == -1) return false;
+
+  uint64_t numBlocks = THCCeilDiv(totalElements, static_cast<uint64_t>(THC_APPLY_THREADS_PER_BLOCK));
+  uint64_t maxGridX = THCState_getCurrentDeviceProperties(state)->maxGridSize[0];
+  if (numBlocks > maxGridX)
+      numBlocks = maxGridX;
+
+  // For 32-bit indices, make sure that gridDim.x * blockDim.x fits in 32 bits.
+  if (totalElements <= INT32_MAX &&
+      numBlocks > INT32_MAX / THC_APPLY_THREADS_PER_BLOCK)
+    numBlocks = INT32_MAX / THC_APPLY_THREADS_PER_BLOCK;
+
+  grid = dim3(numBlocks);
+  return true;
+}
+
+template <typename TensorTypeA,
+          typename Op>
+bool THC_pointwiseApply1(THCState* state,
+                         TensorTypeA* a,
+                         const Op& op,
+                         TensorArgType aType = ReadWrite) {
+  if (TensorUtils<TensorTypeA>::getDims(state, a) > MAX_CUTORCH_DIMS) {
+    return false;
+  }
+
+  if (TensorUtils<TensorTypeA>::getDims(state, a) == 0) {
+    // Zero-dim tensor; do nothing
+    return true;
+  }
+
+  const dim3 block = getApplyBlock();
+
+  dim3 grid;
+  ptrdiff_t totalElements = TensorUtils<TensorTypeA>::getNumElements(state, a);
+
+  if (!getApplyGrid(state, totalElements, grid)) {
+    return false;
+  }
+
+  // If tensor args have overlapping indices and are read/write, then
+  // we must expand the tensor to a contiguous form first, since
+  // otherwise there are conflicting writes. Upon copying back to the
+  // non-contiguous form, there will be conflicting writes, but at
+  // least with copy, one of the updaters will win atomically. This is
+  // a sketchy property of the old system as well (writing into all
+  // indices of a tensor with overlapping indices should probably be
+  // an error, since it is unclear which one should win), but we will
+  // preserve this last-writer-wins (in arbitrary copy order) behavior.
+  TensorTypeA* oldA = NULL;
+
+  if (aType == ReadWrite &&
+      TensorUtils<TensorTypeA>::overlappingIndices(state, a)) {
+    // Must perform in contiguous space
+    oldA = a;
+    a = TensorUtils<TensorTypeA>::newContiguous(state, a);
+  }
+
+  // It is possible that the tensor dimensions are able to be collapsed,
+  // and thus we can reduce the actual code complexity of the copy by
+  // exploiting this knowledge statically, since the div/mod is the
+  // most expensive part of the operation, more so than memory accesses.
+  // For instance, when copying a non-contiguous to a contiguous tensor
+  // (or vice versa), the contiguous tensor can be collapsed to one
+  // dimension, and the loop to translate the linear index to the array
+  // index can be similarly collapsed. That is what this unrolling is for.
+#if defined(__HIP_PLATFORM_HCC__)
+#define HANDLE_CASE(TYPE, A)                                            \
+ hipLaunchKernelGGL(                                                    \
+  (kernelPointwiseApply1<Op,                                            \
+                        typename TensorUtils<TensorTypeA>::DataType,    \
+                        TYPE, A>),                                      \
+      grid, block, 0, THCState_getCurrentStream(state),                 \
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+          (aInfo),                                                      \
+      (TYPE) totalElements, op);
+#else
+#define HANDLE_CASE(TYPE, A)                                            \
+  kernelPointwiseApply1<Op,                                             \
+                        typename TensorUtils<TensorTypeA>::DataType,    \
+                        TYPE, A>                                        \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+          (aInfo),                                                      \
+      (TYPE) totalElements, op);
+#endif
+#define HANDLE_A_CASE(TYPE, A)                  \
+  {                                             \
+    if (aInfo.isContiguous()) {                 \
+      HANDLE_CASE(TYPE, -2);                    \
+    } else {                                    \
+      switch (A) {                              \
+        case 1:                                 \
+        HANDLE_CASE(TYPE, 1);                   \
+        break;                                  \
+        case 2:                                 \
+        HANDLE_CASE(TYPE, 2);                   \
+        break;                                  \
+        default:                                \
+        HANDLE_CASE(TYPE, -1);                  \
+        break;                                  \
+      }                                         \
+    }                                           \
+  }
+
+  // Can we use 32-bit integer math in the kernel (the linear ID for the copy
+  // and the resulting non-linear offset is all computable using 32-bit math?)
+  // We also use unsigned index math in the kernel, as signed div/mod has
+  // additional overhead.
+  if (TensorUtils<TensorTypeA>::canUse32BitIndexMath(state, a)) {
+    TensorInfo<typename TensorUtils<TensorTypeA>::DataType, unsigned int> aInfo =
+      getTensorInfo<TensorTypeA, unsigned int>(state, a);
+    rearrangeDims(&aInfo);
+    aInfo.collapseDims();
+#if CUDA_VERSION < 9000
+    if (!aInfo.isContiguous())
+        grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
+#endif
+    HANDLE_A_CASE(unsigned int, aInfo.dims);
+  } else {
+    TensorInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t> aInfo =
+      getTensorInfo<TensorTypeA, uint64_t>(state, a);
+    rearrangeDims(&aInfo);
+    aInfo.collapseDims();
+
+    // For large tensors, we only compile the completely contiguous
+    // version and the completely generic version, to reduce
+    // compilation time.
+    if (aInfo.isContiguous()) {
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -2>
+        aOffset(aInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+    hipLaunchKernelGGL(
+      (kernelPointwiseApply1<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            uint64_t, -2>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, (uint64_t) totalElements, op);
+#else
+      kernelPointwiseApply1<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            uint64_t, -2>
+        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+          aOffset, (uint64_t) totalElements, op);
+#endif
+    } else {
+
+#if CUDA_VERSION < 9000
+        grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
+#endif
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -1>
+        aOffset(aInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+     hipLaunchKernelGGL(
+      (kernelPointwiseApply1<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            uint64_t, -1>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, (uint64_t) totalElements, op);
+#else
+      kernelPointwiseApply1<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            uint64_t, -1>
+        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+          aOffset, (uint64_t) totalElements, op);
+#endif
+    }
+  }
+#undef HANDLE_CASE
+#undef HANDLE_A_CASE
+
+  if (oldA) {
+    // Ignore overlaps when copying back; if we use THCTensor_copy
+    // instead, it will recursively try and invoke ourselves to make
+    // oldA contiguous.
+    TensorUtils<TensorTypeA>::copyIgnoringOverlaps(state, oldA, a);
+    TensorUtils<TensorTypeA>::free(state, a);
+    a = oldA;
+  }
+
+  return true;
+}
+
+template <typename TensorTypeA,
+          typename TensorTypeB,
+          typename Op>
+bool THC_pointwiseApply2(THCState* state,
+                         TensorTypeA* a,
+                         TensorTypeB* b,
+                         const Op& op,
+                         TensorArgType aType = ReadWrite,
+                         TensorArgType bType = ReadOnly) {
+  ptrdiff_t totalElements = TensorUtils<TensorTypeA>::getNumElements(state, a);
+
+  if (totalElements != TensorUtils<TensorTypeB>::getNumElements(state, b)) {
+    return false;
+  }
+
+  if (TensorUtils<TensorTypeA>::getDims(state, a) > MAX_CUTORCH_DIMS ||
+      TensorUtils<TensorTypeB>::getDims(state, b) > MAX_CUTORCH_DIMS) {
+    return false;
+  }
+
+  if (TensorUtils<TensorTypeA>::getDims(state, a) == 0) {
+    // Zero-dim tensor; do nothing
+    return true;
+  }
+
+  const dim3 block = getApplyBlock();
+
+  dim3 grid;
+  if (!getApplyGrid(state, totalElements, grid)) {
+    return false;
+  }
+
+  // If tensor args have overlapping indices and are read/write, then
+  // we must expand the tensor to a contiguous form first, since
+  // otherwise there are conflicting writes. Upon copying back to the
+  // non-contiguous form, there will be conflicting writes, but at
+  // least with copy, one of the updaters will win atomically. This is
+  // a sketchy property of the old system as well (writing into all
+  // indices of a tensor with overlapping indices should probably be
+  // an error, since it is unclear which one should win), but we will
+  // preserve this last-writer-wins (in arbitrary copy order) behavior.
+  TensorTypeA* oldA = NULL;
+  TensorTypeB* oldB = NULL;
+
+  if (aType == ReadWrite &&
+      TensorUtils<TensorTypeA>::overlappingIndices(state, a)) {
+    // Must perform in contiguous space
+    oldA = a;
+    a = TensorUtils<TensorTypeA>::newContiguous(state, a);
+  }
+  if (bType == ReadWrite &&
+      TensorUtils<TensorTypeB>::overlappingIndices(state, b)) {
+    // Must perform in contiguous space
+    oldB = b;
+    b = TensorUtils<TensorTypeB>::newContiguous(state, b);
+  }
+
+  // It is possible that the tensor dimensions are able to be collapsed,
+  // and thus we can reduce the actual code complexity of the copy by
+  // exploiting this knowledge statically, since the div/mod is the
+  // most expensive part of the operation, more so than memory accesses.
+  // For instance, when copying a non-contiguous to a contiguous tensor
+  // (or vice versa), the contiguous tensor can be collapsed to one
+  // dimension, and the loop to translate the linear index to the array
+  // index can be similarly collapsed. That is what this unrolling is for.
+#if defined(__HIP_PLATFORM_HCC__)
+#define HANDLE_CASE(TYPE, A, B)                                         \
+  hipLaunchKernelGGL(                                                   \
+   (kernelPointwiseApply2<Op,                                           \
+                        typename TensorUtils<TensorTypeA>::DataType,    \
+                        typename TensorUtils<TensorTypeB>::DataType,    \
+                        TYPE, A, B>),                                   \
+      grid, block, 0, THCState_getCurrentStream(state),                 \
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+          (aInfo),                                                      \
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
+          (bInfo),                                                      \
+      (TYPE) totalElements, op);
+#else
+#define HANDLE_CASE(TYPE, A, B)                                         \
+  kernelPointwiseApply2<Op,                                             \
+                        typename TensorUtils<TensorTypeA>::DataType,    \
+                        typename TensorUtils<TensorTypeB>::DataType,    \
+                        TYPE, A, B>                                     \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+          (aInfo),                                                      \
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
+          (bInfo),                                                      \
+      (TYPE) totalElements, op);
+#endif
+#define HANDLE_B_CASE(TYPE, A, B)               \
+  {                                             \
+    if (bInfo.isContiguous()) {                 \
+      HANDLE_CASE(TYPE, A, -2);                 \
+    } else {                                    \
+      switch (B) {                              \
+        case 1:                                 \
+        HANDLE_CASE(TYPE, A, 1);                \
+        break;                                  \
+        case 2:                                 \
+        HANDLE_CASE(TYPE, A, 2);                \
+        break;                                  \
+        default:                                \
+        HANDLE_CASE(TYPE, A, -1);               \
+        break;                                  \
+      }                                         \
+    }                                           \
+  }
+
+#define HANDLE_A_CASE(TYPE, A, B)               \
+  {                                             \
+    if (aInfo.isContiguous()) {                 \
+      HANDLE_B_CASE(TYPE, -2, B);               \
+    } else {                                    \
+      switch (A) {                              \
+        case 1:                                 \
+        HANDLE_B_CASE(TYPE, 1, B);              \
+        break;                                  \
+        case 2:                                 \
+        HANDLE_B_CASE(TYPE, 2, B);              \
+        break;                                  \
+        default:                                \
+        HANDLE_B_CASE(TYPE, -1, B);             \
+        break;                                  \
+      }                                         \
+    }                                           \
+  }
+
+  if (TensorUtils<TensorTypeA>::canUse32BitIndexMath(state, a) &&
+      TensorUtils<TensorTypeB>::canUse32BitIndexMath(state, b)) {
+    TensorInfo<typename TensorUtils<TensorTypeA>::DataType, unsigned int> aInfo =
+      getTensorInfo<TensorTypeA, unsigned int>(state, a);
+
+    TensorInfo<typename TensorUtils<TensorTypeB>::DataType, unsigned int> bInfo =
+      getTensorInfo<TensorTypeB, unsigned int>(state, b);
+
+    rearrangeDims(&aInfo, &bInfo);
+    aInfo.collapseDims();
+    bInfo.collapseDims();
+#if CUDA_VERSION < 9000
+    if (!(aInfo.isContiguous() && bInfo.isContiguous()))
+        grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
+#endif
+
+    HANDLE_A_CASE(unsigned int, aInfo.dims, bInfo.dims);
+  } else {
+    TensorInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t> aInfo =
+      getTensorInfo<TensorTypeA, uint64_t>(state, a);
+
+    TensorInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t> bInfo =
+      getTensorInfo<TensorTypeB, uint64_t>(state, b);
+
+    rearrangeDims(&aInfo, &bInfo);
+    aInfo.collapseDims();
+    bInfo.collapseDims();
+
+    // For large tensors, we only compile the completely contiguous
+    // version and the completely generic version, to reduce
+    // compilation time.
+    if (aInfo.isContiguous() && bInfo.isContiguous()) {
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -2>
+        aOffset(aInfo);
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t, -2>
+        bOffset(bInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL(
+      (kernelPointwiseApply2<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            uint64_t, -2, -2>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, bOffset, (uint64_t) totalElements, op);
+#else
+      kernelPointwiseApply2<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            uint64_t, -2, -2>
+        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+          aOffset, bOffset, (uint64_t) totalElements, op);
+#endif
+    } else {
+#if CUDA_VERSION < 9000
+      grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
+#endif
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -1>
+        aOffset(aInfo);
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t, -1>
+        bOffset(bInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL(
+      (kernelPointwiseApply2<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            uint64_t, -1, -1>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, bOffset, (uint64_t) totalElements, op);
+#else
+      kernelPointwiseApply2<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            uint64_t, -1, -1>
+        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+          aOffset, bOffset, (uint64_t) totalElements, op);
+#endif
+    }
+  }
+#undef HANDLE_CASE
+#undef HANDLE_B_CASE
+#undef HANDLE_A_CASE
+
+  if (oldA) {
+    // Ignore overlaps when copying back; if we use THCTensor_copy
+    // instead, it will recursively try and invoke ourselves to make
+    // oldA contiguous.
+    TensorUtils<TensorTypeA>::copyIgnoringOverlaps(state, oldA, a);
+    TensorUtils<TensorTypeA>::free(state, a);
+    a = oldA;
+  }
+
+  if (oldB) {
+    // Ignore overlaps when copying back; if we use THCTensor_copy
+    // instead, it will recursively try and invoke ourselves to make
+    // oldB contiguous.
+    TensorUtils<TensorTypeB>::copyIgnoringOverlaps(state, oldB, b);
+    TensorUtils<TensorTypeB>::free(state, b);
+    b = oldB;
+  }
+
+  return true;
+}
+
+template <typename TensorTypeA,
+          typename TensorTypeB,
+          typename TensorTypeC,
+          typename Op>
+bool THC_pointwiseApply3(THCState* state,
+                         TensorTypeA* a,
+                         TensorTypeB* b,
+                         TensorTypeC* c,
+                         const Op& op,
+                         TensorArgType aType = ReadWrite,
+                         TensorArgType bType = ReadOnly,
+                         TensorArgType cType = ReadOnly) {
+  ptrdiff_t totalElements = TensorUtils<TensorTypeA>::getNumElements(state, a);
+
+  if (totalElements != TensorUtils<TensorTypeB>::getNumElements(state, b) ||
+      totalElements != TensorUtils<TensorTypeC>::getNumElements(state, c)) {
+    return false;
+  }
+
+  if (TensorUtils<TensorTypeA>::getDims(state, a) > MAX_CUTORCH_DIMS ||
+      TensorUtils<TensorTypeB>::getDims(state, b) > MAX_CUTORCH_DIMS ||
+      TensorUtils<TensorTypeC>::getDims(state, c) > MAX_CUTORCH_DIMS) {
+    return false;
+  }
+
+  if (TensorUtils<TensorTypeA>::getDims(state, a) == 0) {
+    // Zero-dim tensor; do nothing
+    return true;
+  }
+
+  const dim3 block = getApplyBlock();
+
+  dim3 grid;
+  if (!getApplyGrid(state, totalElements, grid)) {
+    return false;
+  }
+
+  // If tensor args have overlapping indices and are read/write, then
+  // we must expand the tensor to a contiguous form first, since
+  // otherwise there are conflicting writes. Upon copying back to the
+  // non-contiguous form, there will be conflicting writes, but at
+  // least with copy, one of the updaters will win atomically. This is
+  // a sketchy property of the old system as well (writing into all
+  // indices of a tensor with overlapping indices should probably be
+  // an error, since it is unclear which one should win), but we will
+  // preserve this last-writer-wins (in arbitrary copy order) behavior.
+  TensorTypeA* oldA = NULL;
+  TensorTypeB* oldB = NULL;
+  TensorTypeC* oldC = NULL;
+
+  if (aType == ReadWrite &&
+      TensorUtils<TensorTypeA>::overlappingIndices(state, a)) {
+    // Must perform in contiguous space
+    oldA = a;
+    a = TensorUtils<TensorTypeA>::newContiguous(state, a);
+  }
+  if (bType == ReadWrite &&
+      TensorUtils<TensorTypeB>::overlappingIndices(state, b)) {
+    // Must perform in contiguous space
+    oldB = b;
+    b = TensorUtils<TensorTypeB>::newContiguous(state, b);
+  }
+  if (cType == ReadWrite &&
+      TensorUtils<TensorTypeC>::overlappingIndices(state, c)) {
+    // Must perform in contiguous space
+    oldC = c;
+    c = TensorUtils<TensorTypeC>::newContiguous(state, c);
+  }
+#if defined(__HIP_PLATFORM_HCC__)
+#define HANDLE_CASE(TYPE, A, B, C)                                      \
+hipLaunchKernelGGL(                                                     \
+  (kernelPointwiseApply3<Op,                                            \
+                        typename TensorUtils<TensorTypeA>::DataType,    \
+                        typename TensorUtils<TensorTypeB>::DataType,    \
+                        typename TensorUtils<TensorTypeC>::DataType,    \
+                        TYPE, A, B, C>),                                \
+      grid, block, 0, THCState_getCurrentStream(state),                 \
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+          (aInfo),                                                      \
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
+          (bInfo),                                                      \
+      OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, TYPE, C>  \
+          (cInfo),                                                      \
+      (TYPE) totalElements, op);
+#else
+#define HANDLE_CASE(TYPE, A, B, C)                                      \
+  kernelPointwiseApply3<Op,                                             \
+                        typename TensorUtils<TensorTypeA>::DataType,    \
+                        typename TensorUtils<TensorTypeB>::DataType,    \
+                        typename TensorUtils<TensorTypeC>::DataType,    \
+                        TYPE, A, B, C>                                  \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, TYPE, A>  \
+          (aInfo),                                                      \
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, TYPE, B>  \
+          (bInfo),                                                      \
+      OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, TYPE, C>  \
+          (cInfo),                                                      \
+      (TYPE) totalElements, op);
+#endif
+
+#define HANDLE_C_CASE(TYPE, A, B, C)            \
+  {                                             \
+    if (cInfo.isContiguous()) {                 \
+      HANDLE_CASE(TYPE, A, B, -2);              \
+    } else {                                    \
+      switch (C) {                              \
+        case 1:                                 \
+        HANDLE_CASE(TYPE, A, B, 1);             \
+        break;                                  \
+        case 2:                                 \
+        HANDLE_CASE(TYPE, A, B, 2);             \
+        break;                                  \
+        default:                                \
+        HANDLE_CASE(TYPE, A, B, -1);            \
+        break;                                  \
+      }                                         \
+    }                                           \
+  }
+
+#define HANDLE_B_CASE(TYPE, A, B, C)            \
+  {                                             \
+    if (bInfo.isContiguous()) {                 \
+      HANDLE_C_CASE(TYPE, A, -2, C);            \
+    } else {                                    \
+      switch (B) {                              \
+        case 1:                                 \
+        HANDLE_C_CASE(TYPE, A, 1, C);           \
+        break;                                  \
+        case 2:                                 \
+        HANDLE_C_CASE(TYPE, A, 2, C);           \
+        break;                                  \
+        default:                                \
+        HANDLE_C_CASE(TYPE, A, -1, C);          \
+        break;                                  \
+      }                                         \
+    }                                           \
+  }
+
+#define HANDLE_A_CASE(TYPE, A, B, C)            \
+  {                                             \
+    if (aInfo.isContiguous()) {                 \
+      HANDLE_B_CASE(TYPE, -2, B, C);            \
+    } else {                                    \
+      switch (A) {                              \
+        case 1:                                 \
+        HANDLE_B_CASE(TYPE, 1, B, C);           \
+        break;                                  \
+        case 2:                                 \
+        HANDLE_B_CASE(TYPE, 2, B, C);           \
+        break;                                  \
+        default:                                \
+        HANDLE_B_CASE(TYPE, -1, B, C);          \
+        break;                                  \
+      }                                         \
+    }                                           \
+  }
+
+  if (TensorUtils<TensorTypeA>::canUse32BitIndexMath(state, a) &&
+      TensorUtils<TensorTypeB>::canUse32BitIndexMath(state, b) &&
+      TensorUtils<TensorTypeC>::canUse32BitIndexMath(state, c)) {
+    TensorInfo<typename TensorUtils<TensorTypeA>::DataType, unsigned int> aInfo =
+      getTensorInfo<TensorTypeA, unsigned int>(state, a);
+
+    TensorInfo<typename TensorUtils<TensorTypeB>::DataType, unsigned int> bInfo =
+      getTensorInfo<TensorTypeB, unsigned int>(state, b);
+
+    TensorInfo<typename TensorUtils<TensorTypeC>::DataType, unsigned int> cInfo =
+      getTensorInfo<TensorTypeC, unsigned int>(state, c);
+
+    rearrangeDims(&aInfo, &bInfo, &cInfo);
+    aInfo.collapseDims();
+    bInfo.collapseDims();
+    cInfo.collapseDims();
+
+#if CUDA_VERSION < 9000
+      if (!(aInfo.isContiguous() && bInfo.isContiguous() && cInfo.isContiguous()))
+          grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
+#endif
+    HANDLE_A_CASE(unsigned int, aInfo.dims, bInfo.dims, cInfo.dims);
+  } else {
+    TensorInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t> aInfo =
+      getTensorInfo<TensorTypeA, uint64_t>(state, a);
+
+    TensorInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t> bInfo =
+      getTensorInfo<TensorTypeB, uint64_t>(state, b);
+
+    TensorInfo<typename TensorUtils<TensorTypeC>::DataType, uint64_t> cInfo =
+      getTensorInfo<TensorTypeC, uint64_t>(state, c);
+
+    rearrangeDims(&aInfo, &bInfo, &cInfo);
+    aInfo.collapseDims();
+    bInfo.collapseDims();
+    cInfo.collapseDims();
+
+    // For large tensors, we only compile the completely contiguous
+    // version and the completely generic version, to reduce
+    // compilation time.
+    if (aInfo.isContiguous() && bInfo.isContiguous() && cInfo.isContiguous()) {
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -2>
+        aOffset(aInfo);
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t, -2>
+        bOffset(bInfo);
+      OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, uint64_t, -2>
+        cOffset(cInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+hipLaunchKernelGGL(
+      (kernelPointwiseApply3<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            typename TensorUtils<TensorTypeC>::DataType,
+                            uint64_t, -2, -2, -2>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
+#else
+      kernelPointwiseApply3<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            typename TensorUtils<TensorTypeC>::DataType,
+                            uint64_t, -2, -2, -2>
+        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+          aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
+#endif
+    } else {
+#if CUDA_VERSION < 9000
+      grid.x = min(THCState_getCurrentDeviceProperties(state)->multiProcessorCount * THC_APPLY_BLOCKS_PER_SM , grid.x);
+#endif
+
+      OffsetInfo<typename TensorUtils<TensorTypeA>::DataType, uint64_t, -1>
+        aOffset(aInfo);
+      OffsetInfo<typename TensorUtils<TensorTypeB>::DataType, uint64_t, -1>
+        bOffset(bInfo);
+      OffsetInfo<typename TensorUtils<TensorTypeC>::DataType, uint64_t, -1>
+        cOffset(cInfo);
+#if defined(__HIP_PLATFORM_HCC__)
+hipLaunchKernelGGL(
+      (kernelPointwiseApply3<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            typename TensorUtils<TensorTypeC>::DataType,
+                            uint64_t, -1, -1, -1>),
+          grid, block, 0, THCState_getCurrentStream(state),
+          aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
+#else
+      kernelPointwiseApply3<Op,
+                            typename TensorUtils<TensorTypeA>::DataType,
+                            typename TensorUtils<TensorTypeB>::DataType,
+                            typename TensorUtils<TensorTypeC>::DataType,
+                            uint64_t, -1, -1, -1>
+        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+          aOffset, bOffset, cOffset, (uint64_t) totalElements, op);
+#endif
+    }
+  }
+#undef HANDLE_CASE
+#undef HANDLE_C_CASE
+#undef HANDLE_B_CASE
+#undef HANDLE_A_CASE
+
+  if (oldA) {
+    // Ignore overlaps when copying back; if we use THCTensor_copy
+    // instead, it will recursively try and invoke ourselves to make
+    // oldA contiguous.
+    TensorUtils<TensorTypeA>::copyIgnoringOverlaps(state, oldA, a);
+    TensorUtils<TensorTypeA>::free(state, a);
+    a = oldA;
+  }
+
+  if (oldB) {
+    // Ignore overlaps when copying back; if we use THCTensor_copy
+    // instead, it will recursively try and invoke ourselves to make
+    // oldB contiguous.
+    TensorUtils<TensorTypeB>::copyIgnoringOverlaps(state, oldB, b);
+    TensorUtils<TensorTypeB>::free(state, b);
+    b = oldB;
+  }
+
+  if (oldC) {
+    // Ignore overlaps when copying back; if we use THCTensor_copy
+    // instead, it will recursively try and invoke ourselves to make
+    // oldC contiguous.
+    TensorUtils<TensorTypeC>::copyIgnoringOverlaps(state, oldC, c);
+    TensorUtils<TensorTypeC>::free(state, c);
+    c = oldC;
+  }
+
+  return true;
+}
+
+#undef THC_APPLY_THREADS_PER_BLOCK
+#undef THC_APPLY_BLOCKS_PER_SM
+
+#endif // THC_APPLY_INC
diff --git a/src/THC/THCAsmUtils.cuh b/src/THC/THCAsmUtils.cuh
index 3b1db9a8c..550b87ef7 100644
--- a/src/THC/THCAsmUtils.cuh
+++ b/src/THC/THCAsmUtils.cuh
@@ -6,6 +6,38 @@
 template <typename T>
 struct Bitfield {};
 
+#if defined(__HIP_PLATFORM_HCC__)
+template <>
+struct Bitfield<unsigned int> {
+  static __device__
+  inline
+  unsigned int getBitfield(unsigned int val, int pos, int len)
+  {
+    pos &= 0x1f;
+    len &= 0x1f;
+
+    unsigned int m = (1u << len) - 1u;
+    m <<= pos;
+    return val & m;
+  }
+
+ static  __device__
+  inline
+  unsigned int setBitfield(
+    unsigned int val, unsigned int toInsert, int pos, int len)
+  {
+    pos &= 0x1f;
+    len &= 0x1f;
+
+    unsigned int m = (1u << len) - 1u;
+    toInsert &= m;
+    toInsert <<= pos;
+    m <<= pos;
+
+    return (val & ~m) | toInsert;
+  }
+};
+#else
 template <>
 struct Bitfield<unsigned int> {
   static __device__ __forceinline__
@@ -23,7 +55,40 @@ struct Bitfield<unsigned int> {
     return ret;
   }
 };
+#endif
 
+#if defined(__HIP_PLATFORM_HCC__)
+template<>
+struct Bitfield<uint64_t>{
+  static __device__
+  inline
+  uint64_t getBitfield(uint64_t val, int pos, int len)
+  {
+    pos &= 0x1f;
+    len &= 0x1f;
+
+    uint64_t m = (1u << len) - 1u;
+    m <<= pos;
+    return val & m;
+  }
+
+  static __device__
+  inline
+  uint64_t setBitfield(
+    uint64_t val, uint64_t toInsert, int pos, int len)
+  {
+    pos &= 0x1f;
+    len &= 0x1f;
+
+    uint64_t m = (1u << len) - 1u;
+    toInsert &= m;
+    toInsert <<= pos;
+    m <<= pos;
+
+    return (val & ~m) | toInsert;
+  }
+};
+#else
 template <>
 struct Bitfield<uint64_t> {
   static __device__ __forceinline__
@@ -41,36 +106,73 @@ struct Bitfield<uint64_t> {
     return ret;
   }
 };
+#endif
 
+#if defined(__HIP_PLATFORM_HCC__)
+__device__ __forceinline__ inline int getLaneId() {
+    return hc::__lane_id();
+#else
 __device__ __forceinline__ int getLaneId() {
   int laneId;
   asm("mov.s32 %0, %laneid;" : "=r"(laneId) );
   return laneId;
+#endif
 }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __device__ inline std::uint64_t getLaneMaskLt()
+  {
+    std::uint64_t m = (1ull << getLaneId()) - 1ull;
+    return m;
+  }
+#else
 __device__ __forceinline__ unsigned getLaneMaskLt() {
   unsigned mask;
   asm("mov.u32 %0, %%lanemask_lt;" : "=r"(mask));
   return mask;
 }
+#endif
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __device__ inline std::uint64_t getLaneMaskLe()
+  {
+    std::uint64_t m = (1ull << (getLaneId() + 1ull)) - 1ull;
+    return m;
+  }
+#else
 __device__ __forceinline__ unsigned getLaneMaskLe() {
   unsigned mask;
   asm("mov.u32 %0, %%lanemask_le;" : "=r"(mask));
   return mask;
 }
+#endif
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __device__ inline std::uint64_t getLaneMaskGt()
+  {
+    std::uint64_t m = getLaneMaskLe();
+    return m ? ~m : m;
+  }
+#else
 __device__ __forceinline__ unsigned getLaneMaskGt() {
   unsigned mask;
   asm("mov.u32 %0, %%lanemask_gt;" : "=r"(mask));
   return mask;
 }
+#endif
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __device__ inline std::uint64_t getLaneMaskGe()
+  {
+    std::uint64_t m = getLaneMaskLt();
+    return ~m;
+  }
+#else
 __device__ __forceinline__ unsigned getLaneMaskGe() {
   unsigned mask;
   asm("mov.u32 %0, %%lanemask_ge;" : "=r"(mask));
   return mask;
 }
-
+#endif
 
 #endif // THC_ASM_UTILS_INC
diff --git a/src/THC/THCAtomics.cuh b/src/THC/THCAtomics.cuh
index 9e54c56dc..f0283b996 100644
--- a/src/THC/THCAtomics.cuh
+++ b/src/THC/THCAtomics.cuh
@@ -103,7 +103,11 @@ static inline  __device__ void atomicAdd(half *address, half val) {
 
   do {
     assumed = old;
-#if CUDA_VERSION < 9000
+#if defined(__HIP_PLATFORM_HCC__)
+    half hsum;
+    hsum = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
+    hsum = THCNumerics<half>::add(hsum, val);
+#elif CUDA_VERSION < 9000
     half hsum;
     hsum.x = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
     hsum = THCNumerics<half>::add(hsum, val);
@@ -113,7 +117,11 @@ static inline  __device__ void atomicAdd(half *address, half val) {
     half tmpres = THCNumerics<half>::add(hsum, val);
     hsum = __half_raw(tmpres);
 #endif
+#if defined(__HIP_PLATFORM_HCC__)
+    // old = (size_t)address & 2 ? (old & 0xffff) | (hsum << 16) : (old & 0xffff0000) | hsum;
+#else
     old = (size_t)address & 2 ? (old & 0xffff) | (hsum.x << 16) : (old & 0xffff0000) | hsum.x;
+#endif
     old = atomicCAS(address_as_ui, assumed, old);
   } while (assumed != old);
 }
diff --git a/src/THC/THCBlas.cu.hip b/src/THC/THCBlas.cu.hip
new file mode 100644
index 000000000..8abb1639b
--- /dev/null
+++ b/src/THC/THCBlas.cu.hip
@@ -0,0 +1,520 @@
+#include "THCBlas.h"
+#include "THCGeneral.h"
+#include "THCHalf.h"
+
+
+float THCudaBlas_Sdot(THCState *state, long n, float *x, long incx, float *y, long incy)
+{
+  if (n == 1) {
+    incx = 1;
+    incy = 1;
+  }
+
+  if ((n <= INT_MAX) && (incx <= INT_MAX) && (incy <= INT_MAX)) {
+    int i_n = (int)n;
+    int i_incx = (int)incx;
+    int i_incy = (int)incy;
+    float result;
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasSdot(handle, i_n, x, i_incx, y, i_incy, &result));
+    return result;
+  }
+
+  THError("Cublas_Sdot only supports n, incx and incy "
+          "up to signed integer limits: %d", INT_MAX);
+  return 0;
+}
+
+double THCudaBlas_Ddot(THCState *state, long n, double *x, long incx, double *y, long incy)
+{
+  if (n == 1) {
+    incx = 1;
+    incy = 1;
+  }
+
+  if ((n <= INT_MAX) && (incx <= INT_MAX) && (incy <= INT_MAX)) {
+    int i_n = (int)n;
+    int i_incx = (int)incx;
+    int i_incy = (int)incy;
+    double result;
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasDdot(handle, i_n, x, i_incx, y, i_incy, &result));
+    return result;
+  }
+
+  THError("Cublas_Ddot only supports n, incx and incy "
+          "up to signed integer limits: %d", INT_MAX);
+  return 0;
+}
+
+#ifdef CUDA_HALF_TENSOR
+half THCudaBlas_Hdot(THCState *state, long n, half *x, long incx, half *y, long incy)
+{
+#if CUDA_VERSION >= 8000
+  if (n == 1) {
+    incx = 1;
+    incy = 1;
+  }
+
+  if ((n <= INT_MAX) && (incx <= INT_MAX) && (incy <= INT_MAX)) {
+    half result;
+#ifdef HIPBLAS_TODO
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasDotEx(handle, n, x, CUDA_R_16F, incx, y, CUDA_R_16F, incy, &result, CUDA_R_16F, CUDA_R_32F));
+#else
+#ifdef __NVCC__
+    cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    cublasSetStream((cublasHandle_t)handle, THCState_getCurrentStream(state));
+    cublasDotEx(handle, n, x, CUDA_R_16F, incx, y, CUDA_R_16F, incy, &result, CUDA_R_16F, CUDA_R_32F);
+#endif
+#endif
+    return result;
+   }
+
+  THError("Cublas_Hdot only supports n, incx and incy "
+          "up to signed integer limits: %d", INT_MAX);
+  return THC_float2half(0);
+#else
+  THError("Cublas_Hdot requires CUDA 8.0+");
+  return THC_float2half(0);
+#endif
+}
+#endif
+
+/* Level 2 */
+void THCudaBlas_Sgemv(THCState *state, char trans, long m, long n, float alpha, float *a, long lda, float *x, long incx, float beta, float *y, long incy)
+{
+  if(n == 1)
+    lda = m;
+
+  hipblasOperation_t op;
+  if (trans == 't') op = HIPBLAS_OP_T;
+  else if (trans == 'n') op = HIPBLAS_OP_N;
+  else if (trans == 'c') op = HIPBLAS_OP_C;
+  else THError("Cublas_Sgemv parameter trans should be 't', 'n' or 'c'.");
+
+  if( (m <= INT_MAX) && (n <= INT_MAX) &&
+      (lda > 0) && (lda <= INT_MAX) &&
+      (incx > 0) && (incx <= INT_MAX) &&
+      (incy > 0) && (incy <= INT_MAX) )
+  {
+    int i_m = (int)m;
+    int i_n = (int)n;
+    int i_lda = (int)lda;
+    int i_incx = (int)incx;
+    int i_incy = (int)incy;
+
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
+
+    return;
+  }
+  THError("Cublas_Sgemv only supports m, n, lda, incx, incy"
+          "in the range 0 < [val] <= %d", INT_MAX);
+}
+
+void THCudaBlas_Dgemv(THCState *state, char trans, long m, long n, double alpha, double *a, long lda, double *x, long incx, double beta, double *y, long incy)
+{
+  if(n == 1)
+    lda = m;
+
+  hipblasOperation_t op;
+  if (trans == 't') op = HIPBLAS_OP_T;
+  else if (trans == 'n') op = HIPBLAS_OP_N;
+  else if (trans == 'c') op = HIPBLAS_OP_C;
+  else THError("Cublas_Sgemv parameter trans should be 't', 'n' or 'c'.");
+
+  if( (m <= INT_MAX) && (n <= INT_MAX) &&
+      (lda > 0) && (lda <= INT_MAX) &&
+      (incx > 0) && (incx <= INT_MAX) &&
+      (incy > 0) && (incy <= INT_MAX) )
+  {
+    int i_m = (int)m;
+    int i_n = (int)n;
+    int i_lda = (int)lda;
+    int i_incx = (int)incx;
+    int i_incy = (int)incy;
+
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));
+    return;
+  }
+  THError("Cublas_Dgemv only supports m, n, lda, incx, incy"
+          "in the range 0 < [val] <= %d", INT_MAX);
+}
+
+void THCudaBlas_Sger(THCState *state, long m, long n, float alpha, float *x, long incx, float *y, long incy, float *a, long lda)
+{
+  if(n == 1)
+    lda = m;
+
+  if( (m <= INT_MAX) && (n <= INT_MAX) && (lda <= INT_MAX)  && (incx <= INT_MAX) && (incy <= INT_MAX) )
+    {
+      int i_m = (int)m;
+      int i_n = (int)n;
+      int i_lda = (int)lda;
+      int i_incx = (int)incx;
+      int i_incy = (int)incy;
+      hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+      hipblasSetStream(handle, THCState_getCurrentStream(state));
+      THCublasCheck(hipblasSger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
+      return;
+    }
+  THError("Cublas_Sger only supports m, n, lda, incx, incy"
+          "with the bound [val] <= %d", INT_MAX);
+}
+
+void THCudaBlas_Dger(THCState *state, long m, long n, double alpha, double *x, long incx, double *y, long incy, double *a, long lda)
+{
+  if(n == 1)
+    lda = m;
+
+  if( (m <= INT_MAX) && (n <= INT_MAX) && (lda <= INT_MAX)  && (incx <= INT_MAX) && (incy <= INT_MAX) )
+    {
+      int i_m = (int)m;
+      int i_n = (int)n;
+      int i_lda = (int)lda;
+      int i_incx = (int)incx;
+      int i_incy = (int)incy;
+#ifdef HIPBLAS_TODO
+      hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+      hipblasSetStream(handle, THCState_getCurrentStream(state));
+      THCublasCheck(hipblasDger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda));
+#else
+#ifdef __NVCC__
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream((hipblasHandle_t)handle, THCState_getCurrentStream(state));
+    hipblasDger(handle, i_m, i_n, &alpha, x, i_incx, y, i_incy, a, i_lda);
+#endif
+#endif
+      return;
+    }
+  THError("Cublas_Dger only supports m, n, lda, incx, incy"
+          "with the bound [val] <= %d", INT_MAX);
+}
+
+hipblasOperation_t convertTransToHipblasOperation(char trans) {
+  if (trans == 't') return HIPBLAS_OP_T;
+  else if (trans == 'n') return HIPBLAS_OP_N;
+  else if (trans == 'c') return HIPBLAS_OP_C;
+  else {
+    THError("trans must be one of: t, n, c");
+    return HIPBLAS_OP_T;
+  }
+}
+#ifdef __NVCC__
+cublasOperation_t convertTransToCublasOperation(char trans) {
+  if (trans == 't') return CUBLAS_OP_T;
+  else if (trans == 'n') return CUBLAS_OP_N;
+  else if (trans == 'c') return CUBLAS_OP_C;
+  else {
+    THError("trans must be one of: t, n, c");
+    return CUBLAS_OP_T;
+  }
+}
+#endif
+void adjustLd(char transa, char transb, long m, long n, long k, long *lda, long *ldb, long *ldc)
+{
+  int transa_ = ((transa == 't') || (transa == 'T'));
+  int transb_ = ((transb == 't') || (transb == 'T'));
+
+  if(n == 1)
+    *ldc = m;
+
+  if(transa_)
+  {
+    if(m == 1)
+      *lda = k;
+  }
+  else
+  {
+    if(k == 1)
+      *lda = m;
+  }
+
+  if(transb_)
+  {
+    if(k == 1)
+      *ldb = n;
+  }
+  else
+  {
+    if(n == 1)
+      *ldb = k;
+  }
+}
+
+/* Level 3 */
+void THCudaBlas_Sgemm(THCState *state, char transa, char transb, long m, long n, long k, float alpha, float *a, long lda, float *b, long ldb, float beta, float *c, long ldc)
+{
+  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+
+  if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) && (lda <= INT_MAX)  && (ldb <= INT_MAX) && (ldc <= INT_MAX) )
+  {
+    int i_m = (int)m;
+    int i_n = (int)n;
+    int i_k = (int)k;
+    int i_lda = (int)lda;
+    int i_ldb = (int)ldb;
+    int i_ldc = (int)ldc;
+
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasSgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
+    return;
+  }
+  THError("Cublas_Sgemm only supports m, n, k, lda, ldb, ldc"
+          "with the bound [val] <= %d", INT_MAX);
+}
+
+#ifdef CUDA_HALF_TENSOR
+// In CUDA 8.0, definition of data types for sgemmex changed
+#if CUDA_VERSION < 8000
+#  define CUDA_R_16F CUBLAS_DATA_HALF
+#endif
+
+void THCudaBlas_Hgemm(THCState *state, char transa, char transb, long m, long n, long k, half alpha, half *a, long lda, half *b, long ldb, half beta, half *c, long ldc)
+{
+  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+
+  if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) && (lda <= INT_MAX)  && (ldb <= INT_MAX) && (ldc <= INT_MAX) )
+  {
+    int i_m = (int)m;
+    int i_n = (int)n;
+    int i_k = (int)k;
+    int i_lda = (int)lda;
+    int i_ldb = (int)ldb;
+    int i_ldc = (int)ldc;
+
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+
+    // Check for native Hgemm support
+#ifdef __NVCC__
+    if (THC_fastHalfInstructions(state)) {
+      THCublasCheck(hipblasHgemm(handle, opa, opb,
+				i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb,
+				&beta, c, i_ldc));
+    } else {
+      // Simulated Hgemm
+      float fAlpha = THC_half2float(alpha);
+      float fBeta = THC_half2float(beta);
+
+      /*THCublasCheck(hipblasSgemmEx(handle, opa, opb,
+				  i_m, i_n, i_k, &fAlpha,
+                                  a, CUDA_R_16F, i_lda, b, CUDA_R_16F,
+				  i_ldb, &fBeta, c, CUDA_R_16F, i_ldc));*/
+    }
+#elif __HCC__
+//       hipblasHgemm(handle, opa, opb,
+// 				i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb,
+// 				&beta, c, i_ldc);
+#endif
+    return;
+  }
+  THError("Cublas_Hgemm only supports m, n, k, lda, ldb, ldc"
+          "with th bound [val] <= %d", INT_MAX);
+
+}
+#endif
+
+void THCudaBlas_Dgemm(THCState *state, char transa, char transb, long m, long n, long k, double alpha, double *a, long lda, double *b, long ldb, double beta, double *c, long ldc)
+{
+  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+
+  if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) && (lda <= INT_MAX)  && (ldb <= INT_MAX) && (ldc <= INT_MAX) )
+  {
+    int i_m = (int)m;
+    int i_n = (int)n;
+    int i_k = (int)k;
+    int i_lda = (int)lda;
+    int i_ldb = (int)ldb;
+    int i_ldc = (int)ldc;
+
+    hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+    hipblasSetStream(handle, THCState_getCurrentStream(state));
+    THCublasCheck(hipblasDgemm(handle, opa, opb, i_m, i_n, i_k, &alpha, a, i_lda, b, i_ldb, &beta, c, i_ldc));
+    return;
+  }
+  THError("Cublas_Dgemm only supports m, n, k, lda, ldb, ldc"
+          "with the bound [val] <= %d", INT_MAX);
+}
+
+#ifdef __NVCC__
+#if CUDA_VERSION >= 9010
+void THCudaBlas_HgemmStridedBatched(THCState *state, char transa, char transb, long m, long n, long k,
+                             half alpha, const half *a, long lda, long strideA, const half *b, long ldb, long strideB,
+                             half beta, half *c, long ldc, long strideC, long batchCount)
+{
+  if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
+
+  {
+    THError("Cublas_SgemmStridedBatched only supports m, n, k, lda, ldb, ldc, batchCount"
+            "with the bound [val] <= %d", INT_MAX);
+  }
+
+  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+  cublasOperation_t opa = convertTransToCublasOperation(transa);
+  cublasOperation_t opb = convertTransToCublasOperation(transb);
+
+  cublasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  cublasSetStream(handle, THCState_getCurrentStream(state));
+  float fAlpha = THC_half2float(alpha);
+  float fBeta = THC_half2float(beta);
+  THCublasCheck(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));
+  THCublasCheck(cublasGemmStridedBatchedEx(handle,
+                                   opa, opb, (int)m, (int)n, (int)k,
+                                   (void*)&fAlpha, a, CUDA_R_16F, (int)lda, strideA,
+                                   b, CUDA_R_16F, (int)ldb, strideB,
+                                   (void*)&fBeta, c, CUDA_R_16F, (int)ldc, strideC,
+                                   (int)batchCount, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP));
+  THCublasCheck(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));
+}
+#endif
+#endif
+
+void THCudaBlas_SgemmBatched(THCState *state, char transa, char transb, long m, long n, long k,
+                             float alpha, const float *a[], long lda, const float *b[], long ldb,
+                             float beta, float *c[], long ldc, long batchCount)
+{
+  if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
+  {
+    THError("Cublas_SgemmBatched only supports m, n, k, lda, ldb, ldc, batchCount"
+            "with the bound [val] <= %d", INT_MAX);
+  }
+
+  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  for (long i = 0; i < batchCount; i++) {
+    THCublasCheck(hipblasSgemm(handle,
+                              opa, opb, (int)m, (int)n, (int)k,
+                              &alpha, a[i], (int)lda, b[i], (int)ldb, &beta, c[i], (int)ldc));
+  }
+}
+
+void THCudaBlas_DgemmBatched(THCState *state, char transa, char transb, long m, long n, long k,
+                             double alpha, const double *a[], long lda, const double *b[], long ldb,
+                             double beta, double *c[], long ldc, long batchCount)
+{
+  if( (m >= INT_MAX) || (n >= INT_MAX) || (k >= INT_MAX) || (lda >= INT_MAX)  || (ldb >= INT_MAX) || (ldc >= INT_MAX) || (batchCount >= INT_MAX) )
+  {
+    THError("Cublas_DgemmBatched only supports m, n, k, lda, ldb, ldc, batchCount"
+            "with the bound [val] <= %d", INT_MAX);
+  }
+
+  adjustLd(transa, transb, m, n, k, &lda, &ldb, &ldc);
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasOperation_t opb = convertTransToHipblasOperation(transb);
+
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  for (long i = 0; i < batchCount; i++) {
+    THCublasCheck(hipblasDgemm(handle,
+                              opa, opb, (int)m, (int)n, (int)k,
+                              &alpha, a[i], (int)lda, b[i], (int)ldb, &beta, c[i], (int)ldc));
+  }
+}
+
+/* Inverse */
+void THCudaBlas_Sgetrf(THCState *state, int n, float **a, int lda, int *pivot, int *info, int batchSize) {
+  if( (n >= INT_MAX) || (lda >= INT_MAX) || (batchSize >= INT_MAX) )
+  {
+    THError("Cublas_Sgetrf only supports n, lda, batchSize"
+            "with the bound [val] <= %d", INT_MAX);
+  }
+#ifdef HIPBLAS_TODO
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasSgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
+#endif
+}
+
+void THCudaBlas_Dgetrf(THCState *state, int n, double **a, int lda, int *pivot, int *info, int batchSize) {
+  if( (n >= INT_MAX) || (lda >= INT_MAX) || (batchSize >= INT_MAX) )
+  {
+    THError("Cublas_Dgetrf only supports n, lda, batchSize"
+            "with the bound [val] <= %d", INT_MAX);
+  }
+#ifdef HIPBLAS_TODO
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasDgetrfBatched(handle, n, a, lda, pivot, info, batchSize));
+#endif
+}
+
+THC_API void THCudaBlas_Sgetrs(THCState *state, char transa, int n, int nrhs, const float **a, int lda, int *pivot, float **b, int ldb, int *info, int batchSize)
+{
+  if( (n >= INT_MAX) || (nrhs >= INT_MAX) || (lda >= INT_MAX) || (ldb >= INT_MAX) || (batchSize >= INT_MAX) )
+  {
+    THError("Cublas_Dgetrs only supports n, nrhs, lda, ldb, batchSize"
+            "with the bound [val] <= %d", INT_MAX);
+  }
+
+
+#ifdef HIPBLAS_TODO
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasSgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
+#endif
+}
+
+
+THC_API void THCudaBlas_Dgetrs(THCState *state, char transa, int n, int nrhs, const double **a, int lda, int *pivot, double **b, int ldb, int *info, int batchSize)
+{
+  if( (n >= INT_MAX) || (nrhs >= INT_MAX) || (lda >= INT_MAX) || (ldb >= INT_MAX) || (batchSize >= INT_MAX) )
+  {
+    THError("Cublas_Dgetrs only supports n, nrhs, lda, ldb, batchSize"
+            "with the bound [val] <= %d", INT_MAX);
+  }
+
+
+#ifdef HIPBLAS_TODO
+  hipblasOperation_t opa = convertTransToHipblasOperation(transa);
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasDgetrsBatched(handle, opa, n, nrhs, a, lda, pivot, b, ldb, info, batchSize));
+#endif
+}
+void THCudaBlas_Sgetri(THCState *state, int n, const float **a, int lda, int *pivot, float **c, int ldc, int *info, int batchSize) {
+
+  if( (n >= INT_MAX) || (lda >= INT_MAX)|| (ldc >= INT_MAX) || (batchSize >= INT_MAX) )
+  {
+    THError("Cublas_Sgetri only supports n, lda, ldc, batchSize"
+            "with the bound [val] <= %d", INT_MAX);
+  }
+#ifdef HIPBLAS_TODO
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasSgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
+#endif
+}
+
+void THCudaBlas_Dgetri(THCState *state, int n, const double **a, int lda, int *pivot, double **c, int ldc, int *info, int batchSize) {
+
+  if( (n >= INT_MAX) || (lda >= INT_MAX)|| (ldc >= INT_MAX) || (batchSize >= INT_MAX) )
+  {
+    THError("Cublas_Dgetri only supports n, lda, ldc, batchSize"
+            "with the bound [val] <= %d", INT_MAX);
+  }
+#ifdef HIPBLAS_TODO
+  hipblasHandle_t handle = THCState_getCurrentBlasHandle(state);
+  hipblasSetStream(handle, THCState_getCurrentStream(state));
+  THCublasCheck(hipblasDgetriBatched(handle, n, a, lda, pivot, c, ldc, info, batchSize));
+#endif
+}
diff --git a/src/THC/THCCachingAllocator.cpp b/src/THC/THCCachingAllocator.cpp
index 35631df8d..97378685b 100644
--- a/src/THC/THCCachingAllocator.cpp
+++ b/src/THC/THCCachingAllocator.cpp
@@ -547,7 +547,11 @@ THC_API std::mutex* THCCachingAllocator_getCudaFreeMutex()
 
 static inline void assertValidDevice(int device) {
   int device_count;
+#if defined(__HIP_PLATFORM_HCC__)
+  THCudaCheck(hipGetDeviceCount(&device_count));
+#else
   THCudaCheck(cudaGetDeviceCount(&device_count));
+#endif
   THAssertMsg(0 <= device && device < device_count, "Invalid device argument.");
 }
 
diff --git a/src/THC/THCDeviceTensor-inl.cuh b/src/THC/THCDeviceTensor-inl.cuh
index 22ca6c973..86907c637 100644
--- a/src/THC/THCDeviceTensor-inl.cuh
+++ b/src/THC/THCDeviceTensor-inl.cuh
@@ -182,7 +182,8 @@ template <typename T, int Dim,
 __host__ __device__ THCDeviceTensor<T, Dim, IndexT, PtrTraits>
 THCDeviceTensor<T, Dim, IndexT, PtrTraits>::transpose(int dim1,
                                                       int dim2) const {
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   // Device code
   assert(dim1 >= 0 && dim1 < Dim);
   assert(dim1 >= 0 && dim2 < Dim);
@@ -285,7 +286,8 @@ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::downcastOuter() {
   // in all of the dimensions we are collapsing (no padding in
   // them).
   bool cont = isContiguousRange(0, Dim - NewDim);
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   // Device code
   assert(cont);
 #else
@@ -336,7 +338,8 @@ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::downcastInner() {
   // in all of the dimensions we are collapsing (no padding in
   // them).
   bool cont = isContiguousRange(NewDim, Dim);
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   // Device code
   assert(cont);
 #else
@@ -404,7 +407,8 @@ template <typename T, int Dim,
           typename IndexT, template <typename U> class PtrTraits>
 void
 THCDeviceTensor<T, Dim, IndexT, PtrTraits>::zero(cudaStream_t stream) {
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   assert(isContiguous());
 #else
   if (!isContiguous()) {
diff --git a/src/THC/THCDeviceUtils.cuh b/src/THC/THCDeviceUtils.cuh
index 4ae2bee07..ede021866 100644
--- a/src/THC/THCDeviceUtils.cuh
+++ b/src/THC/THCDeviceUtils.cuh
@@ -25,7 +25,11 @@ __host__ __device__ __forceinline__ T THCRoundUp(T a, T b) {
  * For CC 3.5+, perform a load using __ldg
  */
 template <typename T>
+#if defined(__HIP_PLATFORM_HCC__)
+__device__ __forceinline__ inline T doLdg(const T* p) {
+#else
 __device__ __forceinline__ T doLdg(const T* p) {
+#endif
 #if __CUDA_ARCH__ >= 350
   return __ldg(p);
 #else
@@ -33,7 +37,11 @@ __device__ __forceinline__ T doLdg(const T* p) {
 #endif
 }
 
+#if defined(__HIP_PLATFORM_HCC__)
+__device__ __forceinline__ inline unsigned int ACTIVE_MASK()
+#else
 __device__ __forceinline__ unsigned int ACTIVE_MASK()
+#endif
 {
 #if CUDA_VERSION >= 9000
     return __activemask();
@@ -43,7 +51,11 @@ __device__ __forceinline__ unsigned int ACTIVE_MASK()
 #endif
 }
 
+#if defined(__HIP_PLATFORM_HCC__)
+__device__ __forceinline__ inline int WARP_BALLOT(int predicate, unsigned int mask = 0xffffffff)
+#else
 __device__ __forceinline__ int WARP_BALLOT(int predicate, unsigned int mask = 0xffffffff)
+#endif
 {
 #if CUDA_VERSION >= 9000
     return __ballot_sync(mask, predicate);
@@ -52,8 +64,17 @@ __device__ __forceinline__ int WARP_BALLOT(int predicate, unsigned int mask = 0x
 #endif
 }
 
+#if defined(__HIP_PLATFORM_HCC__)
+//To handle ambiguity, add a type double version.
+__device__ __forceinline__ inline double WARP_SHFL_XOR(double value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff) {
+  //(HIP doesn't support double)
+  return (double) __shfl_xor((float) value, laneMask, width);
+}
 template <typename T>
+__device__ __forceinline__ inline T WARP_SHFL_XOR(T value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff)
+#else
 __device__ __forceinline__ T WARP_SHFL_XOR(T value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff)
+#endif
 {
 #if CUDA_VERSION >= 9000
     return __shfl_xor_sync(mask, value, laneMask, width);
@@ -63,7 +84,11 @@ __device__ __forceinline__ T WARP_SHFL_XOR(T value, int laneMask, int width = wa
 }
 
 template <typename T>
+#if defined(__HIP_PLATFORM_HCC__)
+__device__ __forceinline__ inline T WARP_SHFL(T value, int srcLane, int width = warpSize, unsigned int mask = 0xffffffff)
+#else
 __device__ __forceinline__ T WARP_SHFL(T value, int srcLane, int width = warpSize, unsigned int mask = 0xffffffff)
+#endif
 {
 #if CUDA_VERSION >= 9000
     return __shfl_sync(mask, value, srcLane, width);
@@ -73,7 +98,11 @@ __device__ __forceinline__ T WARP_SHFL(T value, int srcLane, int width = warpSiz
 }
 
 template <typename T>
+#if defined(__HIP_PLATFORM_HCC__)
+__device__ __forceinline__ inline T WARP_SHFL_UP(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+#else
 __device__ __forceinline__ T WARP_SHFL_UP(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+#endif
 {
 #if CUDA_VERSION >= 9000
     return __shfl_up_sync(mask, value, delta, width);
@@ -82,8 +111,18 @@ __device__ __forceinline__ T WARP_SHFL_UP(T value, unsigned int delta, int width
 #endif
 }
 
+#if defined(__HIP_PLATFORM_HCC__)
+//To handle ambiguity, add a type double version.
+__device__ __forceinline__ inline double WARP_SHFL_DOWN(double value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+{
+  //(HIP doesn't support double)
+  return (double) __shfl_down((float) value, delta, width);
+}
 template <typename T>
+__device__ __forceinline__ inline T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+#else
 __device__ __forceinline__ T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)
+#endif
 {
 #if CUDA_VERSION >= 9000
     return __shfl_down_sync(mask, value, delta, width);
diff --git a/src/THC/THCGeneral.cpp b/src/THC/THCGeneral.cpp
index 443c7bdef..235a7d2dc 100644
--- a/src/THC/THCGeneral.cpp
+++ b/src/THC/THCGeneral.cpp
@@ -758,11 +758,11 @@ void __THCublasCheck(cublasStatus_t status, const char *file, const int line)
       case CUBLAS_STATUS_INVALID_VALUE:
         errmsg = "an invalid numeric value was used as an argument";
         break;
-
+#ifdef CUDA
       case CUBLAS_STATUS_ARCH_MISMATCH:
         errmsg = "an absent device architectural feature is required";
         break;
-
+#endif
       case CUBLAS_STATUS_MAPPING_ERROR:
         errmsg = "an access to GPU memory space failed";
         break;
@@ -803,11 +803,11 @@ void __THCusparseCheck(cusparseStatus_t status, const char *file, const int line
       case CUSPARSE_STATUS_INVALID_VALUE:
         errmsg = "an invalid numeric value was used as an argument";
         break;
-
+#ifdef CUDA_PATH
       case CUSPARSE_STATUS_ARCH_MISMATCH:
         errmsg = "an absent device architectural feature is required";
         break;
-
+#endif
       case CUSPARSE_STATUS_MAPPING_ERROR:
         errmsg = "an access to GPU memory space failed";
         break;
@@ -819,11 +819,11 @@ void __THCusparseCheck(cusparseStatus_t status, const char *file, const int line
       case CUSPARSE_STATUS_INTERNAL_ERROR:
         errmsg = "an internal operation failed";
         break;
-
+#ifdef CUDA_PATH
       case CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED:
         errmsg = "the matrix type is not supported by this function";
         break;
-
+#endif
       default:
         errmsg = "unknown error";
         break;
@@ -923,6 +923,7 @@ cudaError_t THCudaMemGetInfoCached(THCState *state,  size_t* freeBytes, size_t*
 
 half THC_float2half(float f)
 {
+#ifdef CUDA_PATH
 #if CUDA_VERSION < 9000
   half h;
   TH_float2halfbits(&f, &h.x);
@@ -932,16 +933,22 @@ half THC_float2half(float f)
   TH_float2halfbits(&f, &h_raw.x);
   return half(h_raw);
 #endif
+#else
+  half h;
+  return h;
+#endif
 }
 
 float  THC_half2float(half h)
 {
   float f;
+#ifdef CUDA_PATH
 #if CUDA_VERSION < 9000
   TH_halfbits2float(&h.x, &f);
 #else
   __half_raw h_raw(h);
   TH_halfbits2float(&h_raw.x, &f);
 #endif
+#endif
   return f;
 }
diff --git a/src/THC/THCGeneral.h.in b/src/THC/THCGeneral.h.in
index e53da8198..b8b405888 100644
--- a/src/THC/THCGeneral.h.in
+++ b/src/THC/THCGeneral.h.in
@@ -14,6 +14,33 @@
 
 #cmakedefine USE_MAGMA
 
+// Manual Serialization strategy to bypass the serialization attempted by compiler
+#if defined(__HIP_PLATFORM_HCC__)
+  #include <hip/hip_hcc.h>
+
+  template<typename T>
+    class Magic_wrapper {
+        // TODO: this is temporary, and it has the unpleasant property of
+        //       leaking memory.
+        T* p_ = nullptr;
+    public:
+        Magic_wrapper() = default;
+        explicit
+        Magic_wrapper(const T& x)
+        {
+            hipHostMalloc(&p_, sizeof(T)); new (p_) T{x};
+        }
+
+        operator const T&() const [[hc]] { return p_[0]; }
+    };
+
+  template<typename T>
+  Magic_wrapper<T> make_magic_wrapper(const T& x)
+  {
+    return Magic_wrapper<T>{x};
+  }
+#endif
+
 #ifdef __cplusplus
 # define THC_EXTERNC extern "C"
 #else
diff --git a/src/THC/THCHalf.h b/src/THC/THCHalf.h
index bb21b9d25..ceece3150 100644
--- a/src/THC/THCHalf.h
+++ b/src/THC/THCHalf.h
@@ -4,19 +4,24 @@
 #include "THCGeneral.h"
 
 /* We compile with CudaHalfTensor support if we have this: */
-#if CUDA_VERSION >= 7050 || CUDA_HAS_FP16
+#if CUDA_VERSION >= 7050 || CUDA_HAS_FP16 || defined (__HIP_PLATFORM_HCC__)
 #define CUDA_HALF_TENSOR 1
 #endif
 
 #ifdef CUDA_HALF_TENSOR
 
-#include <cuda_fp16.h>
-#include <stdint.h>
-
-#if CUDA_VERSION >= 9000
-#ifndef __cplusplus
-typedef __half_raw half;
-#endif
+#if defined (__HIP_PLATFORM_HCC__)
+  #include <cstdint>
+  #include <hip/hip_fp16.h>
+#else
+  #include <cuda_fp16.h>
+  #include <stdint.h>
+  
+  #if CUDA_VERSION >= 9000
+    #ifndef __cplusplus
+      typedef __half_raw half;
+    #endif
+  #endif
 #endif
 
 THC_EXTERNC void THCFloat2Half(THCState *state, half *out, float *in, ptrdiff_t len);
diff --git a/src/THC/THCNumerics.cuh.hip b/src/THC/THCNumerics.cuh.hip
new file mode 100644
index 000000000..4b4c06ea4
--- /dev/null
+++ b/src/THC/THCNumerics.cuh.hip
@@ -0,0 +1,961 @@
+#ifndef THC_NUMERICS_INC
+#define THC_NUMERICS_INC
+
+#include <limits.h>
+#include "hip/hip_runtime.h"
+
+#include "THCHalf.h"
+
+#include <climits>
+
+
+/// Class for numeric limits of the particular data type, which
+/// includes support for `half`.
+/// Unfortunately since `half` does not have a constructor, these have
+/// to be expressed as functions (either that or non-const statics).
+template <typename T>
+struct THCNumerics {
+};
+
+template <typename scalar_t>
+static inline __host__ __device__ scalar_t powi(scalar_t a, scalar_t b) {
+  //assert(THCNumerics<scalar_t>::ge(b, 0));
+  scalar_t result = 1;
+  while (b) {
+    if (b & 1) {
+       result *= a;
+    }
+    b /= 2;
+    a *= a;
+  }
+  return result;
+}
+
+template <>
+struct THCNumerics<uint8_t> {
+  static inline __host__ __device__ uint8_t (min)() { return 0; }
+  static inline __host__ __device__ uint8_t (max)() { return UCHAR_MAX; }
+
+  static inline __host__ __device__ bool lt(uint8_t a, uint8_t b) { return a < b; }
+  static inline __host__ __device__ bool le(uint8_t a, uint8_t b) { return a <= b; }
+  static inline __host__ __device__ bool gt(uint8_t a, uint8_t b) { return a > b; }
+  static inline __host__ __device__ bool ge(uint8_t a, uint8_t b) { return a >= b; }
+  static inline __host__ __device__ bool eq(uint8_t a, uint8_t b) { return a == b; }
+  static inline __host__ __device__ bool ne(uint8_t a, uint8_t b) { return a != b; }
+
+  static inline __host__ __device__  uint8_t neg(int8_t a) { return -a; }
+  static inline __host__ __device__  uint8_t add(uint8_t a, uint8_t b) { return a + b; }
+  static inline __host__ __device__  uint8_t mul(uint8_t a, uint8_t b) { return a * b; }
+  static inline __host__ __device__  uint8_t sub(uint8_t a, uint8_t b) { return a - b; }
+  static inline __host__ __device__  uint8_t div(uint8_t a, uint8_t b) { return a / b; }
+  static inline __host__ __device__  uint8_t abs(uint8_t a) { return a; }
+  static inline __host__ __device__  uint8_t pow(uint8_t a, uint8_t b) { return powi<uint8_t>(a, b); }
+};
+
+template <>
+struct THCNumerics<int8_t> {
+  static inline __host__ __device__ int8_t (min)() { return SCHAR_MIN; }
+  static inline __host__ __device__ int8_t (max)() { return SCHAR_MIN; }
+
+  static inline __host__ __device__ bool lt(int8_t a, int8_t b) { return a < b; }
+  static inline __host__ __device__ bool le(int8_t a, int8_t b) { return a <= b; }
+  static inline __host__ __device__ bool gt(int8_t a, int8_t b) { return a > b; }
+  static inline __host__ __device__ bool ge(int8_t a, int8_t b) { return a >= b; }
+  static inline __host__ __device__ bool eq(int8_t a, int8_t b) { return a == b; }
+  static inline __host__ __device__ bool ne(int8_t a, int8_t b) { return a != b; }
+
+  static inline __host__ __device__  int8_t neg(int8_t a) { return -a; }
+  static inline __host__ __device__  int8_t add(int8_t a, int8_t b) { return a + b; }
+  static inline __host__ __device__  int8_t mul(int8_t a, int8_t b) { return a * b; }
+  static inline __host__ __device__  int8_t sub(int8_t a, int8_t b) { return a - b; }
+  static inline __host__ __device__  int8_t div(int8_t a, int8_t b) { return a / b; }
+  static inline __host__ __device__  int8_t pow(int8_t a, int8_t b) { return powi<int8_t>(a, b); }
+  static inline __host__ int8_t abs(int8_t a) { return std::abs(a); }
+  static inline __device__ int8_t abs(int8_t a) { return a < 0 ? -a : a; }
+};
+
+template <>
+struct THCNumerics<int16_t> {
+  static inline __host__ __device__ int16_t (min)() { return SHRT_MIN; }
+  static inline __host__ __device__ int16_t (max)() { return SHRT_MAX; }
+
+  static inline __host__ __device__ bool lt(int16_t a, int16_t b) { return a < b; }
+  static inline __host__ __device__ bool le(int16_t a, int16_t b) { return a <= b; }
+  static inline __host__ __device__ bool gt(int16_t a, int16_t b) { return a > b; }
+  static inline __host__ __device__ bool ge(int16_t a, int16_t b) { return a >= b; }
+  static inline __host__ __device__ bool eq(int16_t a, int16_t b) { return a == b; }
+  static inline __host__ __device__ bool ne(int16_t a, int16_t b) { return a != b; }
+
+  static inline __host__ __device__  int16_t neg(int16_t a) { return -a; }
+  static inline __host__ __device__  int16_t add(int16_t a, int16_t b) { return a + b; }
+  static inline __host__ __device__  int16_t mul(int16_t a, int16_t b) { return a * b; }
+  static inline __host__ __device__  int16_t sub(int16_t a, int16_t b) { return a - b; }
+  static inline __host__ __device__  int16_t div(int16_t a, int16_t b) { return a / b; }
+  static inline __host__ __device__  int16_t pow(int16_t a, int16_t b) { return powi<int16_t>(a, b); }
+  static inline __host__ int16_t abs(int16_t a) { return std::abs(a); }
+  static inline __device__ int16_t abs(int16_t a) { return a < 0 ? -a : a; }
+};
+
+template <>
+struct THCNumerics<int32_t> {
+  static inline __host__ __device__ int32_t (min)() { return INT_MIN; }
+  static inline __host__ __device__ int32_t (max)() { return INT_MAX; }
+
+  static inline __host__ __device__ bool lt(int32_t a, int32_t b) { return a < b; }
+  static inline __host__ __device__ bool le(int32_t a, int32_t b) { return a <= b; }
+  static inline __host__ __device__ bool gt(int32_t a, int32_t b) { return a > b; }
+  static inline __host__ __device__ bool ge(int32_t a, int32_t b) { return a >= b; }
+  static inline __host__ __device__ bool eq(int32_t a, int32_t b) { return a == b; }
+  static inline __host__ __device__ bool ne(int32_t a, int32_t b) { return a != b; }
+
+  static inline __host__ __device__  int32_t neg(int32_t a) { return -a; }
+  static inline __host__ __device__  int32_t add(int32_t a, int32_t b) { return a + b; }
+  static inline __host__ __device__  int32_t mul(int32_t a, int32_t b) { return a * b; }
+  static inline __host__ __device__  int32_t sub(int32_t a, int32_t b) { return a - b; }
+  static inline __host__ __device__  int32_t div(int32_t a, int32_t b) { return a / b; }
+  static inline __host__ __device__  int32_t pow(int32_t a, int32_t b) { return powi<int32_t>(a, b); }
+  static inline __host__ int32_t abs(int32_t a) { return std::abs(a); }
+  static inline __device__ int32_t abs(int32_t a) { return a < 0 ? -a : a; }
+};
+
+template <>
+struct THCNumerics<int64_t> {
+  static inline __host__ __device__ int64_t (min)() { return LONG_MIN; }
+  static inline __host__ __device__ int64_t (max)() { return LONG_MAX; }
+
+  static inline __host__ __device__ bool lt(int64_t a, int64_t b) { return a < b; }
+  static inline __host__ __device__ bool le(int64_t a, int64_t b) { return a <= b; }
+  static inline __host__ __device__ bool gt(int64_t a, int64_t b) { return a > b; }
+  static inline __host__ __device__ bool ge(int64_t a, int64_t b) { return a >= b; }
+  static inline __host__ __device__ bool eq(int64_t a, int64_t b) { return a == b; }
+  static inline __host__ __device__ bool ne(int64_t a, int64_t b) { return a != b; }
+
+  static inline __host__ __device__  int64_t neg(int64_t a) { return -a; }
+  static inline __host__ __device__  int64_t add(int64_t a, int64_t b) { return a + b; }
+  static inline __host__ __device__  int64_t mul(int64_t a, int64_t b) { return a * b; }
+  static inline __host__ __device__  int64_t sub(int64_t a, int64_t b) { return a - b; }
+  static inline __host__ __device__  int64_t div(int64_t a, int64_t b) { return a / b; };
+  static inline __host__ __device__  int64_t pow(int64_t a, int64_t b) { return powi<int64_t>(a, b); }
+  static inline __host__ int64_t abs(int64_t a) { return std::abs(a); }
+  static inline __device__ int64_t abs(int64_t a) { return a < 0 ? -a : a; }
+};
+
+#ifdef CUDA_HALF_TENSOR
+template <>
+struct THCNumerics<half> {
+    __host__ __device__
+    static
+    inline
+    half (min)()
+    {
+            return -65504;
+    }
+    __host__ __device__
+    static
+    inline
+    half (max)()
+    {
+            return 65504;
+    }
+
+  __device__
+  static
+  inline
+  bool lt(half a, half b)
+  {
+        return a < b;
+  }
+  __host__
+  static
+  inline
+  bool lt(half a, half b)
+  {
+    return THC_half2float(a) < THC_half2float(b);
+  }
+
+  __device__
+  static
+  inline
+  bool le(half a, half b)
+  {
+        return a <= b;
+  }
+  __host__
+  static
+  inline
+  bool le(half a, half b)
+  {
+    return THC_half2float(a) <= THC_half2float(b);
+  }
+
+  __device__
+  static
+  inline
+  bool gt(half a, half b)
+  {
+      return a > b;
+  }
+  __host__
+  static
+  inline
+  bool gt(half a, half b)
+  {
+    return THC_half2float(a) > THC_half2float(b);
+  }
+
+  __device__
+  static
+  inline
+  bool ge(half a, half b)
+  {
+      return a >= b;
+  }
+  __host__
+  static
+  inline
+  bool ge(half a, half b)
+  {
+    return THC_half2float(a) >= THC_half2float(b);
+  }
+
+  __device__
+  static
+  inline
+  bool eq(half a, half b)
+  {
+      return a == b;
+  }
+  __host__
+  static
+  inline
+  bool eq(half a, half b)
+  {
+    return THC_half2float(a) == THC_half2float(b);
+  }
+
+  __device__
+  static
+  inline
+  bool ne(half a, half b)
+  {
+      return a != b;
+  }
+  __host__
+  static
+  inline
+  bool ne(half a, half b)
+  {
+    return THC_half2float(a) != THC_half2float(b);
+  }
+
+  __device__
+  static
+  inline
+  half exp(half a)
+  {
+      float fa = __half2float(a);
+      return __float2half(expf(fa));
+  }
+  __host__
+  static
+  inline
+  half exp(half a)
+  {
+    return THC_float2half(expf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half exp10(half a)
+  {
+      float fa = __half2float(a);
+      return __float2half(exp10f(fa));
+  }
+  __host__
+  static
+  inline
+  half exp10(half a)
+  {
+    return THC_float2half(exp10f(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half log(half a)
+  {
+      float fa = __half2float(a);
+      return __float2half(logf(fa));
+  }
+  __host__
+  static
+  inline
+  half log(half a)
+  {
+    return THC_float2half(logf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half log1p(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(log1pf(fa));
+  }
+  __host__
+  static
+  inline
+  half log1p(half a)
+  {
+    return THC_float2half(log1pf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half lgamma(half a) {
+    float fa = __half2float(a);
+    return __float2half(lgammaf(fa));
+  }
+  __host__
+  static
+  inline
+  half lgamma(half a)
+  {
+    return THC_float2half(lgamma(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half expm1(half a) {
+    float fa = __half2float(a);
+    return __float2half(expm1f(fa));
+  }
+
+  __host__
+  static
+  inline
+  half expm1(half a) {
+    return THC_float2half(expm1f(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half cos(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hcos(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(cosf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half cos(half a)
+  {
+    return THC_float2half(cosf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half sin(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hsin(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(sinf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half sin(half a)
+  {
+    return THC_float2half(sinf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half sqrt(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hsqrt(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(sqrtf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half sqrt(half a)
+  {
+    return THC_float2half(sqrtf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half rsqrt(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hrsqrt(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(rsqrtf(fa));
+    #endif
+  }
+//  __host__
+//  static
+//  inline
+//  half rsqrt(half a)
+//  {
+//    return THC_float2half(std::rsqrt(THC_half2float(a)));
+//  }
+
+  __device__
+  static
+  inline
+  half ceil(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hceil(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(ceilf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half ceil(half a)
+  {
+    return THC_float2half(ceilf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half floor(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return hfloor(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(floorf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half floor(half a)
+  {
+    return THC_float2half(floorf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half trunc(half a)
+  {
+    #ifdef CUDA_HALF_INSTRUCTIONS
+      return htrunc(a);
+    #else
+      float fa = __half2float(a);
+      return __float2half(truncf(fa));
+    #endif
+  }
+  __host__
+  static
+  inline
+  half trunc(half a)
+  {
+    return THC_float2half(truncf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half neg(half a)
+  {
+      return -a;
+  }
+  __host__
+  static
+  inline
+  half neg(half a)
+  {
+    return THC_float2half(-(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half acos(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(acosf(fa));
+  }
+  __host__
+  static
+  inline
+  half acos(half a)
+  {
+    return THC_float2half(acosf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half cosh(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(coshf(fa));
+  }
+  __host__
+  static
+  inline
+  half cosh(half a)
+  {
+    return THC_float2half(coshf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half asin(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(asinf(fa));
+  }
+  __host__
+  static
+  inline
+  half asin(half a)
+  {
+    return THC_float2half(asinf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half sinh(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(sinhf(fa));
+  }
+  __host__
+  static
+  inline
+  half sinh(half a)
+  {
+    return THC_float2half(sinhf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half tan(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(tanf(fa));
+  }
+  __host__
+  static
+  inline
+  half tan(half a)
+  {
+    return THC_float2half(tanf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half atan(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(atanf(fa));
+  }
+  __host__
+  static
+  inline
+  half atan(half a)
+  {
+    return THC_float2half(atanf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half tanh(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(tanhf(fa));
+  }
+  __host__
+  static
+  inline
+  half tanh(half a)
+  {
+    return THC_float2half(tanhf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half erf(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(erff(fa));
+  }
+  __host__
+  static
+  inline
+  half erf(half a)
+  {
+    return THC_float2half(erf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half erfinv(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(erfinvf(fa));
+  }
+  __host__
+  static
+  inline
+  half erfinv(half a)
+  {
+    return THC_float2half(erfinv(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half abs(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(fabs(fa));
+  }
+  __host__
+  static
+  inline
+  half abs(half a)
+  {
+    return THC_float2half(fabs(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half round(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(roundf(fa));
+  }
+  __host__
+  static
+  inline
+  half round(half a)
+  {
+    return THC_float2half(roundf(THC_half2float(a)));
+  }
+
+  __device__
+  static
+  inline
+  half frac(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(fa - truncf(fa));
+  }
+  __host__
+  static
+  inline
+  half frac(half a)
+  {
+    float fa = THC_half2float(a);
+    return THC_float2half(fa - floorf(fa));
+  }
+
+  __device__
+  static
+  inline
+  half cinv(half a)
+  {
+    float fa = __half2float(a);
+    return __float2half(1.0f / fa);
+  }
+  __host__
+  static
+  inline
+  half cinv(half a)
+  {
+    return THC_float2half(1.0f / THC_half2float(a));
+  }
+
+  __device__
+  static
+  inline
+  half add(half a, half b)
+  {
+      return a + b;
+  }
+  __host__
+  static
+  inline
+  half add(half a, half b)
+  {
+      return a + b;
+  }
+
+  __device__
+  static
+  inline
+  half div(half a, half b)
+  {
+      return a / b;
+  }
+  __host__
+  static
+  inline
+  half div(half a, half b)
+  {
+    return THC_float2half(THC_half2float(a) / THC_half2float(b));
+  }
+
+  __device__
+  static
+  inline
+  half mul(half a, half b)
+  {
+      return a * b;
+  }
+  __host__
+  static
+  inline
+  half mul(half a, half b)
+  {
+    return THC_float2half(THC_half2float(a) * THC_half2float(b));
+  }
+
+  __device__
+  static
+  inline
+  half sub(half a, half b)
+  {
+      return a - b;
+  }
+  __host__
+  static
+  inline
+  half sub(half a, half b)
+  {
+    return THC_float2half(THC_half2float(a) - THC_half2float(b));
+  }
+
+  __device__
+  static
+  inline
+  half pow(half a, half b)
+  {
+    float fa = __half2float(a);
+    float fb = __half2float(b);
+    return __float2half(powf(fa, fb));
+  }
+  __host__
+  static
+  inline
+  half pow(half a, half b)
+  {
+    return THC_float2half(powf(THC_half2float(a), THC_half2float(b)));
+  }
+
+  __device__
+  static
+  inline
+  half atan2(half a, half b) {
+    float fa = __half2float(a);
+    float fb = __half2float(b);
+    return __float2half(atan2f(fa, fb));
+  }
+
+  __host__
+  static
+  inline
+  half atan2(half a, half b) {
+     return THC_float2half(atan2f(THC_half2float(a), THC_half2float(b)));
+  }
+};
+#endif
+
+template <>
+struct THCNumerics<float> {
+  static inline __host__ __device__ float (min)() { return -FLT_MAX; }
+  static inline __host__ __device__ float (max)() { return FLT_MAX; }
+
+  static inline __host__ __device__ bool lt(float a, float b) { return a < b; }
+  static inline __host__ __device__ bool le(float a, float b) { return a <= b; }
+  static inline __host__ __device__ bool gt(float a, float b) { return a > b; }
+  static inline __host__ __device__ bool ge(float a, float b) { return a >= b; }
+  static inline __host__ __device__ bool eq(float a, float b) { return a == b; }
+  static inline __host__ __device__ bool ne(float a, float b) { return a != b; }
+
+  static inline __host__ __device__  float lgamma(float a) { return lgammaf(a);}
+  static inline __host__ __device__  float erfinv(float a) { return erfinvf(a);}
+  static inline __host__ __device__  float exp  (float a) { return   expf(a); }
+  static inline __host__ __device__  float exp10(float a) { return exp10f(a); }
+  static inline __host__ __device__  float log  (float a) { return   logf(a); }
+  static inline __host__ __device__  float log1p(float a) { return log1pf(a); }
+  static inline __host__ __device__  float expm1(float a) { return expm1f(a); }
+  static inline __host__ __device__  float cos  (float a) { return   cosf(a); }
+  static inline __host__ __device__  float sin  (float a) { return   sinf(a); }
+  static inline __host__ __device__  float sqrt (float a) { return  sqrtf(a); }
+  static inline __host__ __device__  float rsqrt(float a) { return rsqrtf(a); }
+  static inline __host__ __device__  float ceil (float a) { return  ceilf(a); }
+  static inline __host__ __device__  float floor(float a) { return floorf(a); }
+  static inline __host__ __device__  float trunc(float a) { return truncf(a); }
+  static inline __host__ __device__  float neg  (float a) { return        -a; }
+  static inline __host__ __device__  float acos (float a) { return  acosf(a); }
+  static inline __host__ __device__  float cosh (float a) { return  coshf(a); }
+  static inline __host__ __device__  float acosh(float a) { return acoshf(a); }
+  static inline __host__ __device__  float asin (float a) { return  asinf(a); }
+  static inline __host__ __device__  float sinh (float a) { return  sinhf(a); }
+  static inline __host__ __device__  float asinh(float a) { return asinhf(a); }
+  static inline __host__ __device__  float tan  (float a) { return   tanf(a); }
+  static inline __host__ __device__  float atan (float a) { return  atanf(a); }
+  static inline __host__ __device__  float tanh (float a) { return  tanhf(a); }
+  static inline __host__ __device__  float erf  (float a) { return   erff(a); }
+  static inline __host__ __device__  float abs  (float a) { return   fabs(a); }
+  static inline __host__ __device__  float round(float a) { return roundf(a); }
+  static inline __host__ __device__  float frac (float a) { return a - truncf(a); }
+  static inline __host__ __device__  float cinv (float a) { return 1.0f / a; }
+  static inline __host__ __device__  float add  (float a, float b) { return a + b; }
+  static inline __host__ __device__  float div  (float a, float b) { return a / b; }
+  static inline __host__ __device__  float mul  (float a, float b) { return a * b; }
+  static inline __host__ __device__  float sub  (float a, float b) { return a - b; }
+  static inline __host__ __device__  float pow  (float a, float b) { return powf(a, b); }
+  static inline __host__ __device__  float atan2(float a, float b) { return atan2f(a, b); }
+};
+
+template <>
+struct THCNumerics<double> {
+  static inline __host__ __device__ double (min)() { return -DBL_MAX; }
+  static inline __host__ __device__ double (max)() { return DBL_MAX; }
+
+  static inline __host__ __device__ bool lt(double a, double b) { return a < b; }
+  static inline __host__ __device__ bool le(double a, double b) { return a <= b; }
+  static inline __host__ __device__ bool gt(double a, double b) { return a > b; }
+  static inline __host__ __device__ bool ge(double a, double b) { return a >= b; }
+  static inline __host__ __device__ bool eq(double a, double b) { return a == b; }
+  static inline __host__ __device__ bool ne(double a, double b) { return a != b; }
+
+  static inline __host__ __device__  double lgamma(double a) { return ::lgamma(a);}
+  static inline __host__ __device__  double erfinv(double a) { return ::erfinv(a);}
+  static inline __host__ __device__  double exp  (double a) { return   ::exp(a); }
+  static inline __host__ __device__  double exp10(double a) { return ::exp10(a); }
+  static inline __host__ __device__  double log  (double a) { return   ::log(a); }
+  static inline __host__ __device__  double log1p(double a) { return ::log1p(a); }
+  static inline __host__ __device__  double expm1(double a) { return ::expm1(a); }
+  static inline __host__ __device__  double cos  (double a) { return   ::cos(a); }
+  static inline __host__ __device__  double sin  (double a) { return   ::sin(a); }
+  static inline __host__ __device__  double sqrt (double a) { return  ::sqrt(a); }
+  static inline __host__ __device__  double rsqrt(double a) { return ::rsqrt(a); }
+  static inline __host__ __device__  double ceil (double a) { return  ::ceil(a); }
+  static inline __host__ __device__  double floor(double a) { return ::floor(a); }
+  static inline __host__ __device__  double trunc(double a) { return ::trunc(a); }
+  static inline __host__ __device__  double neg  (double a) { return         -a; }
+  static inline __host__ __device__  double acos (double a) { return  ::acos(a); }
+  static inline __host__ __device__  double cosh (double a) { return  ::cosh(a); }
+  static inline __host__ __device__  double acosh(double a) { return ::acosh(a); }
+  static inline __host__ __device__  double asin (double a) { return  ::asin(a); }
+  static inline __host__ __device__  double sinh (double a) { return  ::sinh(a); }
+  static inline __host__ __device__  double asinh(double a) { return ::asinh(a); }
+  static inline __host__ __device__  double tan  (double a) { return   ::tan(a); }
+  static inline __host__ __device__  double atan (double a) { return  ::atan(a); }
+  static inline __host__ __device__  double tanh (double a) { return  ::tanh(a); }
+  static inline __host__ __device__  double erf  (double a) { return   ::erf(a); }
+  static inline __host__ __device__  double abs  (double a) { return   ::fabs(a); }
+  static inline __host__ __device__  double round(double a) { return ::round(a); }
+  static inline __host__ __device__  double frac (double a) { return a - ::trunc(a); }
+  static inline __host__ __device__  double cinv (double a) { return 1.0 / a; }
+  static inline __host__ __device__  double add  (double a, double b) { return a + b; }
+  static inline __host__ __device__  double div  (double a, double b) { return a / b; }
+  static inline __host__ __device__  double mul  (double a, double b) { return a * b; }
+  static inline __host__ __device__  double sub  (double a, double b) { return a - b; }
+  static inline __host__ __device__  double pow  (double a, double b) { return ::pow(a, b); }
+  static inline __host__ __device__  double atan2(double a, double b) { return ::atan2(a, b); }
+};
+
+/// `half` has some type conversion issues associated with it, since it
+/// is a struct without a constructor/implicit conversion constructor.
+/// We use this to convert scalar values to the given type that the
+/// tensor expects.
+template<typename In, typename Out>
+struct ScalarConvert {
+  __host__ __device__
+  static
+  Out to(const In& v) { return static_cast<Out>(v); }
+};
+
+#ifdef CUDA_HALF_TENSOR
+  template<typename Out>
+  struct ScalarConvert<half, Out> {
+    __device__
+    static
+    Out to(half v)
+    {
+        return static_cast<Out>(v);
+    }
+
+    __host__
+    static
+    Out to(half v)
+    {
+      return static_cast<Out>(THC_half2float(v));
+    }
+  };
+
+  template <typename In>
+  struct ScalarConvert<In, half> {
+    __device__
+    static
+    half to(In v)
+    {
+        return static_cast<half>(v);
+    }
+
+    __host__
+    static
+    half to(In v)
+    {
+      return THC_float2half(static_cast<float>(v));
+    }
+  };
+
+  template <>
+  struct ScalarConvert<half, half> {
+    __device__
+    static
+    half to(half v) { return v; }
+  };
+
+  template <typename T, typename U>
+    __host__ __device__ T scalar_cast(U u) {
+    return ScalarConvert<U, T>::to(u);
+  }
+#endif
+
+#endif // THC_NUMERICS_INC
diff --git a/src/THC/THCReduce.cuh b/src/THC/THCReduce.cuh
index a4dba39d5..f274625f1 100644
--- a/src/THC/THCReduce.cuh
+++ b/src/THC/THCReduce.cuh
@@ -255,10 +255,10 @@ inline bool getContigReduceGrid(ptrdiff_t elements, dim3& grid) {
 
 // Performs a reduction out[..., 0, ...] = reduce_i(modify(in[..., i, ...])) for
 // all in where i and the out's 0 are indexed at dimension `dim`
-template <typename TensorType, 
-typename ModifyOp, 
-typename ReduceOp, 
-typename ReduceAccOp, 
+template <typename TensorType,
+typename ModifyOp,
+typename ReduceOp,
+typename ReduceAccOp,
 typename AccT>
 bool THC_reduceDim(THCState* state,
                    TensorType* out,
@@ -346,35 +346,72 @@ bool THC_reduceDim(THCState* state,
   // (or vice versa), the contiguous tensor can be collapsed to one
   // dimension, and the loop to translate the linear index to the array
   // index can be similarly collapsed. That is what this unrolling is for.
-#define HANDLE_CASE(TYPE, OUT, IN)                                      \
-  if (contigReduction) {                                                \
-    kernelReduceContigDim<ModifyOp, ReduceOp, ReduceAccOp,              \
-                          typename TensorUtils<TensorType>::DataType,   \
-                          AccT,                                         \
-                          TYPE, OUT, IN>                                \
-      <<<grid, block, smemSize, THCState_getCurrentStream(state)>>>(    \
-        outInfo, inInfo, reductionSize,                                 \
-        (TYPE) outElements, init, modifyOp, reduceOp, reduceAccOp);     \
-  } else {                                                              \
-    if(block.y == 1){                                                   \
-        kernelReduceNoncontigDim<ModifyOp, ReduceOp, ReduceAccOp,       \
-                           typename TensorUtils<TensorType>::DataType,  \
-                           AccT,                                        \
-                           TYPE, OUT, IN>                               \
-        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(         \
-                       outInfo, inInfo, reductionStride, reductionSize, \
-        (TYPE) outElements, init, modifyOp, reduceOp, reduceAccOp);     \
-    }else{                                                              \
-        kernelReduceNoncontigDim_shared<ModifyOp, ReduceOp,ReduceAccOp, \
-                           typename TensorUtils<TensorType>::DataType,  \
-                           AccT,                                        \
-                           TYPE, OUT, IN>                               \
-        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(         \
-                       outInfo, inInfo, reductionStride, reductionSize, \
-                       (TYPE) outElements, init, modifyOp, reduceOp,    \
-                       reduceAccOp);                                    \
-    }                                                                   \
-  }                                                                     \
+#if defined(__HIP_PLATFORM_HCC__)
+  #define HANDLE_CASE(TYPE, OUT, IN)                                      \
+    if (contigReduction) {                                                \
+      hipLaunchKernelGGL(                                                 \
+        (kernelReduceContigDim<ModifyOp, ReduceOp, ReduceAccOp,           \
+                             typename TensorUtils<TensorType>::DataType,  \
+                             AccT,                                        \
+                             TYPE, OUT, IN>),                             \
+            grid, block, smemSize, THCState_getCurrentStream(state),      \
+            outInfo, inInfo,      \
+            reductionSize,                                                \
+            (TYPE) outElements, init, modifyOp, reduceOp, reduceAccOp);   \
+    } else {                                                              \
+      if(block.y == 1){                                                   \
+        hipLaunchKernelGGL(                                               \
+          (kernelReduceNoncontigDim<ModifyOp, ReduceOp, ReduceAccOp,      \
+                             typename TensorUtils<TensorType>::DataType,  \
+                             AccT,                                        \
+                             TYPE, OUT, IN>),                             \
+            grid, block, 0, THCState_getCurrentStream(state),             \
+            outInfo, inInfo,      \
+            reductionStride, reductionSize,                               \
+            (TYPE) outElements, init, modifyOp, reduceOp, reduceAccOp);   \
+      }else{                                                              \
+        hipLaunchKernelGGL(                                               \
+          (kernelReduceNoncontigDim_shared<ModifyOp, ReduceOp, ReduceAccOp, \
+                             typename TensorUtils<TensorType>::DataType,  \
+                             AccT,                                        \
+                             TYPE, OUT, IN>),                             \
+          grid, block, 0, THCState_getCurrentStream(state),               \
+          outInfo, inInfo,        \
+          reductionStride, reductionSize,                                 \
+          (TYPE) outElements, init, modifyOp, reduceOp, reduceAccOp);     \
+      }                                                                   \
+    }
+#else
+  #define HANDLE_CASE(TYPE, OUT, IN)                                      \
+    if (contigReduction) {                                                \
+      kernelReduceContigDim<ModifyOp, ReduceOp, ReduceAccOp,              \
+                            typename TensorUtils<TensorType>::DataType,   \
+                            AccT,                                         \
+                            TYPE, OUT, IN>                                \
+        <<<grid, block, smemSize, THCState_getCurrentStream(state)>>>(    \
+          outInfo, inInfo, reductionSize,                                 \
+          (TYPE) outElements, init, modifyOp, reduceOp, reduceAccOp);     \
+    } else {                                                              \
+      if(block.y == 1){                                                   \
+          kernelReduceNoncontigDim<ModifyOp, ReduceOp, ReduceAccOp,       \
+                             typename TensorUtils<TensorType>::DataType,  \
+                             AccT,                                        \
+                             TYPE, OUT, IN>                               \
+          <<<grid, block, 0, THCState_getCurrentStream(state)>>>(         \
+                         outInfo, inInfo, reductionStride, reductionSize, \
+          (TYPE) outElements, init, modifyOp, reduceOp, reduceAccOp);     \
+      }else{                                                              \
+          kernelReduceNoncontigDim_shared<ModifyOp, ReduceOp,ReduceAccOp, \
+                             typename TensorUtils<TensorType>::DataType,  \
+                             AccT,                                        \
+                             TYPE, OUT, IN>                               \
+          <<<grid, block, 0, THCState_getCurrentStream(state)>>>(         \
+                         outInfo, inInfo, reductionStride, reductionSize, \
+                         (TYPE) outElements, init, modifyOp, reduceOp,    \
+                         reduceAccOp);                                    \
+      }                                                                   \
+    }
+#endif
 
 #define HANDLE_IN_CASE(TYPE, OUT, IN)                     \
   {                                                       \
diff --git a/src/THC/THCStream.cpp b/src/THC/THCStream.cpp
index 49fe680a3..25c57371b 100644
--- a/src/THC/THCStream.cpp
+++ b/src/THC/THCStream.cpp
@@ -37,7 +37,9 @@ THCStream* THCStream_newWithPriority(int flags, int priority)
   THCStream* self = (THCStream*) malloc(sizeof(THCStream));
   self->refcount = 1;
   THCudaCheck(cudaGetDevice(&self->device));
+#if !defined(__HIP_PLATFORM_HCC__)
   THCudaCheck(cudaStreamCreateWithPriority(&self->stream, flags, priority));
+#endif
   return self;
 }
 
diff --git a/src/THC/THCStream.h b/src/THC/THCStream.h
index 6ccb05720..ebdff1c7e 100644
--- a/src/THC/THCStream.h
+++ b/src/THC/THCStream.h
@@ -1,7 +1,11 @@
 #ifndef THC_STREAM_INC
 #define THC_STREAM_INC
 
+#if defined(__HIP_PLATFORM_HCC__)
+#include <hip/hip_runtime_api.h>
+#else
 #include <cuda_runtime_api.h>
+#endif
 #include "THCGeneral.h"
 
 struct THCStream
diff --git a/src/THC/THCTensorIndex.cu b/src/THC/THCTensorIndex.cu
index ac0065afb..e880fff88 100644
--- a/src/THC/THCTensorIndex.cu
+++ b/src/THC/THCTensorIndex.cu
@@ -14,6 +14,7 @@
 #include "THCTensorSort.cuh"
 #include <thrust/device_ptr.h>
 #include <thrust/sort.h>
+#include <thrust/execution_policy.h>
 #include <algorithm> // for std::min
 
 // We prefer this kernel to avoid reloading index points if the number
@@ -39,8 +40,9 @@ __global__ void indexCopySmallIndex(TensorInfo<T, IndexType> dst,
     // Lua indices begin at 1
     IndexType dstIndex =
       indices.data[IndexToOffset<int64_t, IndexType, IdxDim>::get(srcIndex, indices)] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(dstIndex < dstCopyDimSize);
-
+#endif
     // We stride over the output ignoring the indexed dimension
     // (innerSize), whose offset calculation is handled differently
     for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;
@@ -94,8 +96,9 @@ __global__ void indexCopyLargeIndex(TensorInfo<T, IndexType> dst,
     // Lua indices begin at 1
     IndexType dstIndex =
       indices.data[IndexToOffset<int64_t, IndexType, IdxDim>::get(srcIndex, indices)] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(dstIndex < dstCopyDimSize);
-
+#endif
     IndexType dstOffset =
       IndexToOffset<T, IndexType, DstDim>::get(elementInSlice, dst);
     dstOffset += dstIndex * dst.strides[dstCopyDim];
@@ -131,8 +134,9 @@ __global__ void indexAddSmallIndex(TensorInfo<T, IndexType> dst,
     // Lua indices begin at 1
     IndexType dstIndex =
       indices.data[IndexToOffset<int64_t, IndexType, IdxDim>::get(srcIndex, indices)] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(dstIndex < dstAddDimSize);
-
+#endif
     // We stride over the output ignoring the indexed dimension
     // (innerSize), whose offset calculation is handled differently
     for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;
@@ -185,8 +189,9 @@ __global__ void indexAddLargeIndex(TensorInfo<T, IndexType> dst,
     // Lua indices begin at 1
     IndexType dstIndex =
       indices.data[IndexToOffset<int64_t, IndexType, IdxDim>::get(srcIndex, indices)] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(dstIndex < dstAddDimSize);
-
+#endif
     IndexType dstOffset =
       IndexToOffset<T, IndexType, DstDim>::get(elementInSlice, dst);
     dstOffset += dstIndex * dst.strides[dstAddDim];
@@ -221,8 +226,9 @@ __global__ void indexFillSmallIndex(TensorInfo<T, IndexType> dst,
     // Lua indices begin at 1
     IndexType dstIndex_ =
       indices.data[IndexToOffset<int64_t, IndexType, IdxDim>::get(dstIndex, indices)] - TH_INDEX_BASE;
-    assert(dstIndex_ < dstFillDimSize);
-
+#if defined(__NVCC__)
+    assert(dstIndex < dstFillDimSize);
+#endif
     // We stride over the output ignoring the indexed dimension
     // (innerSize), whose offset calculation is handled differently
     for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;
@@ -270,8 +276,9 @@ __global__ void indexFillLargeIndex(TensorInfo<T, IndexType> dst,
     // Lua indices begin at 1
     IndexType dstIndex_ =
       indices.data[IndexToOffset<int64_t, IndexType, IdxDim>::get(dstIndex, indices)] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(dstIndex_ < dstFillDimSize);
-
+#endif
     IndexType dstOffset =
       IndexToOffset<T, IndexType, DstDim>::get(elementInSlice, dst);
     dstOffset += dstIndex_ * dst.strides[dstFillDim];
@@ -303,8 +310,9 @@ __global__ void indexSelectSmallIndex(TensorInfo<T, IndexType> dst,
     // Lua indices begin at 1
     IndexType srcIndex =
       indices.data[IndexToOffset<int64_t, IndexType, IdxDim>::get(dstIndex, indices)] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(srcIndex < srcSelectDimSize);
-
+#endif
     // We stride over the output ignoring the indexed dimension
     // (innerSize), whose offset calculation is handled differently
     for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;
@@ -357,8 +365,9 @@ __global__ void indexSelectLargeIndex(TensorInfo<T, IndexType> dst,
     // Lua indices begin at 1
     IndexType srcIndex =
       indices.data[IndexToOffset<int64_t, IndexType, IdxDim>::get(dstIndex, indices)] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(srcIndex < srcSelectDimSize);
-
+#endif
     IndexType dstOffset =
       IndexToOffset<T, IndexType, DstDim>::get(elementInSlice, dst);
     dstOffset += dstIndex * dst.strides[dstSelectDim];
@@ -414,7 +423,9 @@ __device__ __forceinline__ int64_t calculateOffset(
       indexAtDim = index - nextIndex * sizeAtDim;
     }
 
+#if defined(__NVCC__)
     assert(indexAtDim < data.baseSizes[dim]);
+#endif
     offset += indexAtDim * strideAtDim;
     index = nextIndex;
   }
@@ -444,7 +455,9 @@ __device__ __forceinline__ IndexType indexToOffset(
     IndexType size)
 {
   IndexType linearIndex = static_cast<IndexType>(index);
+#if defined(__NVCC__)
   assert(linearIndex < size && linearIndex >= -size);
+#endif
   if (linearIndex < 0) {
     linearIndex += size;
   }
@@ -456,7 +469,9 @@ struct WrapIndexOp {
 
   __device__ __forceinline__ void operator()(int64_t* out, int64_t* in) {
     auto idx = *in;
+#if defined(__NVCC__)
     assert(idx < size && idx >= -size);
+#endif
     *out = idx < 0 ? idx + size : idx;
   }
 
diff --git a/src/THC/THCTensorMathCompare.cuh b/src/THC/THCTensorMathCompare.cuh
index e7e1bb560..b443a0fb2 100644
--- a/src/THC/THCTensorMathCompare.cuh
+++ b/src/THC/THCTensorMathCompare.cuh
@@ -9,62 +9,104 @@
 
 template <typename T, typename TOut>
 struct TensorLTValueOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorLTValueOp(T v) : value(v) {}
   __device__ __forceinline__ void operator()(TOut* out, T* in) {
     *out = ScalarConvert<bool, TOut>::to(THCNumerics<T>::lt(*in, value));
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorLTValueOp() {}
+#endif
+
   const T value;
 };
 
 template <typename T, typename TOut>
 struct TensorGTValueOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorGTValueOp(T v) : value(v) {}
   __device__ __forceinline__ void operator()(TOut* out, T* in) {
     *out = ScalarConvert<bool, TOut>::to(THCNumerics<T>::gt(*in, value));
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorGTValueOp() {}
+#endif
+
   const T value;
 };
 
 
 template <typename T, typename TOut>
 struct TensorLEValueOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorLEValueOp(T v) : value(v) {}
   __device__ __forceinline__ void operator()(TOut* out, T* in) {
     *out = ScalarConvert<bool, TOut>::to(THCNumerics<T>::le(*in, value));
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorLEValueOp() {}
+#endif
+
   const T value;
 };
 
 template <typename T, typename TOut>
 struct TensorGEValueOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorGEValueOp(T v) : value(v) {}
   __device__ __forceinline__ void operator()(TOut* out, T* in) {
     *out = ScalarConvert<bool, TOut>::to(THCNumerics<T>::ge(*in, value));
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorGEValueOp() {}
+#endif
+
   const T value;
 };
 
 template <typename T, typename TOut>
 struct TensorEQValueOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorEQValueOp(T v) : value(v) {}
   __device__ __forceinline__ void operator()(TOut* out, T* in) {
     *out = ScalarConvert<bool, TOut>::to(THCNumerics<T>::eq(*in, value));
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorEQValueOp() {}
+#endif
+
   const T value;
 };
 
 template <typename T, typename TOut>
 struct TensorNEValueOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorNEValueOp(T v) : value(v) {}
   __device__ __forceinline__ void operator()(TOut* out, T* in) {
     *out = ScalarConvert<bool, TOut>::to(THCNumerics<T>::ne(*in, value));
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorNEValueOp() {}
+#endif
+
   const T value;
 };
 
diff --git a/src/THC/THCTensorMathPairwise.cu b/src/THC/THCTensorMathPairwise.cu
index a4e0711dc..112338780 100644
--- a/src/THC/THCTensorMathPairwise.cu
+++ b/src/THC/THCTensorMathPairwise.cu
@@ -8,6 +8,10 @@
 
 template <typename T>
 struct TensorAddConstantOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorAddConstantOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in + val;
@@ -17,20 +21,31 @@ struct TensorAddConstantOp {
     *v += val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  ~TensorAddConstantOp() {}
+#endif
+
   const T val;
 };
 
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorAddConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined (UDA_HALF_INSTRUCTIONS)|| defined (__HIP_PLATFORM_HCC__)
+  #if defined(__HIP_PLATFORM_HCC__)
+    __host__ __device__
+    explicit
+  #endif
   TensorAddConstantOp(half v) : val(v) {}
 #else
   TensorAddConstantOp(half v) : fval(THC_half2float(v)) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined (__HIP_PLATFORM_HCC__)
+    *out = *in + val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hadd(*in, val);
 #else
     float fin = __half2float(*in);
@@ -40,7 +55,9 @@ struct TensorAddConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined (__HIP_PLATFORM_HCC__)
+    *v += val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hadd(*v, val);
 #else
     float fv = __half2float(*v);
@@ -49,7 +66,7 @@ struct TensorAddConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -60,6 +77,10 @@ struct TensorAddConstantOp<half> {
 
 template <typename T>
 struct TensorSubConstantOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorSubConstantOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in - val;
@@ -69,6 +90,11 @@ struct TensorSubConstantOp {
     *v -= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  ~TensorSubConstantOp() {}
+#endif
+
   const T val;
 };
 
@@ -76,14 +102,20 @@ struct TensorSubConstantOp {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorSubConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+  TensorSubConstantOp(half v) : val{v} {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
   TensorSubConstantOp(half v): val(THC_float2half(-(THC_half2float(v)))) {}
 #else
   TensorSubConstantOp(half v): fval(-(THC_half2float(v))) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  *out = *in + val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hadd(*in, val);
 #else
     float fin = __half2float(*in);
@@ -93,7 +125,9 @@ struct TensorSubConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *v += val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hadd(*v, val);
 #else
     float fv = __half2float(*v);
@@ -102,7 +136,7 @@ struct TensorSubConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -113,6 +147,10 @@ struct TensorSubConstantOp<half> {
 
 template <typename T>
 struct TensorMulConstantOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorMulConstantOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in * val;
@@ -122,20 +160,30 @@ struct TensorMulConstantOp {
     *v *= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  ~TensorMulConstantOp() {}
+#endif
   const T val;
 };
 
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorMulConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+  TensorMulConstantOp(half v) : val(v) {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
   TensorMulConstantOp(half v) : val(v) {}
 #else
   TensorMulConstantOp(half v) : fval(THC_half2float(v)) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *out = *in * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hmul(*in, val);
 #else
     float fin = __half2float(*in);
@@ -145,7 +193,9 @@ struct TensorMulConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *v = *v * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hmul(*v, val);
 #else
     float fv = __half2float(*v);
@@ -154,7 +204,7 @@ struct TensorMulConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -164,6 +214,10 @@ struct TensorMulConstantOp<half> {
 
 template <typename T>
 struct TensorDivConstantOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorDivConstantOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in / val;
@@ -173,11 +227,21 @@ struct TensorDivConstantOp {
     *v /= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  ~TensorDivConstantOp() {}
+#endif
+
   const T val;
 };
 
+#if !defined(__HIP_PLATFORM_HCC__)
 template <>
 struct TensorDivConstantOp<float> {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorDivConstantOp(float v) : val(1.f / v) {}
   __device__ __forceinline__ void operator()(float* out, float* in) {
     *out = *in * val;
@@ -187,11 +251,20 @@ struct TensorDivConstantOp<float> {
     *v *= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  ~TensorDivConstantOp() {}
+#endif
+
   const float val;
 };
 
 template <>
 struct TensorDivConstantOp<double> {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorDivConstantOp(double v) : val(1. / v) {}
   __device__ __forceinline__ void operator()(double* out, double* in) {
     *out = *in * val;
@@ -201,19 +274,31 @@ struct TensorDivConstantOp<double> {
     *v *= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  ~TensorDivConstantOp() {}
+#endif
+
   const double val;
 };
+#endif
 
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorDivConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+  TensorDivConstantOp(half v) : val(ScalarInv<half>::to(v)) {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
   TensorDivConstantOp(half v) : val(ScalarInv<half>::to(v)) {}
 #else
   TensorDivConstantOp(half v) : fval(1.f / THC_half2float(v)) {}
 #endif
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *out = *in * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hmul(*in, val);
 #else
     float fin = __half2float(*in);
@@ -223,7 +308,9 @@ struct TensorDivConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *v = *v * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hmul(*v, val);
 #else
     float fv = __half2float(*v);
@@ -232,7 +319,7 @@ struct TensorDivConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -242,6 +329,9 @@ struct TensorDivConstantOp<half> {
 
 template <typename T>
 struct TensorRemainderOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorRemainderOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in % val;
@@ -257,11 +347,19 @@ struct TensorRemainderOp {
     }
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  ~TensorRemainderOp() {}  
+#endif
+
   const T val;
 };
 
 template <>
 struct TensorRemainderOp<float> {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorRemainderOp(float v) : val(v) {}
   __device__ __forceinline__ void operator()(float* out, float* in) {
     *out = *in - val * floorf(*in / val);
@@ -271,11 +369,18 @@ struct TensorRemainderOp<float> {
     *v = *v - val * floorf(*v / val);
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorRemainderOp() {};
+#endif
+
   const float val;
 };
 
 template <>
 struct TensorRemainderOp<double> {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorRemainderOp(double v) : val(v) {}
   __device__ __forceinline__ void operator()(double* out, double* in) {
     *out = *in - val * floor(*in / val);
@@ -285,21 +390,29 @@ struct TensorRemainderOp<double> {
     *v = *v - val * floor(*v / val);
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorRemainderOp() {};
+#endif
+
   const double val;
 };
 
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorRemainderOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
   TensorRemainderOp(half v) : val(v) {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
 #else
   TensorRemainderOp(half v): fval(THC_half2float(v)) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) 
     *out = __hsub(*in,  __hmul(val, hfloor(__hdiv(*in,  val))));
+#elif defined(__HIP_PLATFORM_HCC__)
+    *out = __hsub(*in,  __hmul(val, hfloor(hdiv(*in,  val))));
 #else
     float fin = __half2float(*in);
     float fout = fin - fval * floorf(fin / fval);
@@ -308,8 +421,10 @@ struct TensorRemainderOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hsub(*v, __hmul(val, hfloor(__hdiv(*v, val))));
+#elif defined(__HIP_PLATFORM_HCC__)
+    *v = __hsub(*v, __hmul(val, hfloor(hdiv(*v, val))));
 #else
     float fv = __half2float(*v);
     fv = fv - fval * floorf(fv / fval);
@@ -317,7 +432,7 @@ struct TensorRemainderOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -327,6 +442,9 @@ struct TensorRemainderOp<half> {
 
 template <typename T>
 struct TensorFmodOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorFmodOp(T v) : val((float)v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = (T) fmodf((float) *in, val);
@@ -341,6 +459,9 @@ struct TensorFmodOp {
 
 template <>
 struct TensorFmodOp<double> {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorFmodOp(double v) : val(v) {}
   __device__ __forceinline__ void operator()(double* out, double* in) {
     *out = fmod(*in, val);
@@ -356,7 +477,12 @@ struct TensorFmodOp<double> {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorFmodOp<half> {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  TensorFmodOp(half v): fval(v) {}
+#else
   TensorFmodOp(half v): fval(THC_half2float(v)) {}
+#endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
     *out = __float2half(fmodf(__half2float(*in), fval));
@@ -407,6 +533,9 @@ struct TensorTriOp {
 
 template <typename T>
 struct TensorLShiftConstantOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorLShiftConstantOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in << val;
@@ -416,11 +545,18 @@ struct TensorLShiftConstantOp {
     *v <<= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorLShiftConstantOp() {};
+#endif
+
   const T val;
 };
 
 template <typename T>
 struct TensorRShiftConstantOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorRShiftConstantOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in >> val;
@@ -430,11 +566,17 @@ struct TensorRShiftConstantOp {
     *v >>= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorRShiftConstantOp() {};
+#endif
   const T val;
 };
 
 template <typename T>
 struct TensorBitAndConstantOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorBitAndConstantOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in & val;
@@ -444,11 +586,17 @@ struct TensorBitAndConstantOp {
     *v &= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorBitAndConstantOp() {}
+#endif
   const T val;
 };
 
 template <typename T>
 struct TensorBitOrConstantOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorBitOrConstantOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in | val;
@@ -458,11 +606,18 @@ struct TensorBitOrConstantOp {
     *v |= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorBitOrConstantOp() {}
+#endif
+
   const T val;
 };
 
 template <typename T>
 struct TensorBitXorConstantOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorBitXorConstantOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     *out = *in ^ val;
@@ -472,6 +627,10 @@ struct TensorBitXorConstantOp {
     *v ^= val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorBitXorConstantOp() {}
+#endif
+
   const T val;
 };
 
diff --git a/src/THC/THCTensorMathPointwise.cuh b/src/THC/THCTensorMathPointwise.cuh
index 652e8fad8..1c6f4c4f0 100644
--- a/src/THC/THCTensorMathPointwise.cuh
+++ b/src/THC/THCTensorMathPointwise.cuh
@@ -149,6 +149,10 @@ struct TensorAddOp<half> {
 
 template <typename T>
 struct TensorCAddOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorCAddOp(T v) : val(v) {}
 
   __device__ __forceinline__ void operator()(T* out, T* in) {
@@ -159,12 +163,20 @@ struct TensorCAddOp {
     *out = *in1 + val * *in2;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorCAddOp() {}
+#endif
+
   T val;
 };
 
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorCAddOp<half> {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorCAddOp(half v) : val(v) {}
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
@@ -275,6 +287,10 @@ struct TensorMulOp<half> {
 
 template<typename T, int StaticExp>
 struct TensorPowOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorPowOp(T v) : val(v) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     if (StaticExp == 1) {
@@ -301,6 +317,10 @@ struct TensorPowOp {
     }
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorPowOp() {}
+#endif
+
   const T val;
 };
 
@@ -336,6 +356,10 @@ struct TensorPowOp<T, -2> {
 
 template<typename T>
 struct TensorTPowOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorTPowOp(T v) : val(v) {}
 
   __device__ __forceinline__ void operator()(T* out, T* in) {
@@ -346,6 +370,10 @@ struct TensorTPowOp {
     *v = THCNumerics<T>::pow(val, *v);
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorTPowOp() {}
+#endif
+
   const T val;
 };
 
@@ -554,6 +582,10 @@ struct TensorCFmodOp<half> {
 
 template <typename T>
 struct TensorClampOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorClampOp(T min, T max) : minValue(min), maxValue(max) {}
   __device__ __forceinline__ void operator()(T* out, T* in) {
     T val = THCNumerics<T>::lt(*in, maxValue) ? *in : maxValue;
@@ -565,12 +597,20 @@ struct TensorClampOp {
     *v = THCNumerics<T>::gt(minValue, val) ? minValue : val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorClampOp() {}
+#endif
+
   const T minValue;
   const T maxValue;
 };
 
 template <typename T>
 struct TensorLerpOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorLerpOp(T w) : w(w) {}
 
   __device__ __forceinline__ void operator()(T *out, T *a, T *b) {
@@ -583,11 +623,19 @@ struct TensorLerpOp {
     );
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorLerpOp() {}
+#endif
+
   const T w;
 };
 
 template <typename T>
 struct TensorCrossOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorCrossOp(int64_t sx, int64_t sy, int64_t so) : sx(sx), sy(sy), so(so) {}
 
   __device__ __forceinline__ void operator()(T* out, T* x, T*y) {
@@ -611,6 +659,10 @@ struct TensorCrossOp {
     out[2 * so] = val2;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorCrossOp() {}
+#endif
+
   const int64_t sx, sy, so;
 };
 
@@ -638,6 +690,10 @@ struct TensorMinOp {
 
 template <typename T>
 struct TensorMaxValueOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorMaxValueOp(T v) : val(v) {}
 
   __device__ __forceinline__ void operator()(T* out) {
@@ -648,11 +704,19 @@ struct TensorMaxValueOp {
     *out = THCNumerics<T>::gt(*in, val) ? *in : val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorMaxValueOp() {}
+#endif
+
   T val;
 };
 
 template <typename T>
 struct TensorMinValueOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorMinValueOp(T v) : val(v) {}
 
   __device__ __forceinline__ void operator()(T* out) {
@@ -663,11 +727,19 @@ struct TensorMinValueOp {
     *out = THCNumerics<T>::lt(*in, val) ? *in : val;
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorMinValueOp() {}
+#endif
+
   T val;
 };
 
 template <typename T>
 struct TensorAddCMulOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorAddCMulOp(T v) : val(v) {}
 
   __device__ __forceinline__ void operator()(T* out, T* in1, T* in2) {
@@ -680,11 +752,19 @@ struct TensorAddCMulOp {
     );
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorAddCMulOp() {}
+#endif
+
   T val;
 };
 
 template <typename T>
 struct TensorAddCDivOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+#endif
   TensorAddCDivOp(T v) : val(v) {}
 
   __device__ __forceinline__ void operator()(T* out, T* in1, T* in2) {
@@ -697,6 +777,10 @@ struct TensorAddCDivOp {
     );
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorAddCDivOp() {}
+#endif
+
   T val;
 };
 
diff --git a/src/THC/THCTensorMathReduce.cuh b/src/THC/THCTensorMathReduce.cuh
index b5282d97e..7972462e4 100644
--- a/src/THC/THCTensorMathReduce.cuh
+++ b/src/THC/THCTensorMathReduce.cuh
@@ -77,18 +77,28 @@ struct ReduceMultiply<half, float> {
 
 template <typename ResT, typename ArgT>
 struct SquareFunctor {
+#if defined(__HIP_PLATFORM_HCC__)
+    __host__ __device__
+#endif
     SquareFunctor(ResT mean): mean_(mean) {}
 
     inline __device__ ResT operator()(ArgT x) const {
       return (((ResT) x) - mean_) * (((ResT) x) - mean_);
     }
 
+#if defined(__HIP_PLATFORM_HCC__)
+    __host__ __device__ ~SquareFunctor() {}
+#endif
+
     const ResT mean_;
 };
 
 #ifdef CUDA_HALF_TENSOR
 template <typename ResT>
 struct SquareFunctor<ResT, half> {
+#if defined(__HIP_PLATFORM_HCC__)
+    __host__ __device__
+#endif
     SquareFunctor(ResT mean): mean_(mean) {}
 
     inline __device__ ResT operator()(half x) const {
@@ -98,6 +108,10 @@ struct SquareFunctor<ResT, half> {
       );
     }
 
+#if defined(__HIP_PLATFORM_HCC__)
+    __host__ __device__ ~SquareFunctor() {}
+#endif
+
     const ResT mean_;
 };
 #endif // CUDA_HALF_TENSOR
@@ -105,14 +119,24 @@ struct SquareFunctor<ResT, half> {
 template <typename T>
 struct ReduceMin {
   inline __device__ T operator()(T a, T b) const {
+#if defined(__HIP_PLATFORM_HCC__)
+    T diff = THCNumerics<T>::sub(a, b);
+    return (diff < 0 ) ? a : b;
+#else
     return THCNumerics<T>::lt(a, b) ? a : b;
+#endif
   }
 };
 
 template <typename T>
 struct ReduceMax {
   inline __device__ T operator()(T a, T b) const {
+#if defined(__HIP_PLATFORM_HCC__)
+    T diff = THCNumerics<T>::sub(a, b);
+    return (diff > 0 ) ? a : b;
+#else
     return THCNumerics<T>::gt(a, b) ? a : b;
+#endif
   }
 };
 
@@ -181,7 +205,6 @@ __global__ void THCTensor_kernel_renorm(Real *data, const Real value, const ptrd
 template <typename T>
 struct TensorNonZeroOp
 {
-  TensorNonZeroOp() {}
   __host__ __device__ T operator()(T lhs) const {
     if (THCNumerics<T>::eq(lhs, ScalarConvert<float, T>::to(0.0))) {
       return ScalarConvert<int, T>::to(0);
@@ -194,6 +217,9 @@ struct TensorNonZeroOp
 template <typename T, int StaticExp>
 struct TensorNormOp
 {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorNormOp(T exp) : exponent(exp) {}
 
   __host__ __device__ T operator()(T x) const {
@@ -206,12 +232,19 @@ struct TensorNormOp
     }
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorNormOp() {}
+#endif
+
   const T exponent;
 };
 
 template <int StaticExp>
 struct TensorNormOp<double, StaticExp>
 {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorNormOp(double exp) : exponent(exp) {}
 
   __host__ __device__ double operator()(double x) const {
@@ -224,6 +257,10 @@ struct TensorNormOp<double, StaticExp>
     }
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorNormOp() {}
+#endif
+
   const double exponent;
 };
 
@@ -231,6 +268,9 @@ struct TensorNormOp<double, StaticExp>
 template <int StaticExp>
 struct TensorNormOp<half, StaticExp>
 {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorNormOp(half exp) : exponent(exp) {}
 
   __host__ __device__ half operator()(half x) const {
@@ -243,6 +283,10 @@ struct TensorNormOp<half, StaticExp>
     }
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorNormOp() {}
+#endif
+
   const half exponent;
 };
 #endif
@@ -250,6 +294,9 @@ struct TensorNormOp<half, StaticExp>
 template <typename Tacc, typename T>
 struct TensorDistOp
 {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+#endif
   TensorDistOp(Tacc exp) : exponent(exp) {}
 
   __host__ __device__ Tacc operator()(T x, T y) const {
@@ -261,6 +308,10 @@ struct TensorDistOp
     );
   }
 
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__ ~TensorDistOp() {}
+#endif
+
   const Tacc exponent;
 };
 
@@ -325,7 +376,7 @@ __global__ void THCTensor_kernel_varOuterDim(Real *tgt, Real *src_, unsigned num
             THCNumerics<Accreal>::mul(delta, delta2));
         src += num_irows;
       }
-      
+
       if (flag) {
         m2 = THCNumerics<Accreal>::div(m2, ScalarConvert<int, Accreal>::to(row_size));
       } else {
@@ -399,8 +450,8 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
    * Each block computes the var/std of blockDim.y (32) rows at once.
    * One can visualize the computation as a 16 (x) by 32 (y) grid.
    * - Each of the 32 rows of the block is responsible for the computation
-   *   of one input row. 
-   * - Each row has 16 columns; the variance computation of one input row is 
+   *   of one input row.
+   * - Each row has 16 columns; the variance computation of one input row is
    *   split between 16 threads.
    * - Each of those 16 threads handles the accumulation of 1/16 of the input
    *   row's data.
@@ -438,14 +489,14 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
 
     /*
      * We are reducing across each row of 16 threads to find the true sum of the
-     * entire input row. The warp shfl xor loop ultimately gives each thread the 
+     * entire input row. The warp shfl xor loop ultimately gives each thread the
      * true sum.
      */
     for (unsigned lane_mask = 8; lane_mask > 0; lane_mask >>= 1) {
-      local_sum = THCNumerics<Accreal>::add(local_sum, 
+      local_sum = THCNumerics<Accreal>::add(local_sum,
           WARP_SHFL_XOR((row < num_rows) ? local_sum : acc_zero, lane_mask, 16));
     }
-    Accreal true_mean = THCNumerics<Accreal>::div(local_sum, 
+    Accreal true_mean = THCNumerics<Accreal>::div(local_sum,
         ScalarConvert<int, Accreal>::to(row_size));
 
     /*
@@ -468,7 +519,7 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
      * the total sum, which is equal to the M2 for the entire input row.
      */
     for (unsigned s = 8; s >= 1; s >>= 1) {
-      adjusted_M2 = THCNumerics<Accreal>::add(adjusted_M2, 
+      adjusted_M2 = THCNumerics<Accreal>::add(adjusted_M2,
           WARP_SHFL_DOWN((row < num_rows) ? adjusted_M2 : acc_zero, s, 16));
     }
 
@@ -759,6 +810,9 @@ struct MinValuePair {
 
 template <typename T>
 struct AddOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__
+#endif
   __device__ __forceinline__ T operator()(T const &lhs, T const &rhs) {
     return THCNumerics<T>::add(lhs, rhs);
   }
@@ -766,6 +820,9 @@ struct AddOp {
 
 template <typename T>
 struct MulOp {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__
+#endif
   __device__ __forceinline__ T operator()(T const &lhs, T const &rhs) {
     return THCNumerics<T>::mul(lhs, rhs);
   }
diff --git a/src/THC/THCTensorRandom.cpp b/src/THC/THCTensorRandom.cpp
index ddccb7c5a..cad2ec698 100644
--- a/src/THC/THCTensorRandom.cpp
+++ b/src/THC/THCTensorRandom.cpp
@@ -1,8 +1,11 @@
 #include "THCTensorRandom.h"
 
 #include <random>
+#if defined(__HIP_PLATFORM_HCC__)
+#include <hiprng.h>
+#else
 #include <curand.h>
-
+#endif
 
 void initializeGenerator(THCState *state, THCGenerator* gen);
 void createGeneratorState(THCGenerator* gen, uint64_t seed);
@@ -83,7 +86,11 @@ THCGenerator* THCRandom_getGenerator(THCState* state)
   return gen;
 }
 
+#if defined(__HIP_PLATFORM_HCC__)
+hiprngStateMtgp32* THCRandom_generatorStates(struct THCState* state)
+#else
 struct curandStateMtgp32* THCRandom_generatorStates(struct THCState* state)
+#endif
 {
   return THCRandom_getGenerator(state)->gen_states;
 }
diff --git a/src/THC/THCTensorRandom.cu b/src/THC/THCTensorRandom.cu
index a179213cd..9818c756e 100644
--- a/src/THC/THCTensorRandom.cu
+++ b/src/THC/THCTensorRandom.cu
@@ -112,6 +112,26 @@ __device__ inline half half_uniform_scale_and_shift(float x, double a, double b)
 }
 #endif
 
+// Goes from (0, 1] to [0, 1). Note 1-x is not sufficient since for some floats
+// eps near 0, 1-eps will round to 1.
+template <typename T>
+__device__ inline T reverse_bounds(T value) {
+  if (THCNumerics<T>::eq(value, ScalarConvert<int, T>::to(1))) {
+    return ScalarConvert<int, T>::to(0);
+  }
+  return value;
+}
+
+
+#ifdef CUDA_HALF_TENSOR
+__device__ inline half half_uniform_scale_and_shift(float x, double a, double b) {
+  half width = ScalarConvert<double, half>::to(b - a);
+  half start = ScalarConvert<double, half>::to(a);
+  half scaled = THCNumerics<half>::mul(reverse_bounds(ScalarConvert<float, half>::to(x)), width);
+  return THCNumerics<half>::add(scaled, start);
+}
+#endif
+
 #define GENERATE_KERNEL1(NAME, T, ARG1, CURAND_T, CURAND_FUNC, TRANSFORM)      \
 __global__ void NAME(curandStateMtgp32 *state, int size, T *result, ARG1)      \
 {                                                                              \
diff --git a/src/THC/THCTensorRandom.cu.hip b/src/THC/THCTensorRandom.cu.hip
new file mode 100644
index 000000000..77cb5aaa2
--- /dev/null
+++ b/src/THC/THCTensorRandom.cu.hip
@@ -0,0 +1,204 @@
+#include "hip/hip_runtime.h"
+#include "THCTensorRandom.h"
+#include "THCDeviceUtils.cuh"
+#include "THCGeneral.h"
+#include "THCTensorCopy.h"
+#include "THCTensorMath.h"
+#include "THCReduceApplyUtils.cuh"
+#include "THCTensorRandom.cuh"
+
+#include <hiprng.h>
+#include <hiprng_kernel.h>
+
+#include <thrust/functional.h>
+
+#define MAX_NUM_BLOCKS 64
+#define BLOCK_SIZE 256
+
+THCGenerator* THCRandom_getGenerator(THCState* state);
+
+/* Sets up generator. Allocates but does not create the generator states. */
+void initializeGenerator(THCState *state, THCGenerator* gen)
+{
+  THCudaCheck(THCudaMalloc(state, (void**)&gen->gen_states, MAX_NUM_BLOCKS * sizeof(hiprngStateMtgp32)));
+  THCudaCheck(THCudaMalloc(state, (void**)&gen->kernel_params, sizeof(mtgp32_kernel_params)));
+}
+
+/* Creates a new generator state given the seed. */
+void createGeneratorState(THCGenerator* gen, uint64_t seed)
+{
+  if (hiprngMakeMTGP32Constants(mtgp32_params_fast_11213, gen->kernel_params) != HIPRNG_STATUS_SUCCESS)
+  {
+    THError("Creating MTGP constants failed.");
+  }
+  if (hiprngMakeMTGP32KernelState(gen->gen_states, mtgp32_params_fast_11213,
+                                  gen->kernel_params, MAX_NUM_BLOCKS, seed) != HIPRNG_STATUS_SUCCESS)
+  {
+    THError("Creating MTGP kernel state failed.");
+  }
+}
+
+void THCRandom_getRNGState(THCState* state, THByteTensor *rng_state)
+{
+  THCGenerator* gen = THCRandom_getGenerator(state);
+
+  // The RNG state comprises the MTPG32 states, the seed, and an offset used for Philox
+  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(hiprngStateMtgp32);
+  //static const size_t seed_size = sizeof(unsigned long);
+  static const size_t seed_size = sizeof(gen->initial_seed);
+  static const size_t offset_size = sizeof(gen->philox_seed_offset);
+  static const size_t total_size = states_size + seed_size + offset_size;
+  THByteTensor_resize1d(rng_state, total_size);
+  THArgCheck(THByteTensor_nElement(rng_state) == total_size, 1, "RNG state is wrong size");
+  THArgCheck(THByteTensor_isContiguous(rng_state), 1, "RNG state must be contiguous");
+  THCudaCheck(hipMemcpy(THByteTensor_data(rng_state), gen->gen_states,
+                         states_size, hipMemcpyDeviceToHost));
+  memcpy(THByteTensor_data(rng_state) + states_size, &gen->initial_seed, seed_size);
+  memcpy(THByteTensor_data(rng_state) + states_size + seed_size, &gen->philox_seed_offset, offset_size);
+
+}
+
+__global__ void set_rngstate_kernel(hiprngStateMtgp32 *state, mtgp32_kernel_params *kernel)
+{
+  state[hipThreadIdx_x].k = kernel;
+}
+
+
+void THCRandom_setRNGState(THCState* state, THByteTensor *rng_state)
+{
+  THCGenerator* gen = THCRandom_getGenerator(state);
+  static const size_t states_size = MAX_NUM_BLOCKS * sizeof(hiprngStateMtgp32);
+  //static const size_t seed_size = sizeof(unsigned long);
+  static const size_t seed_size = sizeof(gen->initial_seed);
+  static const size_t offset_size = sizeof(gen->philox_seed_offset);
+  static const size_t total_size = states_size + seed_size + offset_size;
+  bool no_philox_seed = false;
+  if (THByteTensor_nElement(rng_state) == total_size - offset_size) {
+    no_philox_seed = true;
+  }
+  else {
+    THArgCheck(THByteTensor_nElement(rng_state) == total_size, 1, "RNG state is wrong size");
+  }
+  THArgCheck(THByteTensor_isContiguous(rng_state), 1, "RNG state must be contiguous");
+
+  THCudaCheck(hipMemcpy(gen->gen_states, THByteTensor_data(rng_state),
+                         states_size, hipMemcpyHostToDevice));
+  hipLaunchKernelGGL(
+    set_rngstate_kernel,
+    dim3(1),
+    dim3(MAX_NUM_BLOCKS),
+    0,
+    THCState_getCurrentStream(state),
+    gen->gen_states,
+    gen->kernel_params);
+
+   memcpy(&gen->initial_seed, THByteTensor_data(rng_state) + states_size, seed_size);
+
+   if (!no_philox_seed) {
+     memcpy(&gen->philox_seed_offset, THByteTensor_data(rng_state) + states_size + seed_size, offset_size);
+   }
+   else {
+     gen->philox_seed_offset = 0;
+   }
+}
+
+// CURAND_PATH
+
+// Goes from (0, 1] to [0, 1). Note 1-x is not sufficient since for some floats
+// eps near 0, 1-eps will round to 1.
+ template <typename T>
+ __device__ inline T reverse_bounds(T value) {
+  if (THCNumerics<T>::eq(value, ScalarConvert<int, T>::to(1))) {
+    return ScalarConvert<int, T>::to(0);
+  }
+  return value;
+}
+
+#ifdef CUDA_HALF_TENSOR
+__device__ inline half half_uniform_scale_and_shift(float x, double a, double b) {
+   half width = ScalarConvert<double, half>::to(b - a);
+   half start = ScalarConvert<double, half>::to(a);
+   half scaled = THCNumerics<half>::mul(reverse_bounds(ScalarConvert<float, half>::to(x)), width);
+   return THCNumerics<half>::add(scaled, start);
+}
+#endif
+
+#define GENERATE_KERNEL1(NAME, T, ARG1, CURAND_T, CURAND_FUNC, TRANSFORM)      \
+__global__ void NAME(hiprngStateMtgp32 *state, int size, T *result, ARG1)      \
+{                                                                              \
+  int idx = hipBlockIdx_x * BLOCK_SIZE + hipThreadIdx_x;                             \
+  int rounded_size = THCCeilDiv(size, BLOCK_SIZE) * BLOCK_SIZE;                \
+  for (int i = idx; i < rounded_size; i += BLOCK_SIZE * MAX_NUM_BLOCKS) {      \
+    CURAND_T x = CURAND_FUNC(&state[hipBlockIdx_x]);                              \
+    if (i < size) {                                                            \
+      T y = TRANSFORM;                                                         \
+      result[i] = y;                                                           \
+    }                                                                          \
+  }                                                                            \
+}
+
+#define GENERATE_KERNEL2(NAME, T, ARG1, ARG2, CURAND_T, CURAND_FUNC, TRANSFORM)      \
+__global__ void NAME(hiprngStateMtgp32 *state, int size, T *result, ARG1, ARG2)      \
+{                                                                                    \
+  int idx = hipBlockIdx_x * BLOCK_SIZE + hipThreadIdx_x;                                   \
+  int rounded_size = THCCeilDiv(size, BLOCK_SIZE) * BLOCK_SIZE;                      \
+  for (int i = idx; i < rounded_size; i += BLOCK_SIZE * MAX_NUM_BLOCKS) {            \
+    CURAND_T x = CURAND_FUNC(&state[hipBlockIdx_x]);                                    \
+    if (i < size) {                                                                  \
+      T y = TRANSFORM;                                                               \
+      result[i] = y;                                                                 \
+    }                                                                                \
+  }                                                                                  \
+}
+
+
+template<typename T, typename U>
+struct is_same { static const bool value = false; };
+
+template<typename T>
+struct is_same<T, T> { static const bool value = true; };
+
+template<typename real, typename prob_type>
+__global__ void generate_bernoulli_tensor(hiprngStateMtgp32 *state, int size,
+        real *result, prob_type *probs)
+{
+  int idx = hipBlockIdx_x * BLOCK_SIZE + hipThreadIdx_x;
+  int rounded_size = THCCeilDiv(size, BLOCK_SIZE) * BLOCK_SIZE;
+  for (int i = idx; i < rounded_size; i += BLOCK_SIZE * MAX_NUM_BLOCKS) {
+    if (is_same<prob_type, double>::value) {
+      double x = hiprng_uniform(&state[hipBlockIdx_x]);
+      if (i < size)
+        result[i] = ScalarConvert<bool, real>::to(x <= probs[i]);
+    } else {
+      float x = hiprng_uniform(&state[hipBlockIdx_x]);
+      if (i < size)
+        result[i] = ScalarConvert<bool, real>::to(x <= probs[i]);
+    }
+  }
+}
+
+GENERATE_KERNEL2(generate_uniform, float, double a, double b, float, hiprng_uniform, x * (b-a) + a)
+GENERATE_KERNEL2(generate_uniform, double, double a, double b, double, hiprng_uniform_double, x * (b-a) + a)
+
+GENERATE_KERNEL2(generate_normal, float, double mean, double stdv, float, hiprng_normal, (x * stdv) + mean)
+GENERATE_KERNEL2(generate_normal, double, double mean, double stdv, double, hiprng_normal_double, (x * stdv) + mean)
+
+GENERATE_KERNEL1(generate_exponential, float, double lambda, float, hiprng_uniform, (float)(-1. / lambda * log(1-x)))
+GENERATE_KERNEL1(generate_exponential, double, double lambda, double, hiprng_uniform_double, (double)(-1. / lambda * log(1-x)))
+
+GENERATE_KERNEL2(generate_cauchy, float, double median, double sigma, float, hiprng_uniform, (float)(median + sigma * tan(M_PI*(x-0.5))))
+GENERATE_KERNEL2(generate_cauchy, double, double median, double sigma, double, hiprng_uniform_double, (double)(median + sigma * tan(M_PI*(x-0.5))))
+
+#ifdef CUDA_HALF_TENSOR
+GENERATE_KERNEL2(generate_uniform, half, double a, double b, float, hiprng_uniform, (half_uniform_scale_and_shift(x, a, b)))
+GENERATE_KERNEL2(generate_normal, half, double mean, double stdv, float, hiprng_normal, (ScalarConvert<float, half>::to((x * stdv) + mean)))
+GENERATE_KERNEL1(generate_exponential, half, double lambda, float, hiprng_uniform, (ScalarConvert<float, half>::to((float)(-1. / lambda * log(1-x)))))
+GENERATE_KERNEL2(generate_cauchy, half, double median, double sigma, float, hiprng_uniform, (ScalarConvert<float, half>::to((float)(median + sigma * tan(M_PI*(x-0.5))))))
+#endif // CUDA_HALF_TENSOR
+
+
+#include "generic/THCTensorRandom.cu"
+#include "THCGenerateAllTypes.h"
+
+#undef GENERATE_KERNEL1
+#undef GENERATE_KERNEL2
diff --git a/src/THC/THCTensorRandom.h.hip b/src/THC/THCTensorRandom.h.hip
new file mode 100644
index 000000000..43baaa22c
--- /dev/null
+++ b/src/THC/THCTensorRandom.h.hip
@@ -0,0 +1,38 @@
+#ifndef TH_CUDA_TENSOR_RANDOM_INC
+#define TH_CUDA_TENSOR_RANDOM_INC
+
+#include "THCTensor.h"
+
+#include "generic/THCTensorRandom.h"
+#include "THCGenerateAllTypes.h"
+
+/* Generator */
+typedef struct _Generator {
+  struct hcrngStateMtgp32* gen_states;
+  struct mtgp32_kernel_params *kernel_params;
+  int initf;
+  uint64_t initial_seed;
+  int64_t philox_seed_offset;
+} THCGenerator;
+
+typedef struct THCRNGState {
+  /* One generator per GPU */
+  THCGenerator* gen;
+  int num_devices;
+} THCRNGState;
+
+struct THCState;
+
+THC_API void THCRandom_init(struct THCState *state, int num_devices, int current_device);
+THC_API void THCRandom_shutdown(struct THCState *state);
+THC_API uint64_t THCRandom_seed(struct THCState *state);
+THC_API uint64_t THCRandom_seedAll(struct THCState *state);
+THC_API void THCRandom_manualSeed(struct THCState *state, uint64_t the_seed_);
+THC_API void THCRandom_manualSeedAll(struct THCState *state, uint64_t the_seed_);
+THC_API uint64_t THCRandom_initialSeed(struct THCState *state);
+THC_API void THCRandom_getRNGState(struct THCState *state, THByteTensor *rng_state);
+THC_API void THCRandom_setRNGState(struct THCState *state, THByteTensor *rng_state);
+
+THC_API struct hcrngStateMtgp32* THCRandom_generatorStates(struct THCState* state);
+
+#endif
diff --git a/src/THC/THCTensorScatterGather.cu b/src/THC/THCTensorScatterGather.cu
index a1ed0d486..edd72e8c8 100644
--- a/src/THC/THCTensorScatterGather.cu
+++ b/src/THC/THCTensorScatterGather.cu
@@ -94,7 +94,9 @@ __global__ void THCudaTensor_gatherKernel(
                                                           src, &srcOffset);
 
     int64_t indexValue = index.data[indexOffset] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(indexValue >= 0 && indexValue < src.sizes[dim]);
+#endif
     srcOffset += indexValue * src.strides[dim];
 
     tensor.data[tensorOffset] = src.data[srcOffset];
@@ -121,7 +123,9 @@ __global__ void THCudaTensor_scatterKernel(
                                                           tensor, &tensorOffset);
 
     int64_t indexValue = index.data[indexOffset] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(indexValue >= 0 && indexValue < tensor.sizes[dim]);
+#endif
     tensorOffset += indexValue * tensor.strides[dim];
 
     tensor.data[tensorOffset] = src.data[srcOffset];
@@ -148,7 +152,9 @@ __global__ void THCudaTensor_scatterAddKernel(
                                                           tensor, &tensorOffset);
 
     int64_t indexValue = index.data[indexOffset] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(indexValue >= 0 && indexValue < tensor.sizes[dim]);
+#endif
     tensorOffset += indexValue * tensor.strides[dim];
 
     atomicAdd(&tensor.data[tensorOffset], src.data[srcOffset]);
@@ -173,7 +179,9 @@ __global__ void THCudaTensor_scatterFillKernel(
                                                           tensor, &tensorOffset);
 
     int64_t indexValue = index.data[indexOffset] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(indexValue >= 0 && indexValue < tensor.sizes[dim]);
+#endif
     tensorOffset += indexValue * tensor.strides[dim];
 
     tensor.data[tensorOffset] = value;
diff --git a/src/THC/THCTensorSort.cu b/src/THC/THCTensorSort.cu
index 8e42e9857..6cbfd74e8 100644
--- a/src/THC/THCTensorSort.cu
+++ b/src/THC/THCTensorSort.cu
@@ -24,10 +24,19 @@ void THCudaLongTensor_fillSliceWithIndex(THCState* state,
 
   dim3 block(numThreads);
 
-#define FILL_INDEX(T, DIM)                                       \
-  fillSliceWithIndex<T, DIM>                                     \
-    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(      \
-      info, numSlices, sliceSize, info.strides[collapseDim])
+#if defined(__HIP_PLATFORM_HCC__)
+  #define FILL_INDEX(T, DIM)                                       \
+    hipLaunchKernelGGL(                                            \
+      (fillSliceWithIndex<T, DIM>),                                \
+        grid, block, 0, THCState_getCurrentStream(state),          \
+          info, numSlices, sliceSize,                              \
+          info.strides[collapseDim])
+#else
+  #define FILL_INDEX(T, DIM)                                       \
+    fillSliceWithIndex<T, DIM>                                     \
+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(      \
+        info, numSlices, sliceSize, info.strides[collapseDim])
+#endif
 
   if (TensorUtils<THCudaLongTensor>::canUse32BitIndexMath(state, t)) {
     TensorInfo<int64_t, uint32_t> info =
diff --git a/src/THC/THCTensorTopK.cuh b/src/THC/THCTensorTopK.cuh
index c24331664..39bb80e93 100644
--- a/src/THC/THCTensorTopK.cuh
+++ b/src/THC/THCTensorTopK.cuh
@@ -58,8 +58,10 @@ template <>
 struct TopKTypeConfig<int16_t> {
   typedef uint32_t RadixType;
 
-  static inline __device__ RadixType convert(int16_t v) {
+  static inline __device__ RadixType convert(int64_t v) {
+#if defined(__NVCC__)
     assert(sizeof(short) == 2);
+#endif
     return 32768u + v;
   }
 
@@ -73,7 +75,9 @@ struct TopKTypeConfig<int32_t> {
   typedef uint32_t RadixType;
 
   static inline __device__ RadixType convert(int32_t v) {
+#if defined(__NVCC__)
     assert(sizeof(int) == 4);
+#endif
     return 2147483648u + v;
   }
 
@@ -87,7 +91,9 @@ struct TopKTypeConfig<int64_t> {
   typedef uint64_t RadixType;
 
   static inline __device__ RadixType convert(int64_t v) {
+#if defined(__NVCC__)
     assert(sizeof(int64_t) == 8);
+#endif
     return 9223372036854775808ull + v;
   }
 
@@ -123,7 +129,9 @@ struct TopKTypeConfig<half> {
     RadixType mask = -((x >> 15)) | 0x8000;
     return (x ^ mask);
 #else
+#if defined(__NVCC__)
     assert(false);
+#endif
     return 0u;
 #endif
   }
@@ -133,7 +141,9 @@ struct TopKTypeConfig<half> {
     RadixType mask = ((v >> 15) - 1) | 0x8000;
     return __ushort_as_half(v ^ mask);
 #else
+#if defined(__NVCC__)
     assert(false);
+#endif
     return ScalarConvert<int, half>::to(0);
 #endif
   }
@@ -248,7 +258,9 @@ __device__ DataType findPattern(DataType* smem,
   }
 
   // should not get here
+#if defined(__NVCC__)
   assert(false);
+#endif
   return ScalarConvert<int, DataType>::to(0);
 }
 
@@ -428,8 +440,9 @@ __global__ void gatherTopK(TensorInfo<T, IndexType> input,
 
     if (hasTopK) {
       int writeIndex = writeIndexStart + index;
+#if defined(__NVCC__)
       assert(writeIndex < outputSliceSize);
-
+#endif
       IndexType topKOffset = writeIndex * topKWithinSliceStride;
       IndexType indexOffset = writeIndex * indicesWithinSliceStride;
 
@@ -445,7 +458,9 @@ __global__ void gatherTopK(TensorInfo<T, IndexType> input,
   // writeIndexStart. There might be more than that number available,
   // in which case we have to choose the first seen set. We do this
   // via a prefix sum to calculate indices for writing results.
+#if defined(__NVCC__)
   assert(outputSliceSize >= writeIndexStart);
+#endif
   IndexType topKRemaining = (outputSliceSize - writeIndexStart);
 
   for (IndexType i = threadIdx.x; i < numIterations; i += blockDim.x) {
@@ -460,8 +475,9 @@ __global__ void gatherTopK(TensorInfo<T, IndexType> input,
 
     if (hasTopK && index < topKRemaining) {
       int writeIndex = writeIndexStart + index;
+#if defined(__NVCC__)
       assert(writeIndex < outputSliceSize);
-
+#endif
       IndexType topKOffset = writeIndex * topKWithinSliceStride;
       IndexType indexOffset = writeIndex * indicesWithinSliceStride;
 
diff --git a/src/THC/THCTensorTypeUtils.cuh b/src/THC/THCTensorTypeUtils.cuh
index 78bea9746..70368343e 100644
--- a/src/THC/THCTensorTypeUtils.cuh
+++ b/src/THC/THCTensorTypeUtils.cuh
@@ -143,6 +143,51 @@ struct ScalarInv {
   static __host__ __device__ T to(const T v) { return ((T) 1) / v; }
 };
 
+#if defined(__HIP_PLATFORM_HCC__)
+    template <>
+    struct ScalarNegate<half> {
+      __host__
+      static
+      half to(half v)
+      {
+          return -v;
+      }
+
+      __device__
+      static
+      half to(half v)
+      {
+          return -v;
+      }
+    };
+
+    template <>
+    struct ScalarInv<half> {
+      __host__
+      static
+      half to(half v)
+      {
+        float fv = THC_half2float(v);
+        fv = 1.0f / fv;
+        return THC_float2half(fv);
+      }
+
+      __device__
+      static
+      half to(half v)
+      {
+          return static_cast<half>(1) / v;
+      }
+    };
+
+// inline bool operator==(half a, half b) {
+//   return a == b;
+// }
+// 
+// inline bool operator!=(half a, half b) {
+//   return a != b;
+// }
+#else
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct ScalarNegate<half> {
@@ -201,5 +246,6 @@ inline bool operator!=(half a, half b) {
 }
 
 #endif // CUDA_HALF_TENSOR
+#endif
 
 #endif // THC_TENSOR_TYPE_UTILS_INC
diff --git a/src/THC/generic/THCTensor.cu b/src/THC/generic/THCTensor.cu
index 8f13a7d0a..2e1af9de2 100644
--- a/src/THC/generic/THCTensor.cu
+++ b/src/THC/generic/THCTensor.cu
@@ -2,6 +2,7 @@
 #define THC_GENERIC_FILE "generic/THCTensor.cu"
 #else
 
+#if defined(__NVCC__)
 cudaTextureObject_t THCTensor_(getTextureObject)(THCState *state, THCTensor *self)
 {
   THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self));
@@ -27,10 +28,10 @@ cudaTextureObject_t THCTensor_(getTextureObject)(THCState *state, THCTensor *sel
   }
   return texObj;
 }
+#endif
 
 THC_API int THCTensor_(getDevice)(THCState* state, const THCTensor* tensor) {
   if (!tensor->storage) return -1;
   return THCStorage_(getDevice)(state, tensor->storage);
 }
-
 #endif
diff --git a/src/THC/generic/THCTensor.h b/src/THC/generic/THCTensor.h
index dcfcf8ad2..3019fd17f 100644
--- a/src/THC/generic/THCTensor.h
+++ b/src/THC/generic/THCTensor.h
@@ -130,7 +130,9 @@ THC_API real THCTensor_(get3d)(THCState *state, const THCTensor *tensor, int64_t
 THC_API real THCTensor_(get4d)(THCState *state, const THCTensor *tensor, int64_t x0, int64_t x1, int64_t x2, int64_t x3);
 
 /* CUDA-specific functions */
+#if defined(__NVCC__)
 THC_API cudaTextureObject_t THCTensor_(getTextureObject)(THCState *state, THCTensor *self);
+#endif
 THC_API int THCTensor_(getDevice)(THCState *state, const THCTensor *self);
 THC_API int THCTensor_(checkGPU)(THCState *state, unsigned int nTensors, ...);
 
diff --git a/src/THC/generic/THCTensorIndex.cu b/src/THC/generic/THCTensorIndex.cu
index 94d9b1e64..f535715a9 100644
--- a/src/THC/generic/THCTensorIndex.cu
+++ b/src/THC/generic/THCTensorIndex.cu
@@ -131,21 +131,41 @@ void THCTensor_(indexCopy)(THCState *state, THCTensor *dst, int dim, THCudaLongT
 
   int mpc = THCState_getCurrentDeviceProperties(state)->multiProcessorCount;
 
-#define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM) \
-  indexCopySmallIndex<TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM>       \
-    <<<smallIndexGrid, smallIndexBlock, 0, stream>>>(           \
-      dstInfo, srcInfo, indicesInfo,                            \
-      dstCopyDim, srcCopyDim, sliceSize, dstCopyDimSize);
-
-#define LARGE_INDEX(TENSOR_TYPE, TYPE,                         \
-                    DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR)   \
-  indexCopyLargeIndex<TENSOR_TYPE, TYPE,                       \
-                      DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR> \
-    <<<largeIndexGrid, largeIndexBlock, 0, stream>>>(          \
-      dstInfo, srcInfo, indicesInfo,                           \
-      dstCopyDim, srcCopyDim, srcTotalSize,                    \
-      (IDX_IS_MAJOR) ? sliceSize : numIndices,                 \
-      dstCopyDimSize);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM)      \
+    hipLaunchKernelGGL(                                                  \
+    (indexCopySmallIndex<TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM>), \
+        smallIndexGrid, smallIndexBlock, 0, stream,                      \
+        dstInfo, srcInfo, indicesInfo,                                   \
+        dstCopyDim, srcCopyDim, sliceSize, dstCopyDimSize);
+
+  #define LARGE_INDEX(TENSOR_TYPE, TYPE,                           \
+                      DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR)     \
+    hipLaunchKernelGGL(                                            \
+    (indexCopyLargeIndex<TENSOR_TYPE, TYPE,                        \
+                        DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR>), \
+        largeIndexGrid, largeIndexBlock, 0, stream,                \
+        dstInfo, srcInfo, indicesInfo,                             \
+        dstCopyDim, srcCopyDim, srcTotalSize,                      \
+        (IDX_IS_MAJOR) ? sliceSize : numIndices,                   \
+        dstCopyDimSize);
+#else
+  #define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM)   \
+    indexCopySmallIndex<TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM> \
+      <<<smallIndexGrid, smallIndexBlock, 0, stream>>>(               \
+        dstInfo, srcInfo, indicesInfo,                                \
+        dstCopyDim, srcCopyDim, sliceSize, dstCopyDimSize);
+
+  #define LARGE_INDEX(TENSOR_TYPE, TYPE,                         \
+                      DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR)   \
+    indexCopyLargeIndex<TENSOR_TYPE, TYPE,                       \
+                        DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR> \
+      <<<largeIndexGrid, largeIndexBlock, 0, stream>>>(          \
+        dstInfo, srcInfo, indicesInfo,                           \
+        dstCopyDim, srcCopyDim, srcTotalSize,                    \
+        (IDX_IS_MAJOR) ? sliceSize : numIndices,                 \
+        dstCopyDimSize);
+#endif
 
   dim3 smallIndexGrid(std::min(THCCeilDiv(sliceSize, (ptrdiff_t)128), (ptrdiff_t)(mpc * 8)));
   dim3 smallIndexBlock(std::min(sliceSize, (ptrdiff_t)128));
@@ -252,7 +272,11 @@ static void THCTensor_(sort_indices)(THCState *state, THCudaLongTensor *index, T
   auto numel = THCTensor_(numel)(state, src);
 
   thrust::sort_by_key(
+#if CUDA_VERSION >= 7000
     thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
+#else
+    thrust::device,
+#endif
     index_iter, index_iter + numel,
     src_iter, ThrustLTOp<int64_t>());
 }
@@ -332,21 +356,42 @@ void THCTensor_(indexAdd)(THCState *state, THCTensor *dst, int dim, THCudaLongTe
 
   int mpc = THCState_getCurrentDeviceProperties(state)->multiProcessorCount;
 
-#define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM) \
-  indexAddSmallIndex<TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM> \
-    <<<smallIndexGrid, smallIndexBlock, 0, stream>>>(   \
-      dstInfo, srcInfo, indicesInfo,                    \
-      dstAddDim, srcAddDim, sliceSize, dstAddDimSize);
-
-#define LARGE_INDEX(TENSOR_TYPE, TYPE,                        \
-                    DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR)  \
-  indexAddLargeIndex<TENSOR_TYPE, TYPE,                       \
-                     DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR> \
-    <<<largeIndexGrid, largeIndexBlock, 0, stream>>>(         \
-      dstInfo, srcInfo, indicesInfo,                          \
-      dstAddDim, srcAddDim, srcTotalSize,                     \
-      (IDX_IS_MAJOR) ? sliceSize : numIndices,                \
-      dstAddDimSize);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM)  \
+    hipLaunchKernelGGL(                                              \
+      (indexAddSmallIndex<TENSOR_TYPE, TYPE,                         \
+                          DST_DIM, SRC_DIM, IDX_DIM>),               \
+        smallIndexGrid, smallIndexBlock, 0, stream,                  \
+        dstInfo, srcInfo, indicesInfo,                               \
+        dstAddDim, srcAddDim, sliceSize, dstAddDimSize);
+
+  #define LARGE_INDEX(TENSOR_TYPE, TYPE,                             \
+                      DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR)       \
+    hipLaunchKernelGGL(                                              \
+    (indexAddLargeIndex<TENSOR_TYPE, TYPE,                           \
+                       DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR>),    \
+        largeIndexGrid, largeIndexBlock, 0, stream,                  \
+        dstInfo, srcInfo, indicesInfo,                               \
+        dstAddDim, srcAddDim, srcTotalSize,                          \
+        (IDX_IS_MAJOR) ? sliceSize : numIndices,                     \
+        dstAddDimSize);
+#else
+  #define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM)  \
+    indexAddSmallIndex<TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM> \
+      <<<smallIndexGrid, smallIndexBlock, 0, stream>>>(              \
+        dstInfo, srcInfo, indicesInfo,                               \
+        dstAddDim, srcAddDim, sliceSize, dstAddDimSize);
+
+  #define LARGE_INDEX(TENSOR_TYPE, TYPE,                             \
+                      DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR)       \
+    indexAddLargeIndex<TENSOR_TYPE, TYPE,                            \
+                       DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR>      \
+      <<<largeIndexGrid, largeIndexBlock, 0, stream>>>(              \
+        dstInfo, srcInfo, indicesInfo,                               \
+        dstAddDim, srcAddDim, srcTotalSize,                          \
+        (IDX_IS_MAJOR) ? sliceSize : numIndices,                     \
+        dstAddDimSize);
+#endif
 
   dim3 smallIndexGrid(std::min(THCCeilDiv(sliceSize, (ptrdiff_t)128), (ptrdiff_t)(mpc * 8)));
   dim3 smallIndexBlock(std::min(sliceSize, (ptrdiff_t)128));
@@ -463,19 +508,39 @@ void THCTensor_(indexFill)(THCState *state, THCTensor *dst, int dim, THCudaLongT
 
   int mpc = THCState_getCurrentDeviceProperties(state)->multiProcessorCount;
 
-#define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM)  \
-  indexFillSmallIndex<TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM> \
-    <<<smallIndexGrid, smallIndexBlock, 0, stream>>>(   \
-      dstInfo, indicesInfo,                             \
-      dstFillDim, sliceSize, dstFillDimSize, val);
-
-#define LARGE_INDEX(TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM, IDX_IS_MAJOR)   \
-  indexFillLargeIndex<TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM, IDX_IS_MAJOR> \
-    <<<largeIndexGrid, largeIndexBlock, 0, stream>>>(                    \
-      dstInfo, indicesInfo,                                              \
-      dstFillDim, sliceSize * numIndices,                                \
-      (IDX_IS_MAJOR) ? sliceSize : numIndices,                           \
-      dstFillDimSize, val);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM)  \
+    hipLaunchKernelGGL(                                     \
+    (indexFillSmallIndex<TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM>), \
+        smallIndexGrid, smallIndexBlock, 0, stream,         \
+        dstInfo,                        \
+        indicesInfo,                    \
+        dstFillDim, sliceSize, dstFillDimSize, val);
+
+  #define LARGE_INDEX(TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM, IDX_IS_MAJOR)   \
+    hipLaunchKernelGGL(                                     \
+      (indexFillLargeIndex<TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM, IDX_IS_MAJOR>), \
+        largeIndexGrid, largeIndexBlock, 0, stream,         \
+        dstInfo,                        \
+        indicesInfo,                    \
+        dstFillDim, sliceSize * numIndices, \
+        (IDX_IS_MAJOR) ? sliceSize : numIndices, \
+        dstFillDimSize , val);
+#else
+  #define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM)  \
+    indexFillSmallIndex<TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM> \
+      <<<smallIndexGrid, smallIndexBlock, 0, stream>>>(   \
+        dstInfo, indicesInfo,                             \
+        dstFillDim, sliceSize, dstFillDimSize, val);
+
+  #define LARGE_INDEX(TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM, IDX_IS_MAJOR)   \
+    indexFillLargeIndex<TENSOR_TYPE, TYPE, DST_DIM, IDX_DIM, IDX_IS_MAJOR> \
+      <<<largeIndexGrid, largeIndexBlock, 0, stream>>>(                    \
+        dstInfo, indicesInfo,                                              \
+        dstFillDim, sliceSize * numIndices,                                \
+        (IDX_IS_MAJOR) ? sliceSize : numIndices,                           \
+        dstFillDimSize, val);
+#endif
 
   dim3 smallIndexGrid(std::min(THCCeilDiv(sliceSize, (ptrdiff_t)128), (ptrdiff_t)(mpc * 8)));
   dim3 smallIndexBlock(std::min(sliceSize, (ptrdiff_t)128));
@@ -607,21 +672,42 @@ void THCTensor_(indexSelect)(THCState *state, THCTensor *dst, THCTensor *src, in
 
   int mpc = THCState_getCurrentDeviceProperties(state)->multiProcessorCount;
 
-#define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM) \
-  indexSelectSmallIndex<TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM>     \
-    <<<smallIndexGrid, smallIndexBlock, 0, stream>>>(           \
-      dstInfo, srcInfo, indicesInfo,                            \
-      dstSelectDim, srcSelectDim, sliceSize, srcSelectDimSize);
-
-#define LARGE_INDEX(TENSOR_TYPE, TYPE,                           \
-                    DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR)     \
-  indexSelectLargeIndex<TENSOR_TYPE, TYPE,                       \
-                        DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR> \
-    <<<largeIndexGrid, largeIndexBlock, 0, stream>>>(            \
-      dstInfo, srcInfo, indicesInfo,                             \
-      dstSelectDim, srcSelectDim, dstTotalSize,                  \
-      (IDX_IS_MAJOR) ? sliceSize : numIndices,                   \
-      srcSelectDimSize);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM)\
+    hipLaunchKernelGGL(                                            \
+    (indexSelectSmallIndex<TENSOR_TYPE, TYPE, DST_DIM,             \
+                           SRC_DIM, IDX_DIM>),                     \
+        smallIndexGrid, smallIndexBlock, 0, stream,                \
+        dstInfo, srcInfo, indicesInfo,                             \
+        dstSelectDim, srcSelectDim, sliceSize, srcSelectDimSize);
+
+  #define LARGE_INDEX(TENSOR_TYPE, TYPE,                           \
+                      DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR)     \
+    hipLaunchKernelGGL(                                            \
+    (indexSelectLargeIndex<TENSOR_TYPE, TYPE, DST_DIM,             \
+                           SRC_DIM, IDX_DIM, IDX_IS_MAJOR>),       \
+       largeIndexGrid, largeIndexBlock, 0, stream,                 \
+        dstInfo, srcInfo, indicesInfo,                             \
+        dstSelectDim, srcSelectDim, dstTotalSize,                  \
+        (IDX_IS_MAJOR) ? sliceSize : numIndices,                   \
+        srcSelectDimSize);
+#else
+  #define SMALL_INDEX(TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM)     \
+    indexSelectSmallIndex<TENSOR_TYPE, TYPE, DST_DIM, SRC_DIM, IDX_DIM> \
+      <<<smallIndexGrid, smallIndexBlock, 0, stream>>>(                 \
+        dstInfo, srcInfo, indicesInfo,                                  \
+        dstSelectDim, srcSelectDim, sliceSize, srcSelectDimSize);
+
+  #define LARGE_INDEX(TENSOR_TYPE, TYPE,                                 \
+                      DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR)           \
+    indexSelectLargeIndex<TENSOR_TYPE, TYPE,                             \
+                          DST_DIM, SRC_DIM, IDX_DIM, IDX_IS_MAJOR>       \
+      <<<largeIndexGrid, largeIndexBlock, 0, stream>>>(                  \
+        dstInfo, srcInfo, indicesInfo,                                   \
+        dstSelectDim, srcSelectDim, dstTotalSize,                        \
+        (IDX_IS_MAJOR) ? sliceSize : numIndices,                         \
+        srcSelectDimSize);
+#endif
 
   dim3 smallIndexGrid(std::min(THCCeilDiv(sliceSize, (ptrdiff_t)128), (ptrdiff_t)(mpc * 8)));
   dim3 smallIndexBlock(std::min(sliceSize, (ptrdiff_t)128));
@@ -728,27 +814,52 @@ void THCTensor_(calculateAdvancedIndexingOffsets)(
   dim3 grid;
   THAssert(getApplyGrid(state, nElement, grid));
 
-#define HANDLE_CASE(INDEX_TYPE, DIMS)                                                           \
-{                                                                                               \
-  LinearIndexCalcData<INDEX_TYPE, DIMS> data;                                                   \
-  for (int i = 0; i < DIMS; ++i) {                                                              \
-    data.baseSizes[i] = THCTensor_(size)(state, indexed, i);                                    \
-    data.sizes[i] = indexers[i] != NULL ?                                                       \
-      THCudaLongTensor_nElement(state, indexers[i]) :                                           \
-        THCTensor_(size)(state, indexed, i);                                                    \
-    data.strides[i] = THCTensor_(stride)(state, indexed, i);                                    \
-    data.advIndexTensors[i] = indexers[i] != NULL ?                                             \
-      THCudaLongTensor_data(state, indexers[i]) : NULL;                                         \
-  }                                                                                             \
-                                                                                                \
-  calculateLinearIndices<INDEX_TYPE, DIMS>                                                      \
-    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(                                     \
-    THCudaLongTensor_data(state, output),                                                       \
-    nElement,                                                                                   \
-    baseOffset,                                                                                 \
-    data                                                                                        \
-  );                                                                                            \
-}
+#if defined(__HIP_PLATFORM_HCC__)
+  #define HANDLE_CASE(INDEX_TYPE, DIMS)                                                           \
+  {                                                                                               \
+    LinearIndexCalcData<INDEX_TYPE, DIMS> data;                                                   \
+    for (int i = 0; i < DIMS; ++i) {                                                              \
+      data.baseSizes[i] = THCTensor_(size)(state, indexed, i);                                    \
+      data.sizes[i] = indexers[i] != NULL ?                                                       \
+        THCudaLongTensor_nElement(state, indexers[i]) :                                           \
+          THCTensor_(size)(state, indexed, i);                                                    \
+      data.strides[i] = THCTensor_(stride)(state, indexed, i);                                    \
+      data.advIndexTensors[i] = indexers[i] != NULL ?                                             \
+        THCudaLongTensor_data(state, indexers[i]) : NULL;                                         \
+    }                                                                                             \
+                                                                                                  \
+    hipLaunchKernelGGL(                                                                           \
+      (calculateLinearIndices<INDEX_TYPE, DIMS>),                                                 \
+        grid, block, 0, THCState_getCurrentStream(state),                                         \
+        THCudaLongTensor_data(state, output),                                                     \
+        nElement,                                                                                 \
+        baseOffset,                                                                               \
+        data                                                                                      \
+    );                                                                                            \
+  }
+#else
+  #define HANDLE_CASE(INDEX_TYPE, DIMS)                                                           \
+  {                                                                                               \
+    LinearIndexCalcData<INDEX_TYPE, DIMS> data;                                                   \
+    for (int i = 0; i < DIMS; ++i) {                                                              \
+      data.baseSizes[i] = THCTensor_(size)(state, indexed, i);                                    \
+      data.sizes[i] = indexers[i] != NULL ?                                                       \
+        THCudaLongTensor_nElement(state, indexers[i]) :                                           \
+          THCTensor_(size)(state, indexed, i);                                                    \
+      data.strides[i] = THCTensor_(stride)(state, indexed, i);                                    \
+      data.advIndexTensors[i] = indexers[i] != NULL ?                                             \
+        THCudaLongTensor_data(state, indexers[i]) : NULL;                                         \
+    }                                                                                             \
+                                                                                                  \
+    calculateLinearIndices<INDEX_TYPE, DIMS>                                                      \
+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(                                     \
+      THCudaLongTensor_data(state, output),                                                       \
+      nElement,                                                                                   \
+      baseOffset,                                                                                 \
+      data                                                                                        \
+    );                                                                                            \
+  }
+#endif
 
 #define RUN_T(INDEX_TYPE)         \
   switch (ndim) {                 \
diff --git a/src/THC/generic/THCTensorMath.cu b/src/THC/generic/THCTensorMath.cu
index 1fcf3d9e7..cab37a4d9 100644
--- a/src/THC/generic/THCTensorMath.cu
+++ b/src/THC/generic/THCTensorMath.cu
@@ -212,8 +212,14 @@ void THCTensor_(catArray)(THCState *state, THCTensor *result,
     THCStream* stream = THCState_getStream(state);
 
     // Template Declarations for dim = 1, 2, 3, 4
+#if defined (__HIP_PLATFORM_HCC__)
+#define HANDLE_CASE(DIMS) \
+  hipLaunchKernelGGL((CatArrayBatchedCopy<real, unsigned int, DIMS>), catGrid, applyBlock, 0, stream->stream, \
+    data, d_inputs, param, cat_dimension, param.outputStride[cat_dimension]);
+#else
 #define HANDLE_CASE(DIMS) \
   CatArrayBatchedCopy<real, unsigned int, DIMS><<<catGrid, applyBlock, 0, stream->stream>>>(data, d_inputs, param, cat_dimension, param.outputStride[cat_dimension]);
+#endif
 
     // Now we loop
     offset = 0;
diff --git a/src/THC/generic/THCTensorMode.cu b/src/THC/generic/THCTensorMode.cu
index 50b90a52b..43d012943 100644
--- a/src/THC/generic/THCTensorMode.cu
+++ b/src/THC/generic/THCTensorMode.cu
@@ -235,6 +235,19 @@ THC_API void THCTensor_(mode)(THCState *state,
 
     // Macro that calls kernel --> note that we set the block dimensions here, and
     // the amount of shared memory
+#if defined(__HIP_PLATFORM_HCC__)
+  #define HANDLE_MODE(SIZE) \
+  { \
+    dim3 blockSize(SIZE / 2); \
+\
+    int memsize = (sizeof(real) * SIZE) + (2 * SIZE * sizeof(unsigned int)); \
+    hipLaunchKernelGGL( \
+      (computeMode<real, SIZE>), \
+        grid, blockSize, memsize, THCState_getCurrentStream(state), \
+        THCTensor_(data)(state, contiguous), tiValues, \
+        tiIndices, sliceSize); \
+  }
+#else
   #define HANDLE_MODE(SIZE) \
   { \
     dim3 blockSize(SIZE / 2); \
@@ -244,6 +257,7 @@ THC_API void THCTensor_(mode)(THCState *state,
       <<<grid, blockSize, memsize, THCState_getCurrentStream(state)>>>( \
         THCTensor_(data)(state, contiguous), tiValues, tiIndices, sliceSize); \
   }
+#endif
 
     // Tradeoff between compilation time and the number of specializations. Ideally we would have
     // one HANDLE_MODE for each power of 2
diff --git a/src/THC/generic/THCTensorRandom.cu b/src/THC/generic/THCTensorRandom.cu
index a577a53c1..49c267921 100644
--- a/src/THC/generic/THCTensorRandom.cu
+++ b/src/THC/generic/THCTensorRandom.cu
@@ -338,7 +338,7 @@ THC_API void THCTensor_(multinomialAliasSetup)(THCState *state, THCTensor *_prob
                 inputsize,
                 THCudaLongTensor_data(state, smaller_short),
                 THCudaLongTensor_data(state, larger_short),
-                inputsize - h_large_c, h_large_c
+                static_cast<int>(inputsize - h_large_c), h_large_c
                 );
   real q_max = THCTensor_(maxall)(state, _q);
   condDiv<<<
@@ -443,7 +443,7 @@ THC_API void THCTensor_(NAME)(THCState* state,                                 \
   THArgCheck(size == prob_size, 3, "inconsistent tensor size");                \
                                                                                \
   generate_bernoulli_tensor<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>( \
-      gen->gen_states, size, result_data, probs_data);                         \
+      gen->gen_states, static_cast<int>(size), result_data, probs_data);       \
                                                                                \
   PROB_TYPE##_free(state, probs);                                              \
   THCTensor_(freeCopyTo)(state, self, self_);                                  \
diff --git a/src/THC/generic/THCTensorRandom.cu.hip b/src/THC/generic/THCTensorRandom.cu.hip
new file mode 100644
index 000000000..7ec6190ae
--- /dev/null
+++ b/src/THC/generic/THCTensorRandom.cu.hip
@@ -0,0 +1,577 @@
+#ifndef THC_GENERIC_FILE
+#define THC_GENERIC_FILE "generic/THCTensorRandom.cu"
+#else
+
+#define NUM_BLOCKS min((int)THCCeilDiv(size, (ptrdiff_t) BLOCK_SIZE), MAX_NUM_BLOCKS)
+
+#if defined(THC_REAL_IS_FLOAT) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_HALF)
+THC_API void THCTensor_(uniform)(THCState* state, THCTensor *self_, double a, double b)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+  ptrdiff_t size = THCTensor_(nElement)(state, self_);
+  if (size == 0) return;
+  THCGenerator* gen = THCRandom_getGenerator(state);
+  THCTensor *self = THCTensor_(newContiguous)(state, self_);
+  real *data = THCTensor_(data)(state, self);
+
+  generate_uniform<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, a, b);
+
+  THCTensor_(freeCopyTo)(state, self, self_);
+};
+
+THC_API void THCTensor_(normal)(THCState* state, THCTensor *self_, double mean, double stdv)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+  ptrdiff_t size = THCTensor_(nElement)(state, self_);
+  if (size == 0) return;
+  THCGenerator* gen = THCRandom_getGenerator(state);
+  THCTensor *self = THCTensor_(newContiguous)(state, self_);
+  real *data = THCTensor_(data)(state, self);
+
+  generate_normal<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, mean, stdv);
+
+  THCTensor_(freeCopyTo)(state, self, self_);
+};
+
+THC_API void THCTensor_(normal_means)(THCState *state, THCTensor *self, THCTensor *means, double stddev) {
+  THCTensor_(resizeAs)(state, self, means);
+  THCTensor_(normal)(state, self, 0, stddev);
+  THCTensor_(cadd)(state, self, self, ScalarConvert<int, real>::to(1), means);
+}
+
+THC_API void THCTensor_(normal_stddevs)(THCState *state, THCTensor *self, double mean, THCTensor *stddevs)
+{
+  THCTensor_(resizeAs)(state, self, stddevs);
+  THCTensor_(normal)(state, self, 0, 1);
+  THCTensor_(cmul)(state, self, self, stddevs);
+  THCTensor_(add)(state, self, self, ScalarConvert<double, real>::to(mean));
+}
+
+THC_API void THCTensor_(normal_means_stddevs)(THCState *state, THCTensor *self, THCTensor *means, THCTensor *stddevs)
+{
+  THCTensor_(resizeAs)(state, self, means);
+  THCTensor_(normal)(state, self, 0, 1);
+  THCTensor_(cmul)(state, self, self, stddevs);
+  THCTensor_(cadd)(state, self, self, ScalarConvert<int, real>::to(1), means);
+}
+
+THC_API void THCTensor_(logNormal)(THCState* state, THCTensor *self_, double mean, double stdv)
+{
+
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+  ptrdiff_t size = THCTensor_(nElement)(state, self_);
+  if (size == 0) return;
+  THCGenerator* gen = THCRandom_getGenerator(state);
+
+  THCTensor *self = THCTensor_(newContiguous)(state, self_);
+  real *data = THCTensor_(data)(state, self);
+
+  generateLogNormal<real><<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, mean, stdv);
+
+  THCTensor_(freeCopyTo)(state, self, self_);
+};
+
+THC_API void THCTensor_(exponential)(THCState* state, THCTensor *self_, double lambda)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+  ptrdiff_t size = THCTensor_(nElement)(state, self_);
+  if (size == 0) return;
+  THCGenerator* gen = THCRandom_getGenerator(state);
+
+  THCTensor *self = THCTensor_(newContiguous)(state, self_);
+  real *data = THCTensor_(data)(state, self);
+
+  generate_exponential<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, lambda);
+
+  THCTensor_(freeCopyTo)(state, self, self_);
+};
+
+THC_API void THCTensor_(cauchy)(THCState* state, THCTensor *self_, double median, double sigma)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+  ptrdiff_t size = THCTensor_(nElement)(state, self_);
+  if (size == 0) return;
+  THCGenerator* gen = THCRandom_getGenerator(state);
+
+  THCTensor *self = THCTensor_(newContiguous)(state, self_);
+  real *data = THCTensor_(data)(state, self);
+
+  generate_cauchy<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, median, sigma);
+
+  THCTensor_(freeCopyTo)(state, self, self_);
+};
+
+void THCTensor_(renormRows)(struct THCState* state,
+                             THCTensor* t) {
+  THAssert(THCTensor_(nDimension)(state, t) == 2);
+  long rows = THCTensor_(size)(state, t, 0);
+  long cols = THCTensor_(size)(state, t, 1);
+
+  cudaDeviceProp* props = THCState_getCurrentDeviceProperties(state);
+  THAssert(props != NULL);
+
+  int numSM = props->multiProcessorCount;
+  int maxThreads = props->maxThreadsPerBlock;
+
+  dim3 grid(rows < numSM * 4 ? rows : numSM * 4);
+  dim3 block(cols < maxThreads ? cols : maxThreads);
+
+  renormRowsL1<real>
+    <<<grid, block, block.x * sizeof(real),
+    THCState_getCurrentStream(state)>>>(THCTensor_(data)(state, t),
+                                        rows, cols);
+}
+
+THC_API void THCTensor_(multinomial)(struct THCState *state,
+                                      THCudaLongTensor *self,
+                                      THCTensor *prob_dist,
+                                      int n_sample,
+                                      int with_replacement)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, self, prob_dist));
+  THCGenerator* gen = THCRandom_getGenerator(state);
+
+  int inputSize = THCTensor_(nDimension)(state, prob_dist);
+  THArgCheck(inputSize > 0 && inputSize <= 2, 2,
+             "prob_dist must be 1 or 2 dim");
+
+  // Categories are in the innermost dimension
+  long numDist =
+    inputSize == 1 ? 1 : THCTensor_(size)(state, prob_dist, 0);
+  long numCategoriesLong =
+    inputSize == 1 ? THCTensor_(size)(state, prob_dist, 0) :
+    THCTensor_(size)(state, prob_dist, 1);
+
+  // Since the index tensor is float, numCategories cannot exceed max
+  // float integer precision
+  THArgCheck(numCategoriesLong <= FLOAT32_MAX_CONSECUTIVE_INT, 2,
+             "number of categories cannot exceed 2^24");
+  int numCategories = (int) numCategoriesLong;
+
+  THArgCheck(n_sample > 0, 3, "cannot sample <= 0 samples");
+
+  if (!with_replacement) {
+    THArgCheck(n_sample <= numCategories, 2,
+               "cannot sample n_sample > prob_dist:size(1) samples without "
+               "replacement");
+  }
+
+  // It is possible that prob_dist is non-contiguous
+  THCTensor* probDistContig =
+    THCTensor_(newContiguous)(state, prob_dist);
+
+  // Restructure data for 2d
+  if (inputSize == 1) {
+    THCTensor_(resize2d)(state, probDistContig, 1, numCategories);
+  }
+
+  THCudaLongTensor_resize2d(state, self, numDist, n_sample);
+
+  // get current device properties
+  cudaDeviceProp* props = THCState_getCurrentDeviceProperties(state);
+  THAssert(props != NULL);
+  int numSM = props->multiProcessorCount;
+  int maxThreads = props->maxThreadsPerBlock;
+  int maxShared = props->sharedMemPerBlock;
+  int requiredShared = (numCategories < maxThreads ? numCategories : maxThreads)
+                                * (sizeof(real) * sizeof(accreal));
+
+  if (n_sample == 1 && maxShared >= requiredShared) {
+    // Optimized allocation-free implementation
+    // To exploit greater parallelism for the sampling, generate the
+    // Uniform random samples in a separate kernel launch, into
+    // temporarily allocated memory. The device RNG is thread-limited
+    THCTensor *sampled = THCTensor_(newWithSize2d)(state, numDist, n_sample);
+    THCTensor_(uniform)(state, sampled, 0.0, 1.0);
+
+    dim3 block(numCategories < maxThreads ? numCategories : maxThreads);
+    dim3 grid(numDist < numSM * 4 ? numDist : numSM * 4);
+
+    sampleMultinomialOnce<real, accreal>
+      <<<grid, block,
+         requiredShared,
+         THCState_getCurrentStream(state)>>>(
+      THCudaLongTensor_data(state, self),
+      numDist,
+      numCategories,
+      THCTensor_(data)(state, sampled),
+      THCTensor_(data)(state, probDistContig));
+    THCTensor_(free)(state, sampled);
+  } else {
+    // Generic, slow implementation with memory allocations
+
+    // For sampling without replacement, we modify the distribution
+    // for subsequent samples in this space
+    THCTensor* origDist = THCTensor_(new)(state);
+    THCTensor_(resizeAs)(state, origDist, probDistContig);
+    THCTensor_(copy)(state, origDist, probDistContig);
+
+    THCTensor* normDist = THCTensor_(new)(state);
+    THCTensor_(resizeAs)(state, normDist, probDistContig);
+
+    THCTensor* prefixSum = THCTensor_(new)(state);
+
+    // Renorm along rows
+    THCTensor_(copy)(state, normDist, origDist);
+    THCTensor_(renormRows)(state, normDist);
+
+    // Prefix sum along rows
+    THCTensor_(cumsum)(state, prefixSum, normDist, 1);
+
+    if (with_replacement) {
+      // Sample with replacement
+
+      // Binary search is warp divergent (so effectively we're running
+      // with just a single thread), but for better utilization,
+      // we need each block to have at least 4 warps.
+      dim3 block(32, 4);
+
+      // Each warp in a block will generate a sample from one
+      // distribution concurrently.
+      dim3 grid(numDist < MAX_NUM_BLOCKS ? numDist : MAX_NUM_BLOCKS);
+
+      sampleMultinomialWithReplacement
+        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+          gen->gen_states,
+          n_sample,
+          THCudaLongTensor_data(state, self),
+          numDist, numCategories,
+          THCTensor_(data)(state, prefixSum));
+    } else {
+      // Sample without replacement
+
+      // Binary search is warp divergent (so effectively we're running
+      // with just a single thread), but for better utilization,
+      // we need each block to have at least 4 warps.
+      dim3 block(32, 4);
+
+      // Each warp in a block will generate a sample from a different
+      // distribution concurrently.
+      ptrdiff_t numBlocks = THCCeilDiv(numDist, 4L);
+      dim3 grid(numBlocks < MAX_NUM_BLOCKS ? numBlocks : MAX_NUM_BLOCKS);
+
+      for (int sample = 0; sample < n_sample; ++sample) {
+        if (sample > 0) {
+          // Update probabilities
+          // Renorm along rows
+          THCTensor_(copy)(state, normDist, origDist);
+          THCTensor_(renormRows)(state, normDist);
+
+          // Prefix sum along rows
+          THCTensor_(cumsum)(state, prefixSum, normDist, 1);
+        }
+
+        // The kernel can only draw one sample before we have to
+        // recalculate our distribution
+        sampleMultinomialWithoutReplacement
+          <<<grid, block, 0, THCState_getCurrentStream(state)>>>(
+            gen->gen_states,
+            n_sample,
+            sample,
+            THCudaLongTensor_data(state, self),
+            numDist, numCategories,
+            THCTensor_(data)(state, origDist),
+            THCTensor_(data)(state, prefixSum));
+      }
+    }
+
+    THCTensor_(free)(state, prefixSum);
+    THCTensor_(free)(state, normDist);
+    THCTensor_(free)(state, origDist);
+  }
+
+  // Revert data restructuring based on input sizes
+  if (inputSize == 1) {
+    THCudaLongTensor_resize1d(state, self, n_sample);
+
+    // Unfortunately, if prob_dist is contiguous already,
+    // newContiguous is not a private copy, so we have to restructure
+    // this too, so as to not affect prob_dist
+    THCTensor_(resize1d)(state, probDistContig, numCategories);
+  }
+
+  THCTensor_(free)(state, probDistContig);
+}
+
+THC_API void THCTensor_(multinomialAliasSetup)(THCState *state, THCTensor *_probs, THCudaLongTensor *_J, THCTensor *_q){
+  THAssert(THCTensor_(isContiguous)(state, _q));
+  THAssert(THCudaLongTensor_isContiguous(state, _J));
+  THAssert(THCTensor_(isContiguous)(state, _probs));
+  long inputsize = THCTensor_(nElement)(state, _probs);
+  THCudaLongTensor *smaller = THCudaLongTensor_newWithSize1d(state, inputsize);
+  THCudaLongTensor *larger = THCudaLongTensor_newWithSize1d(state, inputsize);
+  THCudaLongTensor *smaller_short = THCudaLongTensor_newWithSize1d(state, inputsize);
+  THCudaLongTensor *larger_short = THCudaLongTensor_newWithSize1d(state, inputsize);
+
+  THCudaLongTensor_resize1d(state, _J, inputsize);
+  THCTensor_(resize1d)(state, _q, inputsize);
+
+  real one = ScalarConvert<long, real>::to(1);
+  int inputBlockDim = THCCeilDiv((int)inputsize + BLOCK_SIZE - 1, BLOCK_SIZE);
+  aliasMultinomialFilter
+    <<<inputBlockDim, BLOCK_SIZE, 0, THCState_getCurrentStream(state) >>>(
+								     THCTensor_(data)(state, _q),
+								     THCTensor_(data)(state, _probs),
+								     THCudaLongTensor_data(state, smaller),
+								     THCudaLongTensor_data(state, larger),
+								     THCudaLongTensor_data(state, _J),
+								     THCudaLongTensor_data(state, smaller_short),
+								     THCudaLongTensor_data(state, larger_short),
+								     one, inputsize
+								     );
+
+  THCudaLongTensor_nonzero(state, smaller_short, smaller);
+  THCudaLongTensor_nonzero(state, larger_short, larger);
+  int h_large_c = THCudaLongTensor_nElement(state, larger_short);
+  THCudaLongTensor_resize1d(state, smaller_short, inputsize);
+  THCudaLongTensor_resize1d(state, larger_short, inputsize);
+  aliasMultinomialSetup
+    <<<1, 1, 0, THCState_getCurrentStream(state)>>>(
+						    THCudaLongTensor_data(state, _J),
+						    THCTensor_(data)(state, _q),
+						    inputsize,
+						    THCudaLongTensor_data(state, smaller_short),
+						    THCudaLongTensor_data(state, larger_short),
+						    static_cast<int>(inputsize - h_large_c), h_large_c
+						    );
+  real q_max = THCTensor_(maxall)(state, _q);
+  condDiv<<<
+    inputBlockDim, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+								      THCTensor_(data)(state, _q),
+								      THCudaLongTensor_data(state, _J),
+								      inputsize, q_max
+								      );
+
+  THCudaLongTensor_free(state, smaller);
+  THCudaLongTensor_free(state, larger);
+  THCudaLongTensor_free(state, smaller_short);
+  THCudaLongTensor_free(state, larger_short);
+}
+
+THC_API void THCTensor_(multinomialAliasDraw)(THCState *state, THCudaLongTensor *self, THCudaLongTensor *_J, THCTensor *_q){
+  THAssert(THCTensor_(isContiguous)(state, _q));
+  THAssert(THCudaLongTensor_isContiguous(state, _J));
+  THCGenerator* gen = THCRandom_getGenerator(state);
+  long K = THCudaLongTensor_nElement(state, _J);
+  long output_nelem = THCudaLongTensor_nElement(state, self);
+  ptrdiff_t size = THCudaLongTensor_nElement(state, self);
+
+  THCTensor *uniform = THCTensor_(newWithSize1d)(state, output_nelem);
+  THCTensor *bernoulli = THCTensor_(newWithSize1d)(state, output_nelem);
+
+  THCTensor_(uniform)(state, uniform, 0, K);
+  THCTensor_(uniform)(state, bernoulli, 0, 1);
+
+  multinomialAliasDrawKernel
+    <<<THCCeilDiv((int)output_nelem+BLOCK_SIZE-1, BLOCK_SIZE), BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+				  static_cast<int>(size),
+				  THCudaLongTensor_data(state, self),
+				  THCudaLongTensor_data(state, _J),
+				  THCTensor_(data)(state, _q),
+				  K,
+				  THCTensor_(data)(state, uniform),
+				  THCTensor_(data)(state, bernoulli)
+				  );
+}
+
+THC_API void THCTensor_(rand)(THCState *state, THCTensor *r_, THLongStorage *size)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, r_));
+  THCTensor_(resize)(state, r_, size, NULL);
+  THCTensor_(uniform)(state, r_, 0, 1);
+}
+
+void THCTensor_(randn)(THCState *state, THCTensor *r_, THLongStorage *size)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, r_));
+  THCTensor_(resize)(state, r_, size, NULL);
+  THCTensor_(normal)(state, r_, 0, 1);
+}
+
+#endif
+
+#if defined(THC_REAL_IS_DOUBLE)
+GENERATE_KERNEL1(generate_bernoulli, double, double p, double, hiprng_uniform_double, x <= p)
+#else
+GENERATE_KERNEL1(generate_bernoulli, real, double p, float, hiprng_uniform, (ScalarConvert<bool, real>::to(x <= p)))
+#endif
+
+THC_API void THCTensor_(bernoulli)(THCState* state, THCTensor *self_, double p)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+  ptrdiff_t size = THCTensor_(nElement)(state, self_);
+  if (size == 0) return;
+  THCGenerator* gen = THCRandom_getGenerator(state);
+  THCTensor *self = THCTensor_(newContiguous)(state, self_);
+  real *data = THCTensor_(data)(state, self);
+
+  generate_bernoulli<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, p);
+
+  THCTensor_(freeCopyTo)(state, self, self_);
+};
+
+void THCTensor_(bernoulli_Tensor)(THCState *state, THCTensor *self, THCTensor* p)
+{
+ #if defined(THC_REAL_IS_FLOAT)
+   THCTensor_(bernoulli_FloatTensor)(state, self, p);
+ #elif defined(THC_REAL_IS_DOUBLE)
+   THCTensor_(bernoulli_DoubleTensor)(state, self, p);
+ #endif
+}
+
+#if defined(__HIP_PLATFORM_HCC__)
+  #define DEFINE_BERNOULLI_TENSOR(NAME, PROB_TYPE, PROB_DATA_TYPE)               \
+  THC_API void THCTensor_(NAME)(THCState* state,                                 \
+          THCTensor *self_, PROB_TYPE *probs_)                                   \
+  {                                                                              \
+    THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, self_, probs_));             \
+    ptrdiff_t size = THCTensor_(nElement)(state, self_);                         \
+    if (size == 0) return;                                                       \
+    THCGenerator* gen = THCRandom_getGenerator(state);                           \
+    THCTensor *self = THCTensor_(newContiguous)(state, self_);                   \
+    PROB_TYPE *probs = PROB_TYPE##_newContiguous(state, probs_);                 \
+    ptrdiff_t prob_size = PROB_TYPE##_nElement(state, probs);                    \
+    real *result_data = THCTensor_(data)(state, self);                           \
+    PROB_DATA_TYPE *probs_data = PROB_TYPE##_data(state, probs);                 \
+                                                                                 \
+    THArgCheck(size == prob_size, 3, "inconsistent tensor size");                \
+                                                                                 \
+    hipLaunchKernelGGL(                                                          \
+      (generate_bernoulli_tensor), NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state), \
+        gen->gen_states, static_cast<int>(size), result_data, probs_data);       \
+                                                                                 \
+    PROB_TYPE##_free(state, probs);                                              \
+    THCTensor_(freeCopyTo)(state, self, self_);                                  \
+  }
+#else
+  #define DEFINE_BERNOULLI_TENSOR(NAME, PROB_TYPE, PROB_DATA_TYPE)               \
+  THC_API void THCTensor_(NAME)(THCState* state,                                 \
+          THCTensor *self_, PROB_TYPE *probs_)                                   \
+  {                                                                              \
+    THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, self_, probs_));             \
+    ptrdiff_t size = THCTensor_(nElement)(state, self_);                         \
+    if (size == 0) return;                                                       \
+    THCGenerator* gen = THCRandom_getGenerator(state);                              \
+    THCTensor *self = THCTensor_(newContiguous)(state, self_);                   \
+    PROB_TYPE *probs = PROB_TYPE##_newContiguous(state, probs_);                 \
+    ptrdiff_t prob_size = PROB_TYPE##_nElement(state, probs);                    \
+    real *result_data = THCTensor_(data)(state, self);                           \
+    PROB_DATA_TYPE *probs_data = PROB_TYPE##_data(state, probs);                 \
+                                                                                 \
+    THArgCheck(size == prob_size, 3, "inconsistent tensor size");                \
+                                                                                 \
+    generate_bernoulli_tensor<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>( \
+        gen->gen_states, size, result_data, probs_data);                         \
+                                                                                 \
+    PROB_TYPE##_free(state, probs);                                              \
+    THCTensor_(freeCopyTo)(state, self, self_);                                  \
+  }
+#endif
+
+DEFINE_BERNOULLI_TENSOR(bernoulli_FloatTensor, THCudaTensor, float)
+DEFINE_BERNOULLI_TENSOR(bernoulli_DoubleTensor, THCudaDoubleTensor, double)
+
+#if defined(THC_REAL_IS_DOUBLE)
+GENERATE_KERNEL1(generate_geometric, double, double p, double, hiprng_uniform_double, ceil(log(x) / log(1-p)))
+#else
+GENERATE_KERNEL1(generate_geometric, real, double p, float, hiprng_uniform, (ScalarConvert<float, real>::to(ceilf(logf(x) / log(1-p)))))
+#endif
+
+#if defined(THC_REAL_IS_LONG) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_FLOAT)
+#define CURAND64(STATE) (((uint64_t) hiprng(&state[blockIdx.x])) << 32) | (uint64_t) hiprng(&state[blockIdx.x])
+GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, hiprng, (real)(x % range + base))
+GENERATE_KERNEL2(generate_random_64, real, int64_t base, uint64_t range, uint64_t, CURAND64, (real)(x % range + base))
+#elif defined(THC_REAL_IS_HALF)
+GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, hiprng, (ScalarConvert<uint32_t, real>::to(x % range + base)))
+#else
+GENERATE_KERNEL2(generate_random, real, int32_t base, uint32_t range, uint32_t, hiprng, (real)(x % range + base))
+#endif
+
+THC_API void THCTensor_(geometric)(THCState* state, THCTensor *self_, double p)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+  ptrdiff_t size = THCTensor_(nElement)(state, self_);
+  if (size == 0) return;
+  THCGenerator* gen = THCRandom_getGenerator(state);
+
+  THCTensor *self = THCTensor_(newContiguous)(state, self_);
+  real *data = THCTensor_(data)(state, self);
+
+  generate_geometric<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, p);
+
+  THCTensor_(freeCopyTo)(state, self, self_);
+};
+
+THC_API void THCTensor_(clampedRandom)(THCState* state, THCTensor *self_, int64_t min_val, int64_t max_val)
+{
+  THArgCheck(min_val < max_val, 2,
+             "max must be greater than min, but got: min = %lld, max = %lld", min_val, max_val);
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+  ptrdiff_t size = THCTensor_(nElement)(state, self_);
+  if (size == 0) return;
+  THCGenerator* gen = THCRandom_getGenerator(state);
+  THCTensor *self = THCTensor_(newContiguous)(state, self_);
+  real *data = THCTensor_(data)(state, self);
+
+  uint64_t range = max_val - min_val;
+
+#if defined(THC_REAL_IS_LONG) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_FLOAT)
+  if (range > 1ULL << 32) {
+    generate_random_64<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+        gen->gen_states, static_cast<int>(size), data, static_cast<int64_t>(min_val), static_cast<uint64_t>(range));
+  } else {
+#endif
+    generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+        gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(min_val), static_cast<uint32_t>(range));
+#if defined(THC_REAL_IS_LONG) || defined(THC_REAL_IS_DOUBLE) || defined(THC_REAL_IS_FLOAT)
+  }
+#endif
+
+  THCTensor_(freeCopyTo)(state, self, self_);
+};
+
+THC_API void THCTensor_(cappedRandom)(THCState* state, THCTensor *self_, int64_t max_val)
+{
+  THCTensor_(clampedRandom)(state, self_, 0LL, max_val);
+};
+
+#define HLF_MANT_DIG 11
+
+THC_API void THCTensor_(random)(THCState* state, THCTensor *self_)
+{
+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 1, self_));
+  ptrdiff_t size = THCTensor_(nElement)(state, self_);
+  if (size == 0) return;
+  THCGenerator* gen = THCRandom_getGenerator(state);
+  THCTensor *self = THCTensor_(newContiguous)(state, self_);
+  real *data = THCTensor_(data)(state, self);
+
+#if defined(THC_REAL_IS_HALF)
+  generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(0UL), static_cast<uint32_t>((1UL << HLF_MANT_DIG) + 1));
+#elif defined(THC_REAL_IS_FLOAT)
+  generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(0UL), static_cast<uint32_t>((1UL << FLT_MANT_DIG) + 1));
+#elif defined(THC_REAL_IS_DOUBLE)
+  generate_random_64<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, static_cast<int64_t>(0ULL), static_cast<uint64_t>((1ULL << DBL_MANT_DIG) + 1));
+#elif defined(THC_REAL_IS_LONG)
+  generate_random_64<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, static_cast<int64_t>(0ULL), static_cast<uint64_t>(std::numeric_limits<real>::max()) + 1);
+#else
+  generate_random<<<NUM_BLOCKS, BLOCK_SIZE, 0, THCState_getCurrentStream(state)>>>(
+      gen->gen_states, static_cast<int>(size), data, static_cast<int32_t>(0UL), static_cast<uint32_t>(std::numeric_limits<real>::max()) + 1);
+#endif
+
+  THCTensor_(freeCopyTo)(state, self, self_);
+};
+#undef NUM_BLOCKS
+
+#endif
diff --git a/src/THC/generic/THCTensorScatterGather.cu b/src/THC/generic/THCTensorScatterGather.cu
index 7f1f99165..0bb571981 100644
--- a/src/THC/generic/THCTensorScatterGather.cu
+++ b/src/THC/generic/THCTensorScatterGather.cu
@@ -2,10 +2,19 @@
 #define THC_GENERIC_FILE "generic/THCTensorScatterGather.cu"
 #else
 
-#define RUN(TYPE, DIMS, REAL)                                           \
-  THCudaTensor_gatherKernel<TYPE, REAL, DIMS>                                \
-  <<<grid, block, 0, THCState_getCurrentStream(state)>>>(               \
-    tensorInfo, srcInfo, indexInfo, dim, (TYPE)totalElements);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define RUN(TYPE, DIMS, REAL)                                           \
+    hipLaunchKernelGGL(                                                   \
+      (THCudaTensor_gatherKernel<TYPE, REAL, DIMS>),                      \
+        grid, block, 0, THCState_getCurrentStream(state),                 \
+        tensorInfo, srcInfo,      \
+        indexInfo, dim, (TYPE)totalElements);
+#else
+  #define RUN(TYPE, DIMS, REAL)                                           \
+    THCudaTensor_gatherKernel<TYPE, REAL, DIMS>                                \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(               \
+      tensorInfo, srcInfo, indexInfo, dim, (TYPE)totalElements);
+#endif
 
 void THCTensor_(gather)(THCState* state, THCTensor *tensor,
                          THCTensor *src, int dim, THCudaLongTensor *index) {
@@ -96,10 +105,19 @@ void THCTensor_(gather)(THCState* state, THCTensor *tensor,
 #undef RUN
 
 
-#define RUN(TYPE, DIMS, REAL)                                           \
-  THCudaTensor_scatterKernel<TYPE, REAL, DIMS>                               \
-  <<<grid, block, 0, THCState_getCurrentStream(state)>>>(               \
-    tensorInfo, srcInfo, indexInfo, dim, (TYPE)totalElements);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define RUN(TYPE, DIMS, REAL)                                           \
+    hipLaunchKernelGGL(                                                   \
+      (THCudaTensor_scatterKernel<TYPE, REAL, DIMS>),                     \
+        grid, block, 0, THCState_getCurrentStream(state),                 \
+        tensorInfo, srcInfo,                                              \
+        indexInfo, dim, (TYPE)totalElements);
+#else
+  #define RUN(TYPE, DIMS, REAL)                                           \
+    THCudaTensor_scatterKernel<TYPE, REAL, DIMS>                               \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(               \
+      tensorInfo, srcInfo, indexInfo, dim, (TYPE)totalElements);
+#endif
 
 void THCTensor_(scatter)(THCState* state, THCTensor *tensor, int dim, THCudaLongTensor *index, THCTensor *src) {
   THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, tensor, src));
@@ -184,10 +202,19 @@ void THCTensor_(scatter)(THCState* state, THCTensor *tensor, int dim, THCudaLong
 
 #undef RUN
 
-#define RUN(TYPE, DIMS, REAL)                                           \
-  THCudaTensor_scatterAddKernel<TYPE, REAL, DIMS>                               \
-  <<<grid, block, 0, THCState_getCurrentStream(state)>>>(               \
-    tensorInfo, srcInfo, indexInfo, dim, (TYPE)totalElements);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define RUN(TYPE, DIMS, REAL)                                           \
+    hipLaunchKernelGGL(                                                   \
+      (THCudaTensor_scatterAddKernel<TYPE, REAL, DIMS>),                  \
+        grid, block, 0, THCState_getCurrentStream(state),                 \
+        tensorInfo, srcInfo,                                              \
+        indexInfo, dim, (TYPE)totalElements);
+#else
+  #define RUN(TYPE, DIMS, REAL)                                           \
+    THCudaTensor_scatterAddKernel<TYPE, REAL, DIMS>                               \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(               \
+      tensorInfo, srcInfo, indexInfo, dim, (TYPE)totalElements);
+#endif
 
 void THCTensor_(scatterAdd)(THCState* state, THCTensor *tensor, int dim, THCudaLongTensor *index, THCTensor *src) {
   THCAssertSameGPU(THCTensor_(checkGPU)(state, 2, tensor, src));
@@ -271,10 +298,19 @@ void THCTensor_(scatterAdd)(THCState* state, THCTensor *tensor, int dim, THCudaL
 
 #undef RUN
 
-#define RUN(TYPE, DIMS, REAL)                                           \
-  THCudaTensor_scatterFillKernel<TYPE, REAL, DIMS>                           \
-      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(      \
-          tensorInfo, indexInfo, value, dim, (TYPE)totalElements);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define RUN(TYPE, DIMS, REAL)                                           \
+    hipLaunchKernelGGL(                                                   \
+      (THCudaTensor_scatterFillKernel<TYPE, REAL, DIMS>),                 \
+        grid, block, 0, THCState_getCurrentStream(state),                 \
+        tensorInfo, indexInfo,    \
+        value, dim, (TYPE)totalElements);
+#else
+  #define RUN(TYPE, DIMS, REAL)                                           \
+    THCudaTensor_scatterFillKernel<TYPE, REAL, DIMS>                           \
+        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(      \
+            tensorInfo, indexInfo, value, dim, (TYPE)totalElements);
+#endif
 
 void
 THCTensor_(scatterFill)(THCState* state, THCTensor *tensor,
diff --git a/src/THC/generic/THCTensorSort.cu b/src/THC/generic/THCTensorSort.cu
index 06ed71f82..9012e8e8f 100644
--- a/src/THC/generic/THCTensorSort.cu
+++ b/src/THC/generic/THCTensorSort.cu
@@ -45,37 +45,73 @@ THC_API void THCTensor_(sortKeyValueInplace)(THCState* state,
     THError("Slice to sort is too large");
   }
 
-#define HANDLE_CASE(TYPE, A, SIZE)                                      \
-  do {                                                                  \
-    int blockSize = SIZE / 2;                                           \
-    if (blockSize < 1) {                                                \
-      blockSize = 1;                                                    \
-    }                                                                   \
-                                                                        \
-    dim3 block(blockSize);                                              \
-                                                                        \
-    if (dir) {                                                          \
-      bitonicSortKVInPlace<real, int64_t, A, -1, GTComp<real>, TYPE, SIZE> \
-        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(         \
-          keyInfo,                                                      \
-          keySlices,                                                    \
-          (TYPE) keySliceSize,                                          \
-          (TYPE) keyInfo.strides[collapseKeyDim],                       \
-          valueInfo,                                                    \
-          (TYPE) valueInfo.strides[collapseValueDim],                   \
-          GTComp<real>());                                              \
-    } else {                                                            \
-      bitonicSortKVInPlace<real, int64_t, A, -1, LTComp<real>, TYPE, SIZE> \
-        <<<grid, block, 0, THCState_getCurrentStream(state)>>>(         \
-          keyInfo,                                                      \
-          keySlices,                                                    \
-          (TYPE) keySliceSize,                                          \
-          (TYPE) keyInfo.strides[collapseKeyDim],                       \
-          valueInfo,                                                    \
-          (TYPE) valueInfo.strides[collapseValueDim],                   \
-          LTComp<real>());                                              \
-    }                                                                   \
-  } while (0)
+#if defined(__HIP_PLATFORM_HCC__)
+  #define HANDLE_CASE(TYPE, A, SIZE)                                      \
+    do {                                                                  \
+      int blockSize = SIZE / 2;                                           \
+      if (blockSize < 1) {                                                \
+        blockSize = 1;                                                    \
+      }                                                                   \
+                                                                          \
+      dim3 block(blockSize);                                              \
+                                                                          \
+      if (dir) {                                                          \
+        hipLaunchKernelGGL(                                               \
+          (bitonicSortKVInPlace<real, int64_t, A, -1, GTComp<real>, TYPE, SIZE>), \
+            grid, block, 0, THCState_getCurrentStream(state),             \
+            keyInfo,                                                      \
+            keySlices,                                                    \
+            (TYPE) keySliceSize,                                          \
+            (TYPE) keyInfo.strides[collapseKeyDim],                       \
+            valueInfo,                                                    \
+            (TYPE) valueInfo.strides[collapseValueDim],                   \
+            GTComp<real>());                                              \
+      } else {                                                            \
+        hipLaunchKernelGGL(                                               \
+          (bitonicSortKVInPlace<real, int64_t, A, -1, LTComp<real>, TYPE, SIZE>), \
+            grid, block, 0, THCState_getCurrentStream(state),             \
+            keyInfo,                                                      \
+            keySlices,                                                    \
+            (TYPE) keySliceSize,                                          \
+            (TYPE) keyInfo.strides[collapseKeyDim],                       \
+            valueInfo,                                                    \
+            (TYPE) valueInfo.strides[collapseValueDim],                   \
+            LTComp<real>());                                              \
+      }                                                                   \
+    } while (0)
+#else
+  #define HANDLE_CASE(TYPE, A, SIZE)                                      \
+    do {                                                                  \
+      int blockSize = SIZE / 2;                                           \
+      if (blockSize < 1) {                                                \
+        blockSize = 1;                                                    \
+      }                                                                   \
+                                                                          \
+      dim3 block(blockSize);                                              \
+                                                                          \
+      if (dir) {                                                          \
+        bitonicSortKVInPlace<real, int64_t, A, -1, GTComp<real>, TYPE, SIZE> \
+          <<<grid, block, 0, THCState_getCurrentStream(state)>>>(         \
+            keyInfo,                                                      \
+            keySlices,                                                    \
+            (TYPE) keySliceSize,                                          \
+            (TYPE) keyInfo.strides[collapseKeyDim],                       \
+            valueInfo,                                                    \
+            (TYPE) valueInfo.strides[collapseValueDim],                   \
+            GTComp<real>());                                              \
+      } else {                                                            \
+        bitonicSortKVInPlace<real, int64_t, A, -1, LTComp<real>, TYPE, SIZE> \
+          <<<grid, block, 0, THCState_getCurrentStream(state)>>>(         \
+            keyInfo,                                                      \
+            keySlices,                                                    \
+            (TYPE) keySliceSize,                                          \
+            (TYPE) keyInfo.strides[collapseKeyDim],                       \
+            valueInfo,                                                    \
+            (TYPE) valueInfo.strides[collapseValueDim],                   \
+            LTComp<real>());                                              \
+      }                                                                   \
+    } while (0)
+#endif
 
 #define HANDLE_SORT_CASE(TYPE, A)                       \
   {                                                     \
@@ -223,38 +259,46 @@ void sortViaThrust(THCState* state,
   // Fill the indices with a global index across all slices
   thrust::counting_iterator<int64_t> countIter(0);
 
+#if defined (__NVCC__)
   thrust::copy(
 #if CUDA_VERSION >= 7000
     thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
     countIter, countIter + totalElements, indexIter);
+#endif
 
   // First, we sort globally (across all slices) according to key
   // (the values we're sorting)
   if (dir) {
+#if defined (__NVCC__)
     thrust::stable_sort_by_key(
 #if CUDA_VERSION >= 7000
       thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
       keyIter, keyIter + totalElements, indexIter, ThrustGTOp<real>());
+#endif
   } else {
+#if defined (__NVCC__)
     thrust::stable_sort_by_key(
 #if CUDA_VERSION >= 7000
       thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
       keyIter, keyIter + totalElements, indexIter, ThrustLTOp<real>());
+#endif
   }
 
   // Then, re-sort according to slice that each index is
   // in. This completes the segment sort in Thrust, since we're
   // stably sorting here, preserving the relative order of values
   // per each slice
+#if defined (__NVCC__)
   thrust::stable_sort_by_key(
 #if CUDA_VERSION >= 7000
     thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
     indexIter, indexIter + totalElements, keyIter,
     SliceComp(sliceSize));
+#endif
 
   // Translate the global integer 0-based index to a per-slice real
   // Lua index
diff --git a/src/THC/generic/THCTensorTopK.cu b/src/THC/generic/THCTensorTopK.cu
index 19804bb1e..63966ab85 100644
--- a/src/THC/generic/THCTensorTopK.cu
+++ b/src/THC/generic/THCTensorTopK.cu
@@ -28,21 +28,40 @@ THC_API void THCTensor_(topk)(THCState* state,
   THCudaLongTensor_resize(state, indices, topKSize, NULL);
   THLongStorage_free(topKSize);
 
-#define RUN_K(INDEX_T, DIM, DIR)                                        \
-  gatherTopK<real, INDEX_T, DIM, DIR>                                   \
-    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \
-      inputInfo,                                                        \
-      sliceSize,                                                        \
-      k,                                                                \
-      inputSlices,                                                      \
-      /* The actual dimension that the k-selection is running in */     \
-      /* may have changed from collapseDims() */                        \
-      inputInfo.strides[collapseInputDim],                              \
-      topKInfo,                                                         \
-      topKSlices,                                                       \
-      topKInfo.strides[collapseTopKDim],                                \
-      indicesInfo,                                                      \
-      indicesInfo.strides[collapseIndicesDim])
+#if defined(__HIP_PLATFORM_HCC__)
+  #define RUN_K(INDEX_T, DIM, DIR)                                        \
+    hipLaunchKernelGGL(                                                   \
+      (gatherTopK<real, INDEX_T, DIM, DIR>),                              \
+        grid, block, 0, THCState_getCurrentStream(state),                 \
+        inputInfo,                                                        \
+        sliceSize,                                                        \
+        k,                                                                \
+        inputSlices,                                                      \
+        /* The actual dimension that the k-selection is running in */     \
+        /* may have changed from collapseDims() */                        \
+        inputInfo.strides[collapseInputDim],                              \
+        topKInfo,                                                         \
+        topKSlices,                                                       \
+        topKInfo.strides[collapseTopKDim],                                \
+        indicesInfo,                                                      \
+        indicesInfo.strides[collapseIndicesDim])
+#else
+  #define RUN_K(INDEX_T, DIM, DIR)                                        \
+    gatherTopK<real, INDEX_T, DIM, DIR>                                   \
+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \
+        inputInfo,                                                        \
+        sliceSize,                                                        \
+        k,                                                                \
+        inputSlices,                                                      \
+        /* The actual dimension that the k-selection is running in */     \
+        /* may have changed from collapseDims() */                        \
+        inputInfo.strides[collapseInputDim],                              \
+        topKInfo,                                                         \
+        topKSlices,                                                       \
+        topKInfo.strides[collapseTopKDim],                                \
+        indicesInfo,                                                      \
+        indicesInfo.strides[collapseIndicesDim])
+#endif
 
 #define RUN_DIR(INDEX_T, DIM)                   \
   if (dir) {                                    \
diff --git a/src/THCS/CMakeLists.txt.hip b/src/THCS/CMakeLists.txt.hip
new file mode 100644
index 000000000..e633ea6af
--- /dev/null
+++ b/src/THCS/CMakeLists.txt.hip
@@ -0,0 +1,102 @@
+# HIP_PATH
+IF(NOT DEFINED $ENV{HIP_PATH})
+  SET(HIP_PATH /opt/rocm/hip)
+ELSE()
+  SET(HIP_PATH $ENV{HIP_PATH})
+ENDIF()
+
+# HCC_PATH
+IF(NOT DEFINED $ENV{HCC_PATH})
+  SET(HCC_PATH /opt/rocm/hcc)
+ELSE()
+  SET(HCC_PATH $ENV{HCC_PATH})
+ENDIF()
+
+# HIPBLAS_PATH
+IF(NOT DEFINED $ENV{HIPBLAS_PATH})
+  SET(HIPBLAS_PATH /opt/rocm/hipblas)
+ELSE()
+  SET(HIPBLAS_PATH $ENV{HIPBLAS_PATH})
+ENDIF()
+
+# HIPRNG_PATH
+IF(NOT DEFINED $ENV{HIPRNG_PATH})
+  SET(HIPRNG_PATH /opt/rocm/hcrng)
+ELSE()
+  SET(HIPRNG_PATH $ENV{HIPRNG_PATH})
+ENDIF()
+
+# HIPSPARSE_PATH
+IF(NOT DEFINED $ENV{HIPSPARSE_PATH})
+  SET(HIPSPARSE_PATH /opt/rocm/hcsparse)
+ELSE()
+  SET(HIPSPARSE_PATH $ENV{HIPSPARSE_PATH})
+ENDIF()
+
+SET(THRUST_PATH "/root/Thrust")
+
+# load HIP cmake module and load platform id
+SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH} "${HIP_PATH}/cmake")
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig -P OUTPUT_VARIABLE PLATFORM)
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig --cpp_config OUTPUT_VARIABLE HIP_CXX_FLAGS)
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+
+SET(HIP_CXX_FLAGS "-D__HIP_PLATFORM_HCC__ -I/opt/rocm/hip/include -I/opt/rocm/hcc/include" ${HIP_CXX_FLAGS})
+
+SET(HIP_HIPCC_FLAGS "-DGENERIC_GRID_LAUNCH=1 ${CMAKE_CXX_FLAGS}")
+SET(HIP_HIPCC_FLAGS "-DGENERIC_GRID_LAUNCH=1 ${HIP_HIPCC_FLAGS}")
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+SET(CMAKE_C_COMPILER ${HIP_HIPCC_EXECUTABLE})
+SET(CMAKE_C_FLAGS "-std=c99 -Werror=implicit-function-declaration ${CMAKE_C_FLAGS}")
+SET(CMAKE_C_FLAGS "${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+SET(CMAKE_CXX_FLAGS  "-std=c++11 ${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+
+SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH})
+
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+
+MESSAGE(STATUS "ROCM TRUE:")
+MESSAGE(STATUS "CMAKE_CXX_COMPILER: " ${CMAKE_CXX_COMPILER})
+
+INCLUDE_DIRECTORIES(${HIPBLAS_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPSPARSE_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPRNG_PATH}/include)
+INCLUDE_DIRECTORIES(${THRUST_PATH})
+
+set(ATen_CUDA_INCLUDE ${ATen_CUDA_INCLUDE}
+  ${TH_INCLUDE_PATH}/TH
+  "${CMAKE_CURRENT_BINARY_DIR}"
+PARENT_SCOPE)
+
+
+set(ATen_CUDA_SRCS ${ATen_CUDA_SRCS}
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCSTensor.cpp
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCSTensor.cu
+  ${CMAKE_CURRENT_SOURCE_DIR}/THCSparse.cu
+  PARENT_SCOPE
+)
+
+INSTALL(FILES
+          THCS.h
+          THCSTensor.h
+          THCSGenerateAllTypes.h
+          THCSGenerateByteType.h
+          THCSGenerateCharType.h
+          THCSGenerateShortType.h
+          THCSGenerateIntType.h
+          THCSGenerateLongType.h
+          THCSGenerateHalfType.h
+          THCSGenerateFloatType.h
+          THCSGenerateFloatTypes.h
+          THCSGenerateDoubleType.h
+          THCSparse.h
+          DESTINATION "${ATEN_INSTALL_INCLUDE_SUBDIR}/THCS")
+
+INSTALL(FILES
+          generic/THCSTensor.cpp
+          generic/THCSTensor.cu
+          generic/THCSTensor.h
+          generic/THCSTensorMath.h
+          generic/THCSTensorMath.cu
+          DESTINATION "${ATEN_INSTALL_INCLUDE_SUBDIR}/THCS/generic")
diff --git a/src/THCS/THCSparse.cu b/src/THCS/THCSparse.cu
index 90b9bc50e..bff7d167e 100644
--- a/src/THCS/THCSparse.cu
+++ b/src/THCS/THCSparse.cu
@@ -1,6 +1,7 @@
 #include "THCSparse.h"
 
 void THCudaSparse_Xcoo2csr(THCState *state, const int *coorowind, int64_t nnz, int64_t m, int *csrrowptr) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THAssertMsg((m <= INT_MAX) && (nnz <= INT_MAX),
     "cusparseXcoo2csr only supports m, nnz with the bound [val] <= %d",
     INT_MAX);
@@ -9,8 +10,10 @@ void THCudaSparse_Xcoo2csr(THCState *state, const int *coorowind, int64_t nnz, i
   THCusparseCheck(cusparseXcoo2csr(handle, coorowind, nnz, m, csrrowptr,
     TH_INDEX_BASE ? CUSPARSE_INDEX_BASE_ONE : CUSPARSE_INDEX_BASE_ZERO
   ));
+#endif
 }
 
+#if !defined(__HIP_PLATFORM_HCC__)
 cusparseOperation_t convertTransToCusparseOperation(char trans) {
   if (trans == 't') return CUSPARSE_OPERATION_TRANSPOSE;
   else if (trans == 'n') return CUSPARSE_OPERATION_NON_TRANSPOSE;
@@ -20,9 +23,11 @@ cusparseOperation_t convertTransToCusparseOperation(char trans) {
     return CUSPARSE_OPERATION_TRANSPOSE;
   }
 }
+#endif
 
 void adjustLd(char transb, int64_t m, int64_t n, int64_t k, int64_t *ldb, int64_t *ldc)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   int transb_ = ((transb == 't') || (transb == 'T'));
 
   if(n == 1)
@@ -38,11 +43,13 @@ void adjustLd(char transb, int64_t m, int64_t n, int64_t k, int64_t *ldb, int64_
     if(n == 1)
       *ldb = k;
   }
+#endif
 }
 
 /* Level 3 */
 void THCudaSparse_Scsrmm2(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k, int64_t nnz, float alpha, float *csrvala, int *csrrowptra, int *csrcolinda, float *b, int64_t ldb, float beta, float *c, int64_t ldc)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   adjustLd(transb, m, n, k, &ldb, &ldc);
   cusparseOperation_t opa = convertTransToCusparseOperation(transa);
   cusparseOperation_t opb = convertTransToCusparseOperation(transb);
@@ -65,10 +72,12 @@ void THCudaSparse_Scsrmm2(THCState *state, char transa, char transb, int64_t m,
   cusparseSetMatIndexBase(&desc, CUSPARSE_INDEX_BASE_ONE);
 #endif
   THCusparseCheck(cusparseScsrmm2(handle, opa, opb, i_m, i_n, i_k, i_nnz, &alpha, desc, csrvala, csrrowptra, csrcolinda, b, i_ldb, &beta, c, i_ldc));
+#endif
 }
 
 void THCudaSparse_Dcsrmm2(THCState *state, char transa, char transb, int64_t m, int64_t n, int64_t k, int64_t nnz, double alpha, double *csrvala, int *csrrowptra, int *csrcolinda, double *b, int64_t ldb, double beta, double *c, int64_t ldc)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   adjustLd(transb, m, n, k, &ldb, &ldc);
   cusparseOperation_t opa = convertTransToCusparseOperation(transa);
   cusparseOperation_t opb = convertTransToCusparseOperation(transb);
@@ -91,10 +100,12 @@ void THCudaSparse_Dcsrmm2(THCState *state, char transa, char transb, int64_t m,
   cusparseSetMatIndexBase(&desc, CUSPARSE_INDEX_BASE_ONE);
 #endif
   THCusparseCheck(cusparseDcsrmm2(handle, opa, opb, i_m, i_n, i_k, i_nnz, &alpha, desc, csrvala, csrrowptra, csrcolinda, b, i_ldb, &beta, c, i_ldc));
+#endif
 }
 
 /* format conversion */
 void THCudaSparse_CreateIdentityPermutation(THCState *state, int64_t nnz, int *P) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THAssertMsg((nnz <= INT_MAX),
     "Xcsrsort_bufferSizeExt only supports m, n, nnz with the bound [val] <= %d",
     INT_MAX);
@@ -103,10 +114,12 @@ void THCudaSparse_CreateIdentityPermutation(THCState *state, int64_t nnz, int *P
   cusparseHandle_t handle = THCState_getCurrentSparseHandle(state);
   cusparseSetStream(handle, THCState_getCurrentStream(state));
   cusparseCreateIdentityPermutation(handle, i_nnz, P);
+#endif
 }
 
 void THCudaSparse_Xcsrsort_bufferSizeExt(THCState *state, int64_t m, int64_t n, int64_t nnz, const int *csrRowPtr, const int *csrColInd, size_t *pBufferSizeInBytes)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   THAssertMsg((m <= INT_MAX) && (n <= INT_MAX) && (nnz <= INT_MAX),
     "Xcsrsort_bufferSizeExt only supports m, n, nnz with the bound [val] <= %d",
     INT_MAX);
@@ -117,10 +130,12 @@ void THCudaSparse_Xcsrsort_bufferSizeExt(THCState *state, int64_t m, int64_t n,
   cusparseHandle_t handle = THCState_getCurrentSparseHandle(state);
   cusparseSetStream(handle, THCState_getCurrentStream(state));
   THCusparseCheck(cusparseXcsrsort_bufferSizeExt(handle, i_m, i_n, i_nnz, csrRowPtr, csrColInd, pBufferSizeInBytes));
+#endif
 }
 
 void THCudaSparse_Xcsrsort(THCState *state, int64_t m, int64_t n, int64_t nnz, const int *csrRowPtr, int *csrColInd, int *P, void *pBuffer)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   THAssertMsg((m <= INT_MAX) && (n <= INT_MAX) && (nnz <= INT_MAX),
     "Xcsrsort only supports m, n, nnz with the bound [val] <= %d",
     INT_MAX);
@@ -136,10 +151,12 @@ void THCudaSparse_Xcsrsort(THCState *state, int64_t m, int64_t n, int64_t nnz, c
   cusparseSetMatIndexBase(&desc, CUSPARSE_INDEX_BASE_ONE);
 #endif
   THCusparseCheck(cusparseXcsrsort(handle, i_m, i_n, i_nnz, desc, csrRowPtr, csrColInd, P, pBuffer));
+#endif
 }
 
 void THCudaSparse_Xcoosort_bufferSizeExt(THCState *state, int64_t m, int64_t n, int64_t nnz, const int *cooRows, const int *cooCols, size_t *pBufferSizeInBytes)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   THAssertMsg((m <= INT_MAX) && (n <= INT_MAX) && (nnz <= INT_MAX),
     "Xcoosort_bufferSizeExt only supports m, n, nnz with the bound [val] <= %d",
     INT_MAX);
@@ -150,10 +167,12 @@ void THCudaSparse_Xcoosort_bufferSizeExt(THCState *state, int64_t m, int64_t n,
   cusparseHandle_t handle = THCState_getCurrentSparseHandle(state);
   cusparseSetStream(handle, THCState_getCurrentStream(state));
   THCusparseCheck(cusparseXcoosort_bufferSizeExt(handle, i_m, i_n, i_nnz, cooRows, cooCols, pBufferSizeInBytes));
+#endif
 }
 
 void THCudaSparse_XcoosortByRow(THCState *state, int64_t m, int64_t n, int64_t nnz, int *cooRows, int *cooCols, int *P, void *pBuffer)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   THAssertMsg((m <= INT_MAX) && (n <= INT_MAX) && (nnz <= INT_MAX),
     "XcoosortByRow only supports m, n, nnz with the bound [val] <= %d",
     INT_MAX);
@@ -164,4 +183,5 @@ void THCudaSparse_XcoosortByRow(THCState *state, int64_t m, int64_t n, int64_t n
   cusparseHandle_t handle = THCState_getCurrentSparseHandle(state);
   cusparseSetStream(handle, THCState_getCurrentStream(state));
   THCusparseCheck(cusparseXcoosortByRow(handle, i_m, i_n, i_nnz, cooRows, cooCols, P, pBuffer));
+#endif
 }
diff --git a/src/THCS/generic/THCSTensor.cpp b/src/THCS/generic/THCSTensor.cpp
index eaa311835..3bc746f44 100644
--- a/src/THCS/generic/THCSTensor.cpp
+++ b/src/THCS/generic/THCSTensor.cpp
@@ -42,20 +42,28 @@ THLongStorage *THCSTensor_(newSizeOf)(THCState *state, THCSTensor *self)
 
 /*** TODO: watch out for memory leaks ***/
 THCIndexTensor *THCSTensor_(newIndices)(THCState *state, const THCSTensor *self) {
+#if !defined(__HIP_PLATFORM_HCC__)
   if (self->nnz == 0) {
     // Narrows don't work on 0-length tensors
     THCIndexTensor_(retain)(state, self->indices);
     return self->indices;
   }
   return THCIndexTensor_(newNarrow)(state, self->indices, 1, 0, self->nnz);
+#else
+  return NULL;
+#endif
 }
 
 THCTensor *THCSTensor_(newValues)(THCState *state, const THCSTensor *self) {
+#if !defined(__HIP_PLATFORM_HCC__)
   if (self->nnz == 0) {
     THCTensor_(retain)(state, self->values);
     return self->values;
   }
   return THCTensor_(newNarrow)(state, self->values, 0, 0, self->nnz);
+#else
+  return NULL;
+#endif
 }
 
 
@@ -66,6 +74,7 @@ THCTensor *THCSTensor_(newValues)(THCState *state, const THCSTensor *self) {
 /*** Helper methods ***/
 static void THCSTensor_(rawInit)(THCState *state, THCSTensor *self)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   self->size = NULL;
   self->indices = THCIndexTensor_(new)(state);
   self->values = THCTensor_(new)(state);
@@ -75,9 +84,11 @@ static void THCSTensor_(rawInit)(THCState *state, THCSTensor *self)
   self->nnz = 0;
   // self->flag = TH_TENSOR_REFCOUNTED;
   self->refcount = 1;
+#endif
 }
 
 THCSTensor* THCSTensor_(rawResize)(THCState *state, THCSTensor *self, int nDimI, int nDimV, int64_t *size) {
+#if !defined(__HIP_PLATFORM_HCC__)
   // Only resize valid sizes into tensor.
   self->size = (int64_t *)THRealloc(self->size, sizeof(int64_t)*(nDimI + nDimV));
 
@@ -86,11 +97,13 @@ THCSTensor* THCSTensor_(rawResize)(THCState *state, THCSTensor *self, int nDimI,
   }
   self->nDimensionI = nDimI;
   self->nDimensionV = nDimV;
-  return self;
+#endif
+return self;
 }
 
 // directly assign without cloning or retaining (internal method)
 THCSTensor* THCSTensor_(_move)(THCState *state, THCSTensor *self, THCIndexTensor *indices, THCTensor *values) {
+#if !defined(__HIP_PLATFORM_HCC__)
   int empty = THCTensor_(nDimension)(state, values) == 0;
   if (!empty) {
     THArgCheck(THCIndexTensor_(nDimension)(state, indices) == 2, 2,
@@ -113,11 +126,18 @@ THCSTensor* THCSTensor_(_move)(THCState *state, THCSTensor *self, THCIndexTensor
   self->coalesced = 0;
 
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor* THCSTensor_(_set)(THCState *state, THCSTensor *self, THCIndexTensor *indices, THCTensor *values) {
+#if !defined(__HIP_PLATFORM_HCC__)
   // Note: Not like torch.set, this is an internal method
   return THCSTensor_(_move)(state, self, THCIndexTensor_(newClone)(state, indices), THCTensor_(newClone)(state, values));
+#else
+  return NULL;
+#endif
 }
 
 /*** end helper methods ***/
@@ -125,22 +145,28 @@ THCSTensor* THCSTensor_(_set)(THCState *state, THCSTensor *self, THCIndexTensor
 /* Empty init */
 THCSTensor *THCSTensor_(new)(THCState *state)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCSTensor *self = (THCSTensor *)THAlloc(sizeof(THCSTensor));
   THCSTensor_(rawInit)(state, self);
   return self;
+#else
+  return NULL;
+#endif
 }
 
 /* Pointer-copy init */
 THCSTensor *THCSTensor_(newWithTensor)(THCState *state, THCIndexTensor *indices, THCTensor *values)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   return THCSTensor_(newWithTensorAndSize)(state, indices, values, NULL);
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(newWithTensorAndSize)(THCState *state, THCIndexTensor *indices, THCTensor *values, THLongStorage *sizes)
-{
-  THCAssertSameGPU(THCSTensor_(checkGPU)(state, 0, 2, indices, values));
-
-  // If sizes are not given, it is inferred as max index of each dim.
+{  // If sizes are not given, it is inferred as max index of each dim.
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t nDimI, nDimV;
 
   THCSTensor *self = (THCSTensor *)THAlloc(sizeof(THCSTensor));
@@ -186,19 +212,27 @@ THCSTensor *THCSTensor_(newWithTensorAndSize)(THCState *state, THCIndexTensor *i
   THCSTensor_(_move)(state, self, THCIndexTensor_(newWithTensor)(state, indices), THCTensor_(newWithTensor)(state, values));
 
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(newWithSize)(THCState *state, THLongStorage *size, THLongStorage *_ignored)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCSTensor *self = (THCSTensor *)THAlloc(sizeof(THCSTensor));
   THCSTensor_(rawInit)(state, self);
   THCSTensor_(rawResize)(state, self, size->size, 0, size->data);
 
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(newWithSize1d)(THCState *state, int64_t size0)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t size[1] = {size0};
 
   THCSTensor *self = (THCSTensor *)THAlloc(sizeof(THCSTensor));
@@ -206,10 +240,14 @@ THCSTensor *THCSTensor_(newWithSize1d)(THCState *state, int64_t size0)
   THCSTensor_(rawResize)(state, self, 1, 0, size);
 
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(newWithSize2d)(THCState *state, int64_t size0, int64_t size1)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t size[2] = {size0, size1};
 
   THCSTensor *self = (THCSTensor *)THAlloc(sizeof(THCSTensor));
@@ -217,10 +255,14 @@ THCSTensor *THCSTensor_(newWithSize2d)(THCState *state, int64_t size0, int64_t s
   THCSTensor_(rawResize)(state, self, 2, 0, size);
 
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(newWithSize3d)(THCState *state, int64_t size0, int64_t size1, int64_t size2)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t size[3] = {size0, size1, size2};
 
   THCSTensor *self = (THCSTensor *)THAlloc(sizeof(THCSTensor));
@@ -228,10 +270,14 @@ THCSTensor *THCSTensor_(newWithSize3d)(THCState *state, int64_t size0, int64_t s
   THCSTensor_(rawResize)(state, self, 3, 0, size);
 
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(newWithSize4d)(THCState *state, int64_t size0, int64_t size1, int64_t size2, int64_t size3)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t size[4] = {size0, size1, size2, size3};
 
   THCSTensor *self = (THCSTensor *)THAlloc(sizeof(THCSTensor));
@@ -239,9 +285,13 @@ THCSTensor *THCSTensor_(newWithSize4d)(THCState *state, int64_t size0, int64_t s
   THCSTensor_(rawResize)(state, self, 4, 0, size);
 
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(newClone)(THCState *state, THCSTensor *self) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCSTensor *other = THCSTensor_(new)(state);
   THCSTensor_(rawResize)(state, other, self->nDimensionI, self->nDimensionV, self->size);
 
@@ -250,15 +300,23 @@ THCSTensor *THCSTensor_(newClone)(THCState *state, THCSTensor *self) {
   other->nnz = self->nnz;
   other->coalesced = self->coalesced;
   return other;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(newTranspose)(THCState *state, THCSTensor *self, int d1, int d2) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCSTensor *other = THCSTensor_(newClone)(state, self);
   THCSTensor_(transpose)(state, other, d1, d2);
   return other;
+#else
+  return NULL;
+#endif
 }
 
 THCTensor *THCSTensor_(newValuesWithSizeOf)(THCState *state, THCTensor *values, int64_t nnz) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCTensor *new_values;
   if (THCTensor_(nDimension)(state, values) == 0) { // values tensor uninitialized
     new_values = THCTensor_(newWithSize1d)(state, nnz);
@@ -269,6 +327,9 @@ THCTensor *THCSTensor_(newValuesWithSizeOf)(THCState *state, THCTensor *values,
     THLongStorage_free(size);
   }
   return new_values;
+#else
+  return NULL;
+#endif
 }
 
 /******************************************************************************
@@ -277,6 +338,7 @@ THCTensor *THCSTensor_(newValuesWithSizeOf)(THCState *state, THCTensor *values,
 
 int THCSTensor_(isSameSizeAs)(THCState *state, const THCSTensor *self, const THCSTensor* src)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   if (self->nDimensionI != src->nDimensionI || self->nDimensionV != src->nDimensionV)
     return 0;
   for(int d = 0; d < self->nDimensionI + self->nDimensionV; ++d) {
@@ -285,10 +347,14 @@ int THCSTensor_(isSameSizeAs)(THCState *state, const THCSTensor *self, const THC
     }
   }
   return 1;
+#else
+  return 0;
+#endif
 }
 
 int THCSTensor_(isSameSizeAsDense)(THCState *state, const THCSTensor *self, const THCTensor* src)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   if (self->nDimensionI + self->nDimensionV != src->nDimension)
     return 0;
   for(int d = 0; d < src->nDimension; ++d) {
@@ -297,64 +363,98 @@ int THCSTensor_(isSameSizeAsDense)(THCState *state, const THCSTensor *self, cons
     }
   }
   return 1;
+#else
+  return 0;
+#endif
 }
 
 THCSTensor *THCSTensor_(resize)(THCState *state, THCSTensor *self, THLongStorage *size)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCSTensor_(rawResize)(state, self, size->size, 0, size->data);
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(resizeAs)(THCState *state, THCSTensor *self, THCSTensor *src)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   if(!THCSTensor_(isSameSizeAs)(state, self, src)) {
     THCSTensor_(rawResize)(state, self, src->nDimensionI, src->nDimensionV, src->size);
   }
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(resize1d)(THCState *state, THCSTensor *self, int64_t size0)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t size[1] = {size0};
   THCSTensor_(rawResize)(state, self, 1, 0, size);
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(resize2d)(THCState *state, THCSTensor *self, int64_t size0, int64_t size1)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t size[2] = {size0, size1};
   THCSTensor_(rawResize)(state, self, 2, 0, size);
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(resize3d)(THCState *state, THCSTensor *self, int64_t size0, int64_t size1, int64_t size2)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t size[3] = {size0, size1, size2};
   THCSTensor_(rawResize)(state, self, 3, 0, size);
   return self;
+#else
+  return NULL;
+#endif
 }
 
 THCSTensor *THCSTensor_(resize4d)(THCState *state, THCSTensor *self, int64_t size0, int64_t size1, int64_t size2, int64_t size3)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t size[4] = {size0, size1, size2, size3};
   THCSTensor_(rawResize)(state, self, 4, 0, size);
   return self;
+#else
+  return NULL;
+#endif
 }
 
 void THCSTensor_(copy)(THCState *state, THCSTensor *self, THCSTensor *src) {
+#if !defined(__HIP_PLATFORM_HCC__)
   if (self == src) return;
   THCSTensor_(rawResize)(state, self, src->nDimensionI, src->nDimensionV, src->size);
   THCSTensor_(_set)(state, self, src->indices, src->values);
   self->nnz = src->nnz;
   self->coalesced = src->coalesced;
+#endif
 }
 
 int THCSTensor_(isCoalesced)(THCState *state, const THCSTensor *self) {
+#if !defined(__HIP_PLATFORM_HCC__)
   return self->coalesced;
+#else
+  return 0;
+#endif
 }
 
 void THCSTensor_(free)(THCState *state, THCSTensor *self)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   if(!self)
     return;
   if(THAtomicDecrementRef(&self->refcount))
@@ -364,15 +464,19 @@ void THCSTensor_(free)(THCState *state, THCSTensor *self)
     THCTensor_(free)(state, self->values);
     THFree(self);
   }
+#endif
 }
 
 void THCSTensor_(retain)(THCState *state, THCSTensor *self)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   THAtomicIncrementRef(&self->refcount);
+#endif
 }
 
 int THCSTensor_(checkGPU)(THCState *state, unsigned int nSparseTensors, unsigned int nTensors, ...)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   /* FIXME: remove this flag after any users stop using it since it is
      now superseded by the runtime option */
 #ifdef DISABLE_CHECK_GPU
@@ -428,9 +532,13 @@ int THCSTensor_(checkGPU)(THCState *state, unsigned int nSparseTensors, unsigned
   va_end(args);
   return valid;
 #endif // DISABLE_CHECK_GPU
+#else
+  return 0;
+#endif
 }
 
 void THCTensor_(sparseMask)(THCState *state, THCSTensor *r_, THCTensor *t, THCSTensor *mask) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THArgCheck(mask->coalesced, 2, "mask is uncoalesced");
   THCAssertSameGPU(THCSTensor_(checkGPU)(state, 2, 3, r_, mask, t));
   if(!THCSTensor_(isSameSizeAsDense)(state, mask, t)) {
@@ -472,6 +580,7 @@ void THCTensor_(sparseMask)(THCState *state, THCSTensor *r_, THCTensor *t, THCST
   THCTensor_(free)(state, t_view);
   THCIndexTensor_(free)(state, maskIndices);
   THCTensor_(free)(state, maskValues);
+#endif
 }
 
 #endif
diff --git a/src/THCS/generic/THCSTensor.cu b/src/THCS/generic/THCSTensor.cu
index 368c7a7fa..e350df0eb 100644
--- a/src/THCS/generic/THCSTensor.cu
+++ b/src/THCS/generic/THCSTensor.cu
@@ -20,6 +20,7 @@
 #define V_INFO(tensor) getTensorInfo<THCTensor, uint64_t>(state, tensor)
 
 THCTensor *THCSTensor_(toDense)(THCState *state, THCSTensor *self) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THLongStorage *size;
   THCTensor *dst;
 
@@ -33,9 +34,13 @@ THCTensor *THCSTensor_(toDense)(THCState *state, THCSTensor *self) {
   THCSTensor_(spcadd)(state, dst, dst, one, self);
   THCudaCheck(cudaGetLastError());
   return dst;
+#else
+  return nullptr;
+#endif
 }
 
 THCSTensor *THCSTensor_(newCoalesce)(THCState *state, THCSTensor *self) {
+#if !defined(__HIP_PLATFORM_HCC__)
   ptrdiff_t nnz = self->nnz;
   if (nnz < 2) {
     self->coalesced = 1;
@@ -169,11 +174,15 @@ THCSTensor *THCSTensor_(newCoalesce)(THCState *state, THCSTensor *self) {
   THCudaCheck(cudaGetLastError());
   return dst;
 #undef THRUST_EXEC
+#else
+  return nullptr;
+#endif
 }
 
 // forceClone is intended to use as a boolean, if set, the result will forced to
 // be a clone of self.
 THCIndexTensor* THCSTensor_(newFlattenedIndices)(THCState *state, THCSTensor *self, int forceClone) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCIndexTensor *indices = THCSTensor_(newIndices)(state, self);
   int nDimI = self->nDimensionI;
   if (nDimI == 1) {
@@ -202,10 +211,14 @@ THCIndexTensor* THCSTensor_(newFlattenedIndices)(THCState *state, THCSTensor *se
     THCIndexTensor_(free)(state, indicesSlice);
     return indices1D;
   }
+#else
+  return nullptr;
+#endif
 }
 
 // In place transpose
 void THCSTensor_(transpose)(THCState *state, THCSTensor *self, int d1, int d2) {
+#if !defined(__HIP_PLATFORM_HCC__)
   int64_t nDimI = THCSTensor_(nDimensionI)(state, self);
   int64_t nDimV = THCSTensor_(nDimensionV)(state, self);
   THArgCheck(d1 < nDimI && d2 < nDimI, 1, "Transposed dimensions should be sparse. Got nDimI: %ld, d1: %ld, d2: %ld", nDimI, d1, d2);
@@ -224,11 +237,16 @@ void THCSTensor_(transpose)(THCState *state, THCSTensor *self, int d1, int d2) {
   THCIndexTensor_(free)(state, buffer);
   THCIndexTensor_(free)(state, slice1);
   THCIndexTensor_(free)(state, slice2);
+#endif
 }
 
 int THCSTensor_(getDevice)(THCState* state, const THCSTensor* tensor) {
+#if !defined(__HIP_PLATFORM_HCC__)
   if (!tensor->values || !tensor->values->storage) return -1;
   return THCStorage_(getDevice)(state, tensor->values->storage);
+#else
+  return 0;
+#endif
 }
 
 #endif
diff --git a/src/THCS/generic/THCSTensorMath.cu b/src/THCS/generic/THCSTensorMath.cu
index 319652fdf..285b01189 100644
--- a/src/THCS/generic/THCSTensorMath.cu
+++ b/src/THCS/generic/THCSTensorMath.cu
@@ -56,6 +56,7 @@ void THCTensor_(spaddcdiv)(THCState *state, THCTensor *r_, THCTensor *t, real va
 }
 
 void THCSTensor_(spaddmm)(THCState *state, THCTensor *r_, real beta, THCTensor *t, real alpha, THCSTensor *sparse_, THCTensor *dense) {
+#if !defined(__HIP_PLATFORM_HCC__)
 #if defined(THCS_REAL_IS_FLOAT) || defined(THCS_REAL_IS_DOUBLE)
   THCAssertSameGPU(THCSTensor_(checkGPU)(state, 1, 4, sparse_, r_, t, dense));
   THCudaIntTensor *csr;
@@ -165,20 +166,27 @@ void THCSTensor_(spaddmm)(THCState *state, THCTensor *r_, real beta, THCTensor *
 #else
   THError("unimplemented data type");
 #endif
+#endif
 }
 
 void THCSTensor_(sspaddmm)(THCState *state, THCSTensor *r_, real beta, THCSTensor *t, real alpha, THCSTensor *sparse, THCTensor *dense) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THError("WARNING: Sparse Cuda Tensor op sspaddmm is not implemented");
   // TODO Write some kernels
+#endif
 }
 
 void THCSTensor_(hspmm)(THCState *state, THCSTensor *r_, real alpha, THCSTensor *sparse_, THCTensor *dense) {
+#if defined(__HIP_PLATFORM_HCC__)
+#define THRUST_EXEC(fn, ...) // whitespace
+#else
 #if CUDA_VERSION >= 7000
   THCThrustAllocator thrustAlloc(state);
 #define THRUST_EXEC(fn, ...) fn(thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)), ##__VA_ARGS__)
 #else
 #define THRUST_EXEC(fn, ...) fn(##__VA_ARGS__)
 #endif
+#endif
 
   THCAssertSameGPU(THCSTensor_(checkGPU)(state, 2, 3, r_, sparse_, dense));
 
@@ -230,6 +238,7 @@ void THCSTensor_(hspmm)(THCState *state, THCSTensor *r_, real alpha, THCSTensor
 }
 
 void THCSTensor_(spcadd)(THCState *state, THCTensor *r_, THCTensor *dense, real value, THCSTensor *sparse) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCAssertSameGPU(THCSTensor_(checkGPU)(state, 1, 3, sparse, r_, dense));
 
   const ptrdiff_t nnz = THCSTensor_(nnz)(state, sparse);
@@ -313,9 +322,11 @@ void THCSTensor_(spcadd)(THCState *state, THCTensor *r_, THCTensor *dense, real
   THCIndexTensor_(free)(state, indices);
   THCTensor_(free)(state, values);
   THCTensor_(free)(state, r);
+#endif
 }
 
 void THCSTensor_(mul)(THCState *state, THCSTensor *r_, THCSTensor *t, real value) {
+#if !defined(__HIP_PLATFORM_HCC__)
   if (r_ == t) {
     THCTensor *r_values_ = THCSTensor_(newValues)(state, r_);
     THCTensor_(mul)(state, r_values_, r_values_, value);
@@ -339,9 +350,11 @@ void THCSTensor_(mul)(THCState *state, THCSTensor *r_, THCSTensor *t, real value
     THCIndexTensor_(free)(state, t_indices_);
     THCTensor_(free)(state, t_values_);
   }
+#endif
 }
 
 void THCSTensor_(div)(THCState *state, THCSTensor *r_, THCSTensor *t, real value) {
+#if !defined(__HIP_PLATFORM_HCC__)
   if (r_ == t) {
     THCTensor *r_values_ = THCSTensor_(newValues)(state, r_);
     THCTensor_(div)(state, r_values_, r_values_, value);
@@ -365,9 +378,11 @@ void THCSTensor_(div)(THCState *state, THCSTensor *r_, THCSTensor *t, real value
     THCIndexTensor_(free)(state, t_indices_);
     THCTensor_(free)(state, t_values_);
   }
+#endif
 }
 
 void THCSTensor_(cadd)(THCState *state, THCSTensor *r_, THCSTensor *t, real value, THCSTensor *src) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCAssertSameGPU(THCSTensor_(checkGPU)(state, 3, 3, r_, t, src));
   if(!THCSTensor_(isSameSizeAs)(state, t, src)) {
     THError("cadd operands have incompatible sizes or dimension types");
@@ -415,13 +430,17 @@ void THCSTensor_(cadd)(THCState *state, THCSTensor *r_, THCSTensor *t, real valu
   THCTensor_(free)(state, t_values_);
   THCIndexTensor_(free)(state, s_indices_);
   THCTensor_(free)(state, s_values_);
+#endif
 }
 
 void THCSTensor_(csub)(THCState *state, THCSTensor *r_, THCSTensor *t, real value, THCSTensor *src) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCSTensor_(cadd)(state, r_, t, ScalarNegate<real>::to(value), src);
+#endif
 }
 
 void THCSTensor_(cmul)(THCState *state, THCSTensor *r_, THCSTensor *t_, THCSTensor *src_) {
+#if !defined(__HIP_PLATFORM_HCC__)
   THCAssertSameGPU(THCSTensor_(checkGPU)(state, 3, 3, r_, t_, src_));
   if(!THCSTensor_(isSameSizeAs)(state, t_, src_)) {
     THError("cmul operands have incompatible sizes or dimension types");
@@ -477,9 +496,11 @@ void THCSTensor_(cmul)(THCState *state, THCSTensor *r_, THCSTensor *t_, THCSTens
   THCTensor_(free)(state, s_values_);
   THCSTensor_(free)(state, t);
   THCSTensor_(free)(state, src);
+#endif
 }
 
 void THCSTensor_(pow)(THCState *state, THCSTensor *r_, THCSTensor *t_, real value) {
+#if !defined(__HIP_PLATFORM_HCC__)
   if (THCNumerics<real>::eq(value, ScalarConvert<int, real>::to(0))) {
     THError("cannot raise to zeroth power on sparse tensor");
   }
@@ -502,12 +523,13 @@ void THCSTensor_(pow)(THCState *state, THCSTensor *r_, THCSTensor *t_, real valu
   THCIndexTensor_(free)(state, t_indices_);
   THCTensor_(free)(state, t_values_);
   THCSTensor_(free)(state, t);
+#endif
 }
 
 #if defined(THCS_REAL_IS_FLOAT) || defined(THCS_REAL_IS_DOUBLE) || defined(THCS_REAL_IS_HALF)
 accreal THCSTensor_(normall)(THCState *state, THCSTensor *self, real value) {
   THCSTensor* self_coalesced = THCSTensor_(newCoalesce)(state, self);
-  accreal result = THCTensor_(normall)(state, self_coalesced->values, value); 
+  accreal result = THCTensor_(normall)(state, self_coalesced->values, value);
   THCSTensor_(free)(state, self_coalesced);
   return result;
 }
diff --git a/src/THCUNN/Abs.cu b/src/THCUNN/Abs.cu
index f3c7592e2..638181101 100644
--- a/src/THCUNN/Abs.cu
+++ b/src/THCUNN/Abs.cu
@@ -8,7 +8,11 @@ struct absupdateOutput_functor
 {
   __device__ void operator()(T* output, const T* input) const
   {
+#ifdef __HIP_PLATFORM_HCC__
+    *output = fabsf(*input);
+#else
     *output = abs(*input);
+#endif
   }
 };
 
diff --git a/src/THCUNN/BCECriterion.cu b/src/THCUNN/BCECriterion.cu
index ccb40008c..da2dc7c91 100644
--- a/src/THCUNN/BCECriterion.cu
+++ b/src/THCUNN/BCECriterion.cu
@@ -9,14 +9,21 @@
 #include <thrust/transform.h>
 #include <thrust/transform_reduce.h>
 
+#if defined(__HIP_PLATFORM_HCC__)
+template <typename T>
+inline __host__ __device__ T eps();
+template <>
+inline __host__ __device__ float eps() { return 1e-12f; }
+template <>
+inline __host__ __device__ double eps() { return 1e-12; }
+#else
 template <typename T>
 inline __device__ T eps();
-
 template <>
 inline __device__ float eps() { return 1e-12f; }
-
 template <>
 inline __device__ double eps() { return 1e-12; }
+#endif
 
 template <typename Dtype, typename Acctype>
 struct bce_functor
@@ -27,7 +34,9 @@ struct bce_functor
   {
     Dtype input = thrust::get<0>(x);
     Dtype t = thrust::get<1>(x);
+#if defined (__NVCC__)
     assert(input >= 0. && input <= 1.);
+#endif
     return - (t * THCNumerics<Acctype>::log(input + eps<Acctype>()) + (Acctype(1)- t) * THCNumerics<Acctype>::log(Acctype(1) - input + eps<Acctype>()));
   }
 };
@@ -41,7 +50,9 @@ struct bce_updateOutput_no_reduce_functor
       const Dtype *target,
       Dtype *output)
   {
+#if defined (__NVCC__)
     assert(*input >= 0. && *input <= 1.);
+#endif
     *output = ScalarConvert<Acctype, Dtype>::to(
         -(*target * THCNumerics<Acctype>::log(*input + eps<Acctype>()) +
           (Acctype(1) - *target) * THCNumerics<Acctype>::log(Acctype(1) - *input + eps<Acctype>())));
@@ -58,7 +69,9 @@ struct bce_functor_weights
     Dtype input = thrust::get<0>(x);
     Dtype t = thrust::get<1>(x);
     Dtype w = thrust::get<2>(x);
+#if defined (__NVCC__)
     assert(input >= 0. && input <= 1.);
+#endif
     return - w * (t * THCNumerics<Acctype>::log(input + eps<Acctype>()) +
         (Acctype(1) - t) * THCNumerics<Acctype>::log(Acctype(1) - input + eps<Acctype>()));
   }
diff --git a/src/THCUNN/BatchNormalization.cu b/src/THCUNN/BatchNormalization.cu
index 865323a16..3148fc639 100644
--- a/src/THCUNN/BatchNormalization.cu
+++ b/src/THCUNN/BatchNormalization.cu
@@ -6,15 +6,29 @@
 #include "THCDeviceTensor.cuh"
 #include "THCDeviceTensorUtils.cuh"
 #include "THCDeviceUtils.cuh"
+
+#if defined(__HIP_PLATFORM_HCC__)
+const int WARP_SIZE = 64;
+#else
 const int WARP_SIZE = 32;
+#endif
 
 // The maximum number of threads in a block
+#if defined(__HIP_PLATFORM_HCC__)
+const int MAX_BLOCK_SIZE = 256;
+#else
 const int MAX_BLOCK_SIZE = 512;
+#endif
 
 // Number of threads in a block given an input size up to MAX_BLOCK_SIZE
 static int getNumThreads(int nElem) {
+#if defined(__HIP_PLATFORM_HCC__)
+  int threadSizes[3] = { 64, 128, MAX_BLOCK_SIZE };
+  for (int i = 0; i != 3; ++i) {
+#else
   int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };
   for (int i = 0; i != 5; ++i) {
+#endif
     if (nElem <= threadSizes[i]) {
       return threadSizes[i];
     }
@@ -67,7 +81,11 @@ struct GradOp {
     : mean(m), input(i), gradOutput(g) {}
   __device__ __forceinline__ Float2<Dtype, Acctype> operator()(int batch, int plane, int n) {
     Dtype g = gradOutput[batch][plane][n];
+#if defined(__HIP_PLATFORM_HCC__)
+    Dtype c = ScalarConvert<Acctype, Dtype>::to((input[batch][plane][n]).template as<Acctype>() - mean);
+#else
     Dtype c = ScalarConvert<Acctype, Dtype>::to(input[batch][plane][n] - mean);
+#endif
     return Float2<Dtype, Acctype>(g, g * c);
   }
   const Acctype mean;
@@ -196,12 +214,22 @@ __global__ void BatchNormalizationUpdateOutput_kernel(
     Acctype unbiasedVar = varN / (N - 1);
     saveMean[plane] = ScalarConvert<Acctype, Dtype>::to(mean);
     saveStd[plane] = ScalarConvert<Acctype, Dtype>::to(invStd);
+
+#if defined(__HIP_PLATFORM_HCC__)
+    if (runningMean.data() != NULL) {
+      runningMean[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * (runningMean[plane]).template as<Acctype>() + momentum * mean);
+    }
+    if (runningVar.data() != NULL) {
+      runningVar[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * (runningVar[plane]).template as<Acctype>() + momentum * unbiasedVar);
+    }
+#else
     if (runningMean.data() != NULL) {
       runningMean[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * runningMean[plane] + momentum * mean);
     }
     if (runningVar.data() != NULL) {
       runningVar[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * runningVar[plane] + momentum * unbiasedVar);
     }
+#endif
   }
 
   // Write normalized and update the output
@@ -240,7 +268,11 @@ __global__ void BatchNormalizationBackward_kernel(
     stdVal = ScalarConvert<Dtype, Acctype>::to(saveStd[plane]);
   } else {
     mean = ScalarConvert<Dtype, Acctype>::to(runningMean[plane]);
+#if defined(__HIP_PLATFORM_HCC__)
+    stdVal = 1 / sqrt((runningVar[plane]).template as<Acctype>() + eps);
+#else
     stdVal = 1 / sqrt(runningVar[plane] + eps);
+#endif
   }
 
   Acctype weightVal = weight.numElements() > 0 ? ScalarConvert<Dtype, Acctype>::to(weight[plane]) : Acctype(1);
@@ -275,16 +307,23 @@ __global__ void BatchNormalizationBackward_kernel(
 
   if (gradWeight.numElements() > 0) {
     if (threadIdx.x == 0) {
+#if defined(__HIP_PLATFORM_HCC__)
+      (gradWeight[plane]).template as<Dtype>() += ScalarConvert<Acctype, Dtype>::to(scale * dotP * stdVal);
+#else
       gradWeight[plane] += ScalarConvert<Acctype, Dtype>::to(scale * dotP * stdVal);
+#endif
     }
   }
 
   if (gradBias.numElements() > 0) {
     if (threadIdx.x == 0) {
+#if defined(__HIP_PLATFORM_HCC__)
+      (gradBias[plane]).template as<Dtype>() += ScalarConvert<Acctype, Dtype>::to(scale * gradOutputSum);
+#else
       gradBias[plane] += ScalarConvert<Acctype, Dtype>::to(scale * gradOutputSum);
+#endif
     }
   }
 }
-
 #include "generic/BatchNormalization.cu"
 #include "THCGenerateFloatTypes.h"
diff --git a/src/THCUNN/CMakeLists.txt.hip b/src/THCUNN/CMakeLists.txt.hip
new file mode 100644
index 000000000..5dba0d62f
--- /dev/null
+++ b/src/THCUNN/CMakeLists.txt.hip
@@ -0,0 +1,154 @@
+# HIP_PATH
+IF(NOT DEFINED $ENV{HIP_PATH})
+  SET(HIP_PATH /opt/rocm/hip)
+ELSE()
+  SET(HIP_PATH $ENV{HIP_PATH})
+ENDIF()
+
+# HCC_PATH
+IF(NOT DEFINED $ENV{HCC_PATH})
+  SET(HCC_PATH /opt/rocm/hcc)
+ELSE()
+  SET(HCC_PATH $ENV{HCC_PATH})
+ENDIF()
+
+# HIPBLAS_PATH
+IF(NOT DEFINED $ENV{HIPBLAS_PATH})
+  SET(HIPBLAS_PATH /opt/rocm/hipblas)
+ELSE()
+  SET(HIPBLAS_PATH $ENV{HIPBLAS_PATH})
+ENDIF()
+
+# HIPRNG_PATH
+IF(NOT DEFINED $ENV{HIPRNG_PATH})
+  SET(HIPRNG_PATH /opt/rocm/hcrng)
+ELSE()
+  SET(HIPRNG_PATH $ENV{HIPRNG_PATH})
+ENDIF()
+
+# HIPSPARSE_PATH
+IF(NOT DEFINED $ENV{HIPSPARSE_PATH})
+  SET(HIPSPARSE_PATH /opt/rocm/hcsparse)
+ELSE()
+  SET(HIPSPARSE_PATH $ENV{HIPSPARSE_PATH})
+ENDIF()
+
+SET(THRUST_PATH "/root/Thrust")
+
+# load HIP cmake module and load platform id
+SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH} "${HIP_PATH}/cmake")
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig -P OUTPUT_VARIABLE PLATFORM)
+EXECUTE_PROCESS(COMMAND ${HIP_PATH}/bin/hipconfig --cpp_config OUTPUT_VARIABLE HIP_CXX_FLAGS)
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+
+SET(HIP_CXX_FLAGS "-D__HIP_PLATFORM_HCC__ -I/opt/rocm/hip/include -I/opt/rocm/hcc/include" ${HIP_CXX_FLAGS})
+
+SET(HIP_HIPCC_FLAGS "-DGENERIC_GRID_LAUNCH=1 ${CMAKE_CXX_FLAGS}")
+SET(HIP_HIPCC_FLAGS "-DGENERIC_GRID_LAUNCH=1 ${HIP_HIPCC_FLAGS}")
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+SET(CMAKE_C_COMPILER ${HIP_HIPCC_EXECUTABLE})
+SET(CMAKE_C_FLAGS "-std=c99 -Werror=implicit-function-declaration ${CMAKE_C_FLAGS}")
+SET(CMAKE_C_FLAGS "${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+SET(CMAKE_CXX_FLAGS  "-std=c++11 ${CMAKE_CXX_FLAGS} ${HIP_CXX_FLAGS}")
+
+SET(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH})
+
+FIND_PACKAGE(HIP 1.0 REQUIRED)
+
+MESSAGE(STATUS "ROCM TRUE:")
+MESSAGE(STATUS "CMAKE_CXX_COMPILER: " ${CMAKE_CXX_COMPILER})
+
+INCLUDE_DIRECTORIES(${HIPBLAS_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPSPARSE_PATH}/include)
+INCLUDE_DIRECTORIES(${HIPRNG_PATH}/include)
+INCLUDE_DIRECTORIES(${THRUST_PATH})
+
+SET(ATen_CUDA_SRCS ${ATen_CUDA_SRCS}
+${CMAKE_CURRENT_SOURCE_DIR}/AbsCriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/Abs.cu
+${CMAKE_CURRENT_SOURCE_DIR}/BatchNormalization.cu
+${CMAKE_CURRENT_SOURCE_DIR}/BCECriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/ClassNLLCriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/Col2Im.cu
+${CMAKE_CURRENT_SOURCE_DIR}/DistKLDivCriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/ELU.cu
+${CMAKE_CURRENT_SOURCE_DIR}/FeatureLPPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/FusedRNNKernel.cu
+${CMAKE_CURRENT_SOURCE_DIR}/GatedLinearUnit.cu
+${CMAKE_CURRENT_SOURCE_DIR}/HardTanh.cu
+${CMAKE_CURRENT_SOURCE_DIR}/Im2Col.cu
+${CMAKE_CURRENT_SOURCE_DIR}/IndexLinear.cu
+${CMAKE_CURRENT_SOURCE_DIR}/L1Cost.cu
+${CMAKE_CURRENT_SOURCE_DIR}/LeakyReLU.cu
+${CMAKE_CURRENT_SOURCE_DIR}/LogSigmoid.cu
+${CMAKE_CURRENT_SOURCE_DIR}/LogSoftMax.cu
+${CMAKE_CURRENT_SOURCE_DIR}/LookupTableBag.cu
+${CMAKE_CURRENT_SOURCE_DIR}/LookupTable.cu
+${CMAKE_CURRENT_SOURCE_DIR}/MarginCriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/MSECriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/MultiLabelMarginCriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/MultiMarginCriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/PReLU.cu
+${CMAKE_CURRENT_SOURCE_DIR}/RReLU.cu
+${CMAKE_CURRENT_SOURCE_DIR}/Sigmoid.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SmoothL1Criterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SoftMarginCriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SoftMax.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SoftPlus.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SoftShrink.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SparseLinear.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialAdaptiveAveragePooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialAdaptiveMaxPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialAveragePooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialClassNLLCriterion.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialConvolutionLocal.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialConvolutionMM.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialCrossMapLRN.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialDepthwiseConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialDilatedConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialDilatedMaxPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialFractionalMaxPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialFullConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialFullDilatedConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialGridSamplerBilinear.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialMaxPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialMaxUnpooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialReflectionPadding.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialReplicationPadding.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialSubSampling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialUpSamplingBilinear.cu
+${CMAKE_CURRENT_SOURCE_DIR}/SpatialUpSamplingNearest.cu
+${CMAKE_CURRENT_SOURCE_DIR}/Sqrt.cu
+${CMAKE_CURRENT_SOURCE_DIR}/Square.cu
+${CMAKE_CURRENT_SOURCE_DIR}/Tanh.cu
+${CMAKE_CURRENT_SOURCE_DIR}/TemporalConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/TemporalMaxPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/TemporalReflectionPadding.cu
+${CMAKE_CURRENT_SOURCE_DIR}/TemporalReplicationPadding.cu
+${CMAKE_CURRENT_SOURCE_DIR}/TemporalRowConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/TemporalUpSamplingLinear.cu
+${CMAKE_CURRENT_SOURCE_DIR}/TemporalUpSamplingNearest.cu
+${CMAKE_CURRENT_SOURCE_DIR}/Threshold.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricAdaptiveAveragePooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricAdaptiveMaxPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricAveragePooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricDilatedConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricDilatedMaxPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricFractionalMaxPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricFullConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricFullDilatedConvolution.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricMaxPooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricMaxUnpooling.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricReplicationPadding.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricUpSamplingNearest.cu
+${CMAKE_CURRENT_SOURCE_DIR}/VolumetricUpSamplingTrilinear.cu
+PARENT_SCOPE)
+
+set(ATen_CUDA_INCLUDE ${ATen_CUDA_INCLUDE}
+  "${CMAKE_CURRENT_SOURCE_DIR}"
+PARENT_SCOPE)
+
+INSTALL(FILES THCUNN.h DESTINATION "${ATEN_INSTALL_INCLUDE_SUBDIR}/THCUNN")
+INSTALL(FILES generic/THCUNN.h DESTINATION "${ATEN_INSTALL_INCLUDE_SUBDIR}/THCUNN/generic")
diff --git a/src/THCUNN/ClassNLLCriterion.cu b/src/THCUNN/ClassNLLCriterion.cu
index 1043454ff..1568ada78 100644
--- a/src/THCUNN/ClassNLLCriterion.cu
+++ b/src/THCUNN/ClassNLLCriterion.cu
@@ -20,14 +20,18 @@ __global__ void cunn_ClassNLLCriterion_updateOutput_kernel1(Dtype *output,
                                                            int size_average,
                                                            int n_classes,
                                                            int64_t ignore_index) {
+#if defined(__NVCC__)
   assert(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0);
+#endif
 
   // TODO: T4951791 Reuse code between updateOutput_kernel1 and
   // updateOutput_kernel.
 
   int t = (int) *target - TH_INDEX_BASE;
   if (t != (int) ignore_index) {
+#if defined(__NVCC__)
     assert(t >= 0 && t < n_classes);
+#endif
     Dtype cur_weight = weights ? weights[t] : ScalarConvert<int, Dtype>::to(1);
     *output = -cur_weight * input[t];
     *total_weight = cur_weight;
@@ -53,10 +57,16 @@ __global__ void ClassNLLCriterion_updateOutput_no_reduce_kernel(
       output[index] = ScalarConvert<int, Dtype>::to(0);
       continue;
     }
+#if defined(__NVCC__)
     assert(cur_target  >= 0 && cur_target  < n_classes);
+#endif
     Dtype weight =
        weights ? weights[cur_target] : ScalarConvert<int, Dtype>::to(1);
+#if defined(__HIP_PLATFORM_HCC__)
+    output[index] = -weight * input[index][cur_target].template as<Dtype>();
+#else
     output[index] = -weight * input[index][cur_target];
+#endif
   }
 }
 
@@ -75,10 +85,16 @@ __global__ void ClassNLLCriterion_updateGradInput_no_reduce_kernel(
     if (cur_target == ignore_index) {
       continue;
     }
+#if defined(__NVCC__)
     assert(cur_target  >= 0 && cur_target  < n_classes);
+#endif
     Dtype weight =
        weights ? weights[cur_target] : ScalarConvert<int, Dtype>::to(1);
+#if defined(__HIP_PLATFORM_HCC__)
+    gradInput[index][cur_target] = -weight * gradOutput[index].template as<Dtype>();
+#else
     gradInput[index][cur_target] = -weight * gradOutput[index];
+#endif
   }
 }
 
@@ -102,7 +118,9 @@ __global__ void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *output,
   for (i = threadIdx.x; i < nframe; i += NTHREADS) {
       t = target[i] - TH_INDEX_BASE;
       if (t != (int) ignore_index) {
+#if defined(__NVCC__)
         assert(t >= 0 && t < n_classes);
+#endif
         cur_weight = weights ? weights[t] : ScalarConvert<int, Dtype>::to(1);
         shInputs[threadIdx.x] -= input[i * ndim + t] * cur_weight;
         acc_weight[threadIdx.x] += cur_weight;
@@ -148,7 +166,9 @@ __global__ void cunn_ClassNLLCriterion_updateGradInput_kernel1(
   Dtype norm = size_average ? (ScalarConvert<int, Dtype>::to(1) / *total_weight) : ScalarConvert<int, Dtype>::to(1);
   int t = (int)*target - TH_INDEX_BASE;
   if (t != (int) ignore_index) {
+#if defined(__NVCC__)
     assert(t >= 0 && t < n_classes);
+#endif
     gradInput[t] = -(weights ? weights[t] : ScalarConvert<int, Dtype>::to(1)) * norm * gradOutput[0];
   }
 }
@@ -175,7 +195,9 @@ __global__ void cunn_ClassNLLCriterion_updateGradInput_kernel(
   for (i = threadIdx.x; i < nframe; i += NTHREADS) {
     t = (int)target[i] - TH_INDEX_BASE;
     if (t != (int) ignore_index) {
+#if defined(__NVCC__)
       assert(t >= 0 && t < n_classes);
+#endif
       gradInput[i * ndim + t] = -(weights ? weights[t] : ScalarConvert<int, Dtype>::to(1)) * norm * gradOutput[0];
     }
   }
diff --git a/src/THCUNN/FeatureLPPooling.cu b/src/THCUNN/FeatureLPPooling.cu
index 4ad190fbe..648ed49b9 100644
--- a/src/THCUNN/FeatureLPPooling.cu
+++ b/src/THCUNN/FeatureLPPooling.cu
@@ -428,6 +428,17 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
   dim3 grid(nonFeatureSizeBlocks, featureBlocks, input.getSize(0));
   dim3 block(blockSize);
 
+#if defined(__HIP_PLATFORM_HCC__)
+#define L2_STRIDE_CASE(STRIDE, WIDTH)                                   \
+  case STRIDE:                                                          \
+    hipLaunchKernelGGL((detail::featureLPPoolingUpdateOutput<T, WIDTH,  \
+                                 STRIDE,                                \
+                                 detail::power2,                        \
+                                 detail::root2>), grid, block, 0, stream, \
+                                   input, output,                       \
+                                   ScalarConvert<float, T>::to(power)); \
+    return true;
+#else
 #define L2_STRIDE_CASE(STRIDE, WIDTH)                                   \
   case STRIDE:                                                          \
     detail::                                                            \
@@ -438,6 +449,7 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
                                    input, output,                       \
                                    ScalarConvert<float, T>::to(power)); \
     return true;
+#endif
 
 #define L2_WIDTH_CASE(WIDTH)                    \
   case WIDTH:                                   \
@@ -448,6 +460,17 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
       L2_STRIDE_CASE(4, WIDTH);                 \
     }
 
+#if defined(__HIP_PLATFORM_HCC__)
+#define LP_STRIDE_CASE(STRIDE, WIDTH)                                   \
+  case STRIDE:                                                          \
+    hipLaunchKernelGGL((detail::featureLPPoolingUpdateOutput<T, WIDTH,  \
+                                 STRIDE,                                \
+                                 detail::powerN,                        \
+                                 detail::rootN>), grid, block, 0, stream, \
+                                   input, output,                       \
+                                   ScalarConvert<float, T>::to(power)); \
+    return true;
+#else
 #define LP_STRIDE_CASE(STRIDE, WIDTH)                                   \
   case STRIDE:                                                          \
     detail::                                                            \
@@ -458,6 +481,7 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
                                    input, output,                       \
                                    ScalarConvert<float, T>::to(power)); \
     return true;
+#endif
 
 #define LP_WIDTH_CASE(WIDTH)                    \
   case WIDTH:                                   \
@@ -468,6 +492,7 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
       LP_STRIDE_CASE(4, WIDTH);                 \
     }
 
+#if !defined(__HIP_PLATFORM_HCC__)
   if (power == 2.0f) {
     switch (width) {
       L2_WIDTH_CASE(2);
@@ -505,6 +530,7 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
       LP_WIDTH_CASE(16);
     }
   }
+#endif
 
   // Otherwise, we have an unhandled width and/or stride.
   return false;
@@ -566,6 +592,15 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
   dim3 grid(nonFeatureSizeBlocks, featureBlocks, input.getSize(0));
   dim3 block(blockSize);
 
+#if defined(__HIP_PLATFORM_HCC__)
+#define L2_STRIDE_CASE(STRIDE, WIDTH)                                   \
+  case STRIDE:                                                          \
+    hipLaunchKernelGGL((detail::featureLPPoolingUpdateGradInput<        \
+          T, WIDTH, STRIDE, detail::powerGrad2>), grid, block, 0, stream, \
+            gradOutput, input, output, gradInput,                       \
+            ScalarConvert<float, T>::to(power));                        \
+    return true;
+#else
 #define L2_STRIDE_CASE(STRIDE, WIDTH)                                   \
   case STRIDE:                                                          \
     detail::                                                            \
@@ -574,6 +609,7 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
             gradOutput, input, output, gradInput,                       \
             ScalarConvert<float, T>::to(power));                        \
     return true;
+#endif
 
 #define L2_WIDTH_CASE(WIDTH)                    \
   case WIDTH:                                   \
@@ -584,6 +620,15 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
       L2_STRIDE_CASE(4, WIDTH);                 \
     }
 
+#if defined(__HIP_PLATFORM_HCC__)
+#define LP_STRIDE_CASE(STRIDE, WIDTH)                                   \
+  case STRIDE:                                                          \
+    hipLauchKernelGGL((detail::featureLPPoolingUpdateGradInput<         \
+          T, WIDTH, STRIDE, detail::powerGradN>), grid, block, 0, stream, \
+            gradOutput, input, output, gradInput,                       \
+            ScalarConvert<float, T>::to(power));                        \
+    return true;
+#else
 #define LP_STRIDE_CASE(STRIDE, WIDTH)                                   \
   case STRIDE:                                                          \
     detail::                                                            \
@@ -592,6 +637,7 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
             gradOutput, input, output, gradInput,                       \
             ScalarConvert<float, T>::to(power));                        \
     return true;
+#endif
 
 #define LP_WIDTH_CASE(WIDTH)                    \
   case WIDTH:                                   \
@@ -602,6 +648,7 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
       LP_STRIDE_CASE(4, WIDTH);                 \
     }
 
+#if !defined(__HIP_PLATFORM_HCC__)
   if (power == 2.0f) {
     switch (width) {
       L2_WIDTH_CASE(2);
@@ -639,6 +686,7 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
       LP_WIDTH_CASE(16);
     }
   }
+#endif
 
   // Otherwise, we have an unhandled width and/or stride.
   return false;
diff --git a/src/THCUNN/IndexLinear.cu b/src/THCUNN/IndexLinear.cu
index 2729f9277..98ac09ece 100644
--- a/src/THCUNN/IndexLinear.cu
+++ b/src/THCUNN/IndexLinear.cu
@@ -22,7 +22,11 @@ __device__ double atomicExch(double *address, double val) {
 }
 
 template<typename Ty, bool train>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void updateOutput(
     Ty *output,
     Ty *normalizedValues,
@@ -141,7 +145,11 @@ void updateOutput(
 // to generate gradWeight of size [keysSize x outDim]
 // nth block along y dimension computes on the non zero elements from the nth batch.
 template<typename Ty>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void accGradWeight(
     Ty *gradWeight,
     const Ty *gradOutput,
@@ -213,7 +221,11 @@ void accGradWeight(
 // The gradBias is just a reduction of gradOutput along the batches.
 // There is only one block along y dimension performing the reduction.
 template<typename Ty, bool update>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void accGradBias(
     Ty *buffer,
     const Ty *gradOutput,
@@ -267,7 +279,11 @@ void accGradBias(
 // This kernel is launched batchSize number of times.
 // At each step in the iteration, the weights are updated in a sparse manner.
 template<typename Ty>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void updateWeight(
     Ty *weight,
     const Ty *gradWeight,
@@ -336,7 +352,11 @@ void updateWeight(
 // This kernel is launched batchSize number of times.
 // At each step in the iteration, the weights are updated in place in a sparse manner.
 template<typename Ty>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void accUpdateWeight(
     Ty *weight,
     const int64_t weightStride,
diff --git a/src/THCUNN/LogSigmoid.cu b/src/THCUNN/LogSigmoid.cu
index 8facb5bf4..357b7bf95 100644
--- a/src/THCUNN/LogSigmoid.cu
+++ b/src/THCUNN/LogSigmoid.cu
@@ -3,7 +3,7 @@
 #include "THCHalfAutoNumerics.cuh"
 #include <THC/THCApply.cuh>
 
-#ifdef _MSC_VER
+#if defined(_MSC_VER) || defined(__HIP_PLATFORM_HCC__)
 #define ZERO_MACRO zero<T>()
 template <typename T>
 inline __device__ typename std::enable_if<std::is_same<T, double>::value, T>::type zero() {
diff --git a/src/THCUNN/LookupTable.cu b/src/THCUNN/LookupTable.cu
index 854230913..d11376e77 100644
--- a/src/THCUNN/LookupTable.cu
+++ b/src/THCUNN/LookupTable.cu
@@ -5,8 +5,13 @@
 #include "THCHalf.h"
 #include "THCHalfAutoNumerics.cuh"
 #include "THCTensorSort.cuh"
+#if defined(__HIP_PLATFORM_HCC__)
+#include "THC/THCTensorMathReduce.cuh"
+#else
 #include "../THC/THCTensorMathReduce.cuh"
+#endif
 
+#if defined(__NVCC__)
 const int WARP_SIZE = 32;
 
 __device__ __forceinline__ bool warpHasCollision(int val)
@@ -184,6 +189,7 @@ struct FastPow<DType, AccType, 2>
     return xA * xA;
   }
 };
+#endif
 
 /* Calculate norms of the rows of weight_ptr given by idx_ptr and capture them in norms */
 template <typename DType, typename AccType, typename IndexType, int Norm>
@@ -194,6 +200,7 @@ void calculate_norms_and_renorm(DType *weights,
                                 AccType maxNorm, 
                                 IndexType dim)
 {
+#if !defined(__HIP_PLATFORM_HCC__)
   // Some casting hacks since dynamic shared memory and templates don't work together:
   extern __shared__ unsigned char smem[];
   AccType *sdata = reinterpret_cast<AccType *>(smem);
@@ -223,7 +230,7 @@ void calculate_norms_and_renorm(DType *weights,
       weights[baseIndex + i] *= factor;
     }
   }
-
+#endif
 }
 
 #include "generic/LookupTable.cu"
diff --git a/src/THCUNN/LookupTableBag.cu b/src/THCUNN/LookupTableBag.cu
index bf3aa32bf..882ee4907 100644
--- a/src/THCUNN/LookupTableBag.cu
+++ b/src/THCUNN/LookupTableBag.cu
@@ -38,7 +38,9 @@ __global__ void cunn_LookupTableBag_updateOutputKernel(
       Dtype*  weightFeat = weight + featureDim;
       int64_t begin = offsets[bag] - TH_INDEX_BASE;
       int64_t end = (bag < numBags - 1) ? (offsets[bag + 1] - TH_INDEX_BASE) : numIndices;
+#if defined(__NVCC__)
       assert(end >= begin);
+#endif
       Acctype weightFeatSum = ScalarConvert<float, Acctype>::to(0);
       int64_t bag_size_ = 0;
       for (int64_t emb = begin; emb < end; emb++) {
diff --git a/src/THCUNN/MSECriterion.cu b/src/THCUNN/MSECriterion.cu
index dd26e88bf..3e5b2511f 100644
--- a/src/THCUNN/MSECriterion.cu
+++ b/src/THCUNN/MSECriterion.cu
@@ -16,8 +16,6 @@
 template <typename Dtype, typename Acctype>
 struct mse_functor
 {
-  mse_functor() {}
-
   __host__ __device__ Acctype operator()(const Dtype &x, const Dtype &y) const
   {
     Acctype z = ScalarConvert<Dtype, Acctype>::to(x)-y;
diff --git a/src/THCUNN/RReLU.cu b/src/THCUNN/RReLU.cu
index bf4503515..4af16463a 100644
--- a/src/THCUNN/RReLU.cu
+++ b/src/THCUNN/RReLU.cu
@@ -2,6 +2,7 @@
 #include "THCHalf.h"
 #include "THCHalfAutoNumerics.cuh"
 #include <THC/THCApply.cuh>
+#if defined(__NVCC__)
 #include "common.h"
 #include <curand.h>
 #include <curand_kernel.h>
@@ -119,6 +120,7 @@ struct RReLUupdateGradInputEvalIP_functor
     }
   }
 };
+#endif
 
 #include "generic/RReLU.cu"
 #include "THCGenerateFloatTypes.h"
diff --git a/src/THCUNN/SmoothL1Criterion.cu b/src/THCUNN/SmoothL1Criterion.cu
index 8e35599ba..8b32c9246 100644
--- a/src/THCUNN/SmoothL1Criterion.cu
+++ b/src/THCUNN/SmoothL1Criterion.cu
@@ -70,6 +70,7 @@ struct smoothl1_updateGradInput_functor
   const Dtype norm;
   const Dtype gradOutput;
 
+  __host__ __device__
   smoothl1_updateGradInput_functor(Dtype norm_, Dtype gradOutput_)
     : norm(norm_), gradOutput(gradOutput_)
   {}
diff --git a/src/THCUNN/SoftMaxCommon.cuh b/src/THCUNN/SoftMaxCommon.cuh
index 692a08079..3fb2ca79c 100644
--- a/src/THCUNN/SoftMaxCommon.cuh
+++ b/src/THCUNN/SoftMaxCommon.cuh
@@ -50,9 +50,13 @@ void SpatialSoftMax_getLaunchSizes(
   block = SpatialSoftMax_getBlockSize(outer_size, dim_size, inner_size);
   uint32_t block_threads = block.x * block.y;
   smem_size = block.x == 1 ? 0 : block_threads * sizeof(AccumT);
+#if defined(__HIP_PLATFORM_HCC__)
+  int max_active_blocks = 8;
+#else
   int max_active_blocks;
   cudaOccupancyMaxActiveBlocksPerMultiprocessor(&max_active_blocks,
                                                 k, block_threads, smem_size);
+#endif
   max_active_blocks *= THCState_getCurrentDeviceProperties(state)->multiProcessorCount;
   grid = SpatialSoftMax_getGridSize(block, max_active_blocks, outer_size, dim_size, inner_size);
 }
@@ -410,7 +414,7 @@ void HostSoftMaxForward(
 
     cunn_SoftMaxForward<ILP, T, AccumT, Epilogue>
       <<<grid, block, block.x * sizeof(AccumT), THCState_getCurrentStream(state)>>>(
-        output, input, dim_size
+        output, input, static_cast<int>(dim_size)
     );
   // This kernel runs in a 2D grid, where each application along y dimension has a fixed
   // outer_size, and runs in parallel over inner_size. Dimension x is parallel over outer_size.
@@ -420,12 +424,13 @@ void HostSoftMaxForward(
     dim3 grid, block;
     SpatialSoftMax_getLaunchSizes<AccumT>(
         state, &cunn_SpatialSoftMaxForward<T, AccumT, Epilogue>,
-        outer_size, dim_size, inner_size,
+        static_cast<uint32_t>(outer_size), static_cast<uint32_t>(dim_size), static_cast<uint32_t>(inner_size),
         grid, block, smem_size);
 
     cunn_SpatialSoftMaxForward<T, AccumT, Epilogue>
       <<<grid, block, smem_size, THCState_getCurrentStream(state)>>>(
-        output, input, outer_size, dim_size, inner_size
+        output, input, 
+        static_cast<uint32_t>(outer_size), static_cast<uint32_t>(dim_size), static_cast<uint32_t>(inner_size)
     );
   }
   THCudaCheck(cudaGetLastError());
@@ -446,19 +451,20 @@ void HostSoftMaxBackward(
 
     cunn_SoftMaxBackward<ILP, T, AccumT, Epilogue>
       <<<grid, block, block.x * sizeof(AccumT), THCState_getCurrentStream(state)>>>(
-        gradInput, output, gradOutput, dim_size
+        gradInput, output, gradOutput, static_cast<int>(dim_size)
     );
   } else {
     uint32_t smem_size;
     dim3 grid, block;
     SpatialSoftMax_getLaunchSizes<AccumT>(
         state, &cunn_SpatialSoftMaxBackward<T, AccumT, Epilogue>,
-        outer_size, dim_size, inner_size,
+        static_cast<uint32_t>(outer_size), static_cast<uint32_t>(dim_size), static_cast<uint32_t>(inner_size),
         grid, block, smem_size);
 
     cunn_SpatialSoftMaxBackward<T, AccumT, Epilogue>
       <<<grid, block, smem_size, THCState_getCurrentStream(state)>>>(
-        gradInput, output, gradOutput, outer_size, dim_size, inner_size
+        gradInput, output, gradOutput, 
+        static_cast<uint32_t>(outer_size), static_cast<uint32_t>(dim_size), static_cast<uint32_t>(inner_size)
     );
   }
   THCudaCheck(cudaGetLastError());
diff --git a/src/THCUNN/SparseLinear.cu b/src/THCUNN/SparseLinear.cu
index 9110bbcac..596c970f7 100644
--- a/src/THCUNN/SparseLinear.cu
+++ b/src/THCUNN/SparseLinear.cu
@@ -2,6 +2,7 @@
 #include "THCHalf.h"
 #include "THCHalfAutoNumerics.cuh"
 
+#if defined(__NVCC__)
 #include <cusparse.h>
 
 static cusparseHandle_t cusparse_handle = 0;
@@ -14,6 +15,7 @@ static void init_cusparse() {
     }
   }
 }
+#endif
 
 #ifdef CUDA_HALF_TENSOR
 void THNN_CudaHalfSparseLinear_updateOutput(
diff --git a/src/THCUNN/SpatialClassNLLCriterion.cu b/src/THCUNN/SpatialClassNLLCriterion.cu
index 83addd09a..06e32f544 100644
--- a/src/THCUNN/SpatialClassNLLCriterion.cu
+++ b/src/THCUNN/SpatialClassNLLCriterion.cu
@@ -62,7 +62,11 @@ __global__ void SpatialClassNLLCriterion_updateGradInput_no_reduce_kernel(
     }
     Dtype value =
         -(weights ? weights[cur_target] : ScalarConvert<int, Dtype>::to(1));
+#if defined(__HIP_PLATFORM_HCC__)
+    gradInput[b][cur_target][h][w] = value * gradOutput[b][h][w].template as<Dtype>();
+#else
     gradInput[b][cur_target][h][w] = value * gradOutput[b][h][w];
+#endif
   }
 }
 
@@ -96,7 +100,9 @@ __global__ void cunn_SpatialClassNLLCriterion_updateOutput_kernel(
        i += step) {
     t = target[toffset + i] - TH_INDEX_BASE;
     if (t != ignore_index) {
+#if defined(__NVCC__)
       assert(t >= 0 && t < n_classes);
+#endif
       cur_weight = weights ? weights[t] : ScalarConvert<int, T>::to(1);
       input_sum -= input[ioffset + i + map_nelem * t] * cur_weight;
       acc_weight += cur_weight;
@@ -151,7 +157,9 @@ __global__ void cunn_SpatialClassNLLCriterion_updateGradInput_kernel(
        i += step) {
     t = (int)target[toffset + i] - TH_INDEX_BASE;
     if (t != ignore_index) {
+#if defined(__NVCC__)
       assert(t >= 0 && t < n_classes);
+#endif
       gradInput[ioffset + i + map_nelem * t] = -(weights ? weights[t] : ScalarConvert<int, T>::to(1)) * norm * gradOutput[0];
     }
   }
diff --git a/src/THCUNN/SpatialFractionalMaxPooling.cu b/src/THCUNN/SpatialFractionalMaxPooling.cu
index f3ca16245..f1b5bbefa 100644
--- a/src/THCUNN/SpatialFractionalMaxPooling.cu
+++ b/src/THCUNN/SpatialFractionalMaxPooling.cu
@@ -74,9 +74,10 @@ __global__ void SpatialFractionalMaxPooling_updateOutput(
       }
     }
 
+#if defined(__NVCC__)
     assert(THCNumerics<Dtype>::ne(maxVal, THCNumerics<Dtype>::min()));
     assert(maxIndex != -1);
-
+#endif
     // +1 for Lua index
     indices[batch][plane][outputH][outputW] = maxIndex + TH_INDEX_BASE;
     output[batch][plane][outputH][outputW] = maxVal;
@@ -99,11 +100,14 @@ __global__ void SpatialFractionalMaxPooling_updateGradInput(
     int outputH = ourOutputPoint / gradOutput.getSize(3);
 
     int index = indices[batch][plane][outputH][outputW] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(index >= 0);
+#endif
     int inputW = index % gradInput.getSize(3);
     int inputH = index / gradInput.getSize(3);
+#if defined(__NVCC__)
     assert(inputH < gradInput.getSize(2));
-
+#endif
     atomicAdd(gradInput[batch][plane][inputH][inputW].data(),
               gradOutput[batch][plane][outputH][outputW]);
   }
diff --git a/src/THCUNN/SpatialGridSamplerBilinear.cu b/src/THCUNN/SpatialGridSamplerBilinear.cu
index 3bbc10557..592bff544 100644
--- a/src/THCUNN/SpatialGridSamplerBilinear.cu
+++ b/src/THCUNN/SpatialGridSamplerBilinear.cu
@@ -87,6 +87,21 @@ __global__ void SpatialGridSamplerBilinear_updateOutput_kernel(
     Dtype out_val;
     for (c = 0; c < C; ++c) {
       out_val = ScalarConvert<int,Dtype>::to(0);
+#if defined(__HIP_PLATFORM_HCC__)
+      if (WITHIN_BOUNDS(ix_nw, iy_nw, IH, IW)) {
+        out_val += (input[n][c][iy_nw][ix_nw]).template as<Dtype>() * nw;
+      }
+      if (WITHIN_BOUNDS(ix_ne, iy_ne, IH, IW)) {
+        out_val += (input[n][c][iy_ne][ix_ne]).template as<Dtype>() * ne;
+      }
+      if (WITHIN_BOUNDS(ix_sw, iy_sw, IH, IW)) {
+        out_val += (input[n][c][iy_sw][ix_sw]).template as<Dtype>() * sw;
+      }
+      if (WITHIN_BOUNDS(ix_se, iy_se, IH, IW)) {
+        out_val += (input[n][c][iy_se][ix_se]).template as<Dtype>() * se;
+      }
+      output[n][c][h][w] = out_val;
+#else
       if (WITHIN_BOUNDS(ix_nw, iy_nw, IH, IW)) {
         out_val += input[n][c][iy_nw][ix_nw] * nw;
       }
@@ -100,6 +115,7 @@ __global__ void SpatialGridSamplerBilinear_updateOutput_kernel(
         out_val += input[n][c][iy_se][ix_se] * se;
       }
       output[n][c][h][w] = out_val;
+#endif
     }
   }
 }
diff --git a/src/THCUNN/SpatialUpSamplingBilinear.cu b/src/THCUNN/SpatialUpSamplingBilinear.cu
index 039f8e737..86c0083e3 100644
--- a/src/THCUNN/SpatialUpSamplingBilinear.cu
+++ b/src/THCUNN/SpatialUpSamplingBilinear.cu
@@ -51,11 +51,19 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
     //
     for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
+#if defined(__HIP_PLATFORM_HCC__)
+        const Acctype val = h0lambda * (w0lambda * (data1[n][c][h1][w1]).template as<Acctype>()
+                            + w1lambda * (data1[n][c][h1][w1+w1p]).template as<Acctype>())
+                            + h1lambda * (w0lambda * (data1[n][c][h1+h1p][w1]).template as<Acctype>()
+                            + w1lambda * (data1[n][c][h1+h1p][w1+w1p]).template as<Acctype>());
+        data2[n][c][h2][w2] = ScalarConvert<Acctype, Dtype>::to(val);
+#else
         const Acctype val = h0lambda * (w0lambda * data1[n][c][h1][w1]
                             + w1lambda * data1[n][c][h1][w1+w1p])
                             + h1lambda * (w0lambda * data1[n][c][h1+h1p][w1]
                             + w1lambda * data1[n][c][h1+h1p][w1+w1p]);
         data2[n][c][h2][w2] = ScalarConvert<Acctype, Dtype>::to(val);
+#endif
       }
     }
   }
@@ -83,7 +91,11 @@ __global__ void caffe_gpu_interp2_kernel_backward(const int n,
       for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
           const Dtype val = data2[n][c][h1][w1];
+#if defined(__HIP_PLATFORM_HCC__)
+          (data1[n][c][h2][w2]).template as<Dtype>() += val;
+#else
           data1[n][c][h2][w2] += val;
+#endif
         }
       }
       return;
diff --git a/src/THCUNN/THCHalfAutoNumerics.cuh b/src/THCUNN/THCHalfAutoNumerics.cuh
index 6847479ba..a6c9b3239 100644
--- a/src/THCUNN/THCHalfAutoNumerics.cuh
+++ b/src/THCUNN/THCHalfAutoNumerics.cuh
@@ -31,6 +31,8 @@ inline __host__ __device__ double fmaxType(double x, double y) {
 
 // arithmetic functions
 
+#if defined(__HIP_PLATFORM_HCC__)
+#else
 inline __host__ __device__ half operator+(half a, half b) {
   return THCNumerics<half>::add(a, b);
 }
@@ -159,6 +161,7 @@ inline __host__ __device__ half& operator/=(half &lhs, const half &rhs) {
   lhs = lhs / rhs;
   return lhs;
 }
+#endif
 
 inline __host__ __device__ half abs(half a) {
   return THCNumerics<half>::abs(a);
@@ -204,6 +207,8 @@ inline __host__ __device__ half operator*(half a, bool b) {
 
 // comparison functions
 
+#if defined(__HIP_PLATFORM_HCC__)
+#else
 inline __host__ __device__ bool operator<(half a, half b) {
   return THCNumerics<half>::lt(a, b);
 }
@@ -235,6 +240,7 @@ inline __host__ __device__ bool operator>=(half a, half b) {
 inline __host__ __device__ bool operator>=(half a, int b) {
   return THCNumerics<half>::ge(a, ScalarConvert<int ,half>::to(b));
 }
+#endif
 
 #endif
 #endif
diff --git a/src/THCUNN/TemporalUpSamplingLinear.cu b/src/THCUNN/TemporalUpSamplingLinear.cu
index fe59b2a66..173d75393 100644
--- a/src/THCUNN/TemporalUpSamplingLinear.cu
+++ b/src/THCUNN/TemporalUpSamplingLinear.cu
@@ -41,8 +41,13 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
     //
     for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
+#if defined(__HIP_PLATFORM_HCC__)
+        const Acctype val = w0lambda * (data1[n][c][w1]).template as<Acctype>()
+                            + w1lambda * (data1[n][c][w1+w1p]).template as<Acctype>();
+#else
         const Acctype val = w0lambda * data1[n][c][w1]
                             + w1lambda * data1[n][c][w1+w1p];
+#endif
         data2[n][c][w2] = ScalarConvert<Acctype, Dtype>::to(val);
       }
     }
@@ -67,7 +72,11 @@ __global__ void caffe_gpu_interp2_kernel_backward(const int n,
       for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
           const Dtype val = data2[n][c][w1];
+#if defined(__HIP_PLATFORM_HCC__)
+          (data1[n][c][w2]).template as<Dtype>() += val;
+#else
           data1[n][c][w2] += val;
+#endif
         }
       }
       return;
diff --git a/src/THCUNN/VolumetricAveragePooling.cu b/src/THCUNN/VolumetricAveragePooling.cu
index cdc3d04ed..a2ee0d78b 100644
--- a/src/THCUNN/VolumetricAveragePooling.cu
+++ b/src/THCUNN/VolumetricAveragePooling.cu
@@ -120,11 +120,19 @@ __global__ void cuda_VolumetricAveragePooling_updateOutput_fixedKW(
   }
 }
 
+#if defined(__HIP_PLATFORM_HCC__)
+#define LAUNCH_UPDATE_OUTPUT_KERNEL_WIDTH(KW) case KW: \
+  hipLaunchKernelGGL((cuda_VolumetricAveragePooling_updateOutput_fixedKW<KW, real, accreal>), \
+    dim3(grid), dim3(block), 0, 0, \
+    cudaInput, cudaOutput, kT, kH, dT, dH, dW, padT, padH, padW, count_include_pad, offsetZ); \
+  break
+#else
 #define LAUNCH_UPDATE_OUTPUT_KERNEL_WIDTH(KW) case KW: \
   cuda_VolumetricAveragePooling_updateOutput_fixedKW<KW, real, accreal> \
     <<<grid, block, 0, THCState_getCurrentStream(state)>>>( \
       cudaInput, cudaOutput, kT, kH, dT, dH, dW, padT, padH, padW, count_include_pad, offsetZ); \
   break
+#endif
 
 template <typename Dtype, typename Acctype>
 __global__ void cuda_VolumetricAveragePooling_updateGradInput_Stride1(
diff --git a/src/THCUNN/VolumetricFractionalMaxPooling.cu b/src/THCUNN/VolumetricFractionalMaxPooling.cu
index e6260ceab..f6cf0b810 100644
--- a/src/THCUNN/VolumetricFractionalMaxPooling.cu
+++ b/src/THCUNN/VolumetricFractionalMaxPooling.cu
@@ -79,9 +79,10 @@ __global__ void VolumetricFractionalMaxPooling_updateOutput(
       }
     }
 
+#if defined(__NVCC__)
     assert(THCNumerics<Dtype>::ne(maxVal, THCNumerics<Dtype>::min()));
     assert(maxIndex != -1);
-
+#endif
     // +1 for Lua index
     indices[batch][plane][outputH][outputW][outputT] = maxIndex + TH_INDEX_BASE;
     output[batch][plane][outputH][outputW][outputT] = maxVal;
@@ -105,12 +106,15 @@ __global__ void VolumetricFractionalMaxPooling_updateGradInput(
     int outputH = ourOutputPoint / (gradOutput.getSize(3)*gradOutput.getSize(4));
 
     int index = indices[batch][plane][outputH][outputW][outputT] - TH_INDEX_BASE;
+#if defined(__NVCC__)
     assert(index >= 0);
+#endif
     int inputT = index % gradInput.getSize(4);
     int inputW = (index / gradInput.getSize(4)) % gradInput.getSize(3);
     int inputH = index / (gradInput.getSize(3) * gradInput.getSize(4));
+#if defined(__NVCC__)
     assert(inputH < gradInput.getSize(2));
-
+#endif
     atomicAdd(gradInput[batch][plane][inputH][inputW][inputT].data(),
               gradOutput[batch][plane][outputH][outputW][outputT]);
   }
diff --git a/src/THCUNN/VolumetricUpSamplingTrilinear.cu b/src/THCUNN/VolumetricUpSamplingTrilinear.cu
index 5d1493d19..6c91e8033 100644
--- a/src/THCUNN/VolumetricUpSamplingTrilinear.cu
+++ b/src/THCUNN/VolumetricUpSamplingTrilinear.cu
@@ -62,6 +62,17 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
     //
     for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
+#if defined(__HIP_PLATFORM_HCC__)
+        const Acctype val = t0lambda * (h0lambda * (w0lambda * (data1[n][c][t1][h1][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1][h1][w1+w1p]).template as<Acctype>())
+                                      + h1lambda * (w0lambda * (data1[n][c][t1][h1+h1p][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1][h1+h1p][w1+w1p]).template as<Acctype>()))
+                          + t1lambda * (h0lambda * (w0lambda * (data1[n][c][t1+t1p][h1][w1]).template as<Acctype>() 
+                                                  + w1lambda * (data1[n][c][t1+t1p][h1][w1+w1p]).template as<Acctype>())
+                                      + h1lambda * (w0lambda * (data1[n][c][t1+t1p][h1+h1p][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1+t1p][h1+h1p][w1+w1p]).template as<Acctype>()));
+        data2[n][c][t2][h2][w2] = ScalarConvert<Acctype, Dtype>::to(val);
+#else
         const Acctype val = t0lambda * (h0lambda * (w0lambda * data1[n][c][t1][h1][w1] 
                                                   + w1lambda * data1[n][c][t1][h1][w1+w1p])
                                       + h1lambda * (w0lambda * data1[n][c][t1][h1+h1p][w1]
@@ -71,6 +82,7 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
                                       + h1lambda * (w0lambda * data1[n][c][t1+t1p][h1+h1p][w1]
                                                   + w1lambda * data1[n][c][t1+t1p][h1+h1p][w1+w1p]));
         data2[n][c][t2][h2][w2] = ScalarConvert<Acctype, Dtype>::to(val);
+#endif
       }
     }
   }
@@ -103,7 +115,11 @@ __global__ void caffe_gpu_interp2_kernel_backward(const int n,
       for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
           const Dtype val = data2[n][c][t1][h1][w1];
+#if defined(__HIP_PLATFORM_HCC__)
+          (data1[n][c][t2][h2][w2]).template as<Acctype>() += val;
+#else
           data1[n][c][t2][h2][w2] += val;
+#endif
         }
       }
       return;
diff --git a/src/THCUNN/generic/BatchNormalization.cu b/src/THCUNN/generic/BatchNormalization.cu
index eb2dc8405..24f422760 100644
--- a/src/THCUNN/generic/BatchNormalization.cu
+++ b/src/THCUNN/generic/BatchNormalization.cu
@@ -64,7 +64,7 @@ void THNN_(BatchNormalization_updateOutput)(
     dim3 blocks(input.getSize(1));
     dim3 threads(getNumThreads(input.getSize(2)));
     BatchNormalizationUpdateOutput_kernel<real, accreal, DeviceTensor1, DeviceTensor3> <<<blocks, threads, 0, s>>>(
-      input, output, weight, bias, eps, momentum, runningMean, runningVar,
+      input, output, weight, bias, static_cast<accreal>(eps), static_cast<accreal>(momentum), runningMean, runningVar,
       saveMean, saveStd);
   }
   THCudaCheck(cudaGetLastError());
@@ -98,7 +98,7 @@ void THNN_(BatchNormalization_backward)(
   dim3 threads(getNumThreads(gradOutput.getSize(2)));
   BatchNormalizationBackward_kernel<real,  accreal,  DeviceTensor1, DeviceTensor3> <<<blocks, threads, 0, s>>>(
     input, gradOutput, gradInput, gradWeight, gradBias, weight, runningMean, runningVar,
-    saveMean, saveStd, train, scale, eps);
+    saveMean, saveStd, train, static_cast<accreal>(scale), static_cast<accreal>(eps));
   THCudaCheck(cudaGetLastError());
 }
 
diff --git a/src/THCUNN/generic/ClassNLLCriterion.cu b/src/THCUNN/generic/ClassNLLCriterion.cu
index ea6950412..0de4eea6e 100644
--- a/src/THCUNN/generic/ClassNLLCriterion.cu
+++ b/src/THCUNN/generic/ClassNLLCriterion.cu
@@ -57,8 +57,8 @@ void THNN_(ClassNLLCriterion_updateOutput)(
         toDeviceTensor<THCIndex_t, 1>(state, target),
         toDeviceTensor<real, 1>(state, output),
         weights ? THCTensor_(data)(state, weights) : NULL,
-        n_classes,
-        ignore_index);
+        static_cast<int>(n_classes),
+        static_cast<int>(ignore_index));
 
     THCudaCheck(cudaGetLastError());
 
@@ -93,7 +93,7 @@ void THNN_(ClassNLLCriterion_updateOutput)(
         input_data,
         target_data,
         weights_data,
-        sizeAverage,
+        static_cast<int>(sizeAverage),
         n_classes,
         ignore_index
     );
@@ -106,9 +106,9 @@ void THNN_(ClassNLLCriterion_updateOutput)(
         input_data,
         target_data,
         weights_data,
-        sizeAverage,
-        THCTensor_(size)(state, input, 0),
-        THCTensor_(size)(state, input, 1),
+        static_cast<int>(sizeAverage),
+        static_cast<int>(THCTensor_(size)(state, input, 0)),
+        static_cast<int>(THCTensor_(size)(state, input, 1)),
         n_classes,
         ignore_index
     );
@@ -180,8 +180,8 @@ void THNN_(ClassNLLCriterion_updateGradInput)(
         toDeviceTensor<real, 1>(state, gradOutput),
         toDeviceTensor<real, 2>(state, gradInput),
         weights ? THCTensor_(data)(state, weights) : NULL,
-        n_classes,
-        ignore_index);
+        static_cast<int>(n_classes),
+        static_cast<int>(ignore_index));
 
     THCudaCheck(cudaGetLastError());
 
@@ -215,7 +215,7 @@ void THNN_(ClassNLLCriterion_updateGradInput)(
         weights_data,
         target_data,
         total_weight_data,
-        sizeAverage,
+        static_cast<int>(sizeAverage),
         n_classes,
         ignore_index
     );
@@ -227,9 +227,9 @@ void THNN_(ClassNLLCriterion_updateGradInput)(
         target_data,
         weights_data,
         total_weight_data,
-        sizeAverage,
-        THCTensor_(size)(state, input, 0),
-        THCTensor_(size)(state, input, 1),
+        static_cast<int>(sizeAverage),
+        static_cast<int>(THCTensor_(size)(state, input, 0)),
+        static_cast<int>(THCTensor_(size)(state, input, 1)),
         n_classes,
         ignore_index
     );
diff --git a/src/THCUNN/generic/FusedRNNKernel.cu b/src/THCUNN/generic/FusedRNNKernel.cu
index 10ec710bd..c10c59dc7 100644
--- a/src/THCUNN/generic/FusedRNNKernel.cu
+++ b/src/THCUNN/generic/FusedRNNKernel.cu
@@ -383,31 +383,62 @@ __global__ void
     break;                                      \
   }
 
-#define LSTM_FORWARD(ITYPE, DIM) THNN_(LSTMForward)             \
-  <real, ITYPE, DIM>                                            \
-  <<<grid, block, 0, THCState_getCurrentStream(state)>>>        \
-  (inputI, hiddenI,                                             \
-   bias1I, bias2I, cxI, hyI, cyI,                               \
-   hid_size, totalElements);
-
-#define LSTM_BACKWARD(ITYPE, DIM) THNN_(LSTMBackward)           \
-  <real, ITYPE, DIM>                                            \
-  <<<grid, block, 0, THCState_getCurrentStream(state)>>>        \
-  (storageI, gradingatesI, cxI, cyI,                            \
-   gradoutI, gradoutcI, gradincxI,                              \
-   hid_size, totalElements);
-
-#define GRU_FORWARD(ITYPE, DIM) THNN_(GRUForward)<real, ITYPE, DIM> \
-  <<<grid, block, 0, THCState_getCurrentStream(state)>>>            \
-  (inputI, hiddenI, bias1I, bias2I, hxI, hyI, storageI,             \
-   hid_size, totalElements);
-
-#define GRU_BACKWARD(ITYPE, DIM) THNN_(GRUBackward)                     \
-  <real, ITYPE, DIM>                                                    \
-  <<<grid, block, 0, THCState_getCurrentStream(state)>>>                \
-  (gradininputI, gradinhiddenI, gradoutI, gradinhxI, storageI,                        \
-   hid_size, totalElements);
-
+#if defined(__HIP_PLATFORM_HCC__)
+  #define LSTM_FORWARD(ITYPE, DIM)                                \
+    hipLaunchKernelGGL(                                           \
+    (THNN_(LSTMForward) <real, ITYPE, DIM>),                      \
+     grid, block, 0, THCState_getCurrentStream(state),            \
+     inputI, hiddenI,                                             \
+     bias1I, bias2I, cxI, hyI, cyI,                               \
+     hid_size, totalElements);
+
+  #define LSTM_BACKWARD(ITYPE, DIM)                               \
+    hipLaunchKernelGGL(                                           \
+    (THNN_(LSTMBackward) <real, ITYPE, DIM>),                     \
+     grid, block, 0, THCState_getCurrentStream(state),            \
+     storageI, gradingatesI, cxI, cyI,                            \
+     gradoutI, gradoutcI, gradincxI,                              \
+     hid_size, totalElements);
+
+  #define GRU_FORWARD(ITYPE, DIM)                                    \
+    hipLaunchKernelGGL(                                              \
+    (THNN_(GRUForward)<real, ITYPE, DIM>),                           \
+     grid, block, 0, THCState_getCurrentStream(state),               \
+     inputI, hiddenI, bias1I, bias2I, hxI, hyI, storageI,            \
+     hid_size, totalElements);
+
+  #define GRU_BACKWARD(ITYPE, DIM)                                   \
+    hipLaunchKernelGGL(                                              \
+    (THNN_(GRUBackward) <real, ITYPE, DIM>),                         \
+     grid, block, 0, THCState_getCurrentStream(state),               \
+     gradininputI, gradinhiddenI, gradoutI, gradinhxI, storageI,     \
+     hid_size, totalElements);
+#else
+  #define LSTM_FORWARD(ITYPE, DIM) THNN_(LSTMForward)             \
+    <real, ITYPE, DIM>                                            \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>        \
+    (inputI, hiddenI,                                             \
+     bias1I, bias2I, cxI, hyI, cyI,                               \
+     hid_size, totalElements);
+
+  #define LSTM_BACKWARD(ITYPE, DIM) THNN_(LSTMBackward)           \
+    <real, ITYPE, DIM>                                            \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>        \
+    (storageI, gradingatesI, cxI, cyI,                            \
+     gradoutI, gradoutcI, gradincxI,                              \
+     hid_size, totalElements);
+
+  #define GRU_FORWARD(ITYPE, DIM) THNN_(GRUForward)<real, ITYPE, DIM> \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>            \
+    (inputI, hiddenI, bias1I, bias2I, hxI, hyI, storageI,             \
+     hid_size, totalElements);
+
+  #define GRU_BACKWARD(ITYPE, DIM) THNN_(GRUBackward)                     \
+    <real, ITYPE, DIM>                                                    \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>                \
+    (gradininputI, gradinhiddenI, gradoutI, gradinhxI, storageI,          \
+     hid_size, totalElements);
+#endif
 // ************ END Create actual function calls ************ //
 
 template<typename INDTYPE>
diff --git a/src/THCUNN/generic/LookupTable.cu b/src/THCUNN/generic/LookupTable.cu
index 38179fa84..4e85aa31d 100644
--- a/src/THCUNN/generic/LookupTable.cu
+++ b/src/THCUNN/generic/LookupTable.cu
@@ -14,6 +14,7 @@ void THNN_(LookupTable_accGradParameters)(
            int paddingValue,
            accreal scale_)
 {
+#if defined(__NVCC__)
   real scale = ScalarConvert<accreal, real>::to(scale_);
   THCUNN_assertSameGPU(state, 5, input, gradOutput, gradWeight, sortedIndices, origIndices);
   gradOutput = THCTensor_(newContiguous)(state, gradOutput);
@@ -143,6 +144,7 @@ void THNN_(LookupTable_accGradParameters)(
 
   THCTensor_(free)(state, gradOutput);
   THCudaCheck(cudaGetLastError());
+#endif
 }
 
 #define THREADS 256
@@ -158,6 +160,7 @@ void THNN_(LookupTable_renorm)(
            accreal maxNorm,
            accreal normType)
 {
+#if defined(__NVCC__)
   THCUNN_assertSameGPU(state, 2, idx, weight);
   if (!(THCIndexTensor_(isContiguous)(state, idx) &&
         THCTensor_(isContiguous)(state, weight))) {
@@ -201,6 +204,7 @@ void THNN_(LookupTable_renorm)(
       RUN(-1, unsigned long);
     }
   }
+#endif
 }
 
 #endif
diff --git a/src/THCUNN/generic/PReLU.cu b/src/THCUNN/generic/PReLU.cu
index 088847dd5..9ec0b4136 100644
--- a/src/THCUNN/generic/PReLU.cu
+++ b/src/THCUNN/generic/PReLU.cu
@@ -34,8 +34,8 @@ void THNN_(PReLU_updateOutput)(
     int nElemsPerSample = nOutputPlane * mapSize;
     preluForward<<<GET_BLOCKS(n), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(
       THCTensor_(data)(state, output),
-      THCTensor_(data)(state, input),
-      w,
+      static_cast<const real*>(THCTensor_(data)(state, input)),
+      static_cast<const real*>(w),
       n, nElemsPerSample, mapSize
     );
     THCudaCheck(cudaGetLastError());
@@ -79,9 +79,9 @@ void THNN_(PReLU_updateGradInput)(
     int nElemsPerSample = nOutputPlane * mapSize;
     preluBackward<<<GET_BLOCKS(n), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(
       THCTensor_(data)(state, gradInput),
-      THCTensor_(data)(state, input),
-      w,
-      THCTensor_(data)(state, gradOutput),
+      static_cast<const real*>(THCTensor_(data)(state, input)),
+      static_cast<const real*>(w),
+      static_cast<const real*>(THCTensor_(data)(state, gradOutput)),
       n, nElemsPerSample, mapSize
     );
     THCudaCheck(cudaGetLastError());
diff --git a/src/THCUNN/generic/RReLU.cu b/src/THCUNN/generic/RReLU.cu
index 5fba9eb86..ba04beb6c 100644
--- a/src/THCUNN/generic/RReLU.cu
+++ b/src/THCUNN/generic/RReLU.cu
@@ -15,6 +15,7 @@ void THNN_(RReLU_updateOutput)(
            bool inplace,
            void *generator)
 {
+#if defined(__NVCC__)
   THCUNN_assertSameGPU(state, 3, input, output, noise);
   struct curandStateMtgp32* gen_states = THCRandom_generatorStates(state);
 
@@ -55,6 +56,7 @@ void THNN_(RReLU_updateOutput)(
       THC_pointwiseApply2(state, output, input, RReLUUpdateOutputEval_functor<real>(negSlope));
     }
   }
+#endif
 }
 
 void THNN_(RReLU_updateGradInput)(
@@ -68,6 +70,7 @@ void THNN_(RReLU_updateGradInput)(
            bool train,
            bool inplace)
 {
+#if defined(__NVCC__)
   THCUNN_check_nElement(state, input, gradOutput);
   THCUNN_assertSameGPU(state, 4, input, gradOutput, gradInput, noise);
 
@@ -104,6 +107,6 @@ void THNN_(RReLU_updateGradInput)(
   }
 
   THCTensor_(free)(state, gradOutput);
+#endif
 }
-
 #endif
diff --git a/src/THCUNN/generic/SparseLinear.cu b/src/THCUNN/generic/SparseLinear.cu
index 3d143697e..4bdb8a876 100644
--- a/src/THCUNN/generic/SparseLinear.cu
+++ b/src/THCUNN/generic/SparseLinear.cu
@@ -34,6 +34,7 @@ void THNN_(SparseLinear_updateOutput)(
            THCTensor *weight,
            THCTensor *bias)
 {
+#if defined(__NVCC_)
   THAssert(THCTensor_(checkGPU)(state, 4, input, output, weight, bias));
 
   int64_t h;
@@ -120,6 +121,7 @@ void THNN_(SparseLinear_updateOutput)(
   THCudaIntTensor_free(state, rowbuf);
   THCudaIntTensor_free(state, colInds);
   THCudaIntTensor_free(state, csrPtrs);
+#endif
 }
 
 void THNN_(SparseLinear_accGradParameters)(
@@ -133,6 +135,7 @@ void THNN_(SparseLinear_accGradParameters)(
            accreal weightDecay,
            accreal scale)
 {
+#if defined(__NVCC__)
   int64_t outDim = THCTensor_(size)(state, weight, 0);
   int64_t inDim = THCTensor_(size)(state, weight, 1);
 
@@ -225,6 +228,7 @@ void THNN_(SparseLinear_accGradParameters)(
   THCudaIntTensor_free(state, colbuf);
   THCudaIntTensor_free(state, rowInds);
   THCudaIntTensor_free(state, colPtrs);
+#endif
 }
 
 void THNN_(SparseLinear_legacyUpdateOutput)(
diff --git a/src/THCUNN/generic/SpatialAdaptiveAveragePooling.cu b/src/THCUNN/generic/SpatialAdaptiveAveragePooling.cu
index 09923588e..085241071 100644
--- a/src/THCUNN/generic/SpatialAdaptiveAveragePooling.cu
+++ b/src/THCUNN/generic/SpatialAdaptiveAveragePooling.cu
@@ -22,9 +22,9 @@ void THNN_(SpatialAdaptiveAveragePooling_updateOutput)(
                   "3D or 4D (batch mode) tensor expected for input, but got: %s");
 
   if (input->nDimension == 3) {
-    int64_t sizeD  = input->size[0];
-    int64_t isizeH = input->size[1];
-    int64_t isizeW = input->size[2];
+    int32_t sizeD  = input->size[0];
+    int32_t isizeH = input->size[1];
+    int32_t isizeW = input->size[2];
 
     int64_t istrideD = input->stride[0];
     int64_t istrideH = input->stride[1];
@@ -49,10 +49,10 @@ void THNN_(SpatialAdaptiveAveragePooling_updateOutput)(
 
   } else {
     input = THCTensor_(newContiguous)(state, input);
-    int64_t sizeB  = input->size[0];
-    int64_t sizeD  = input->size[1];
-    int64_t isizeH = input->size[2];
-    int64_t isizeW = input->size[3];
+    int32_t sizeB  = input->size[0];
+    int32_t sizeD  = input->size[1];
+    int32_t isizeH = input->size[2];
+    int32_t isizeW = input->size[3];
 
     int64_t istrideD = input->stride[1];
     int64_t istrideH = input->stride[2];
@@ -95,12 +95,12 @@ void THNN_(SpatialAdaptiveAveragePooling_updateGradInput)(
   gradOutput = THCTensor_(newContiguous)(state, gradOutput);
 
   if (input->nDimension == 3) {
-    int64_t sizeD  = input->size[0];
-    int64_t isizeH = input->size[1];
-    int64_t isizeW = input->size[2];
+    int32_t sizeD  = input->size[0];
+    int32_t isizeH = input->size[1];
+    int32_t isizeW = input->size[2];
 
-    int64_t osizeH = gradOutput->size[1];
-    int64_t osizeW = gradOutput->size[2];
+    int32_t osizeH = gradOutput->size[1];
+    int32_t osizeW = gradOutput->size[2];
 
     //bool atomic = (isizeW%osizeW != 0) || (isizeH%osizeH != 0);
 
@@ -129,13 +129,13 @@ void THNN_(SpatialAdaptiveAveragePooling_updateGradInput)(
     }
     THCudaCheck(cudaGetLastError());
   } else {
-    int64_t sizeB  = input->size[0];
-    int64_t sizeD  = input->size[1];
-    int64_t isizeH = input->size[2];
-    int64_t isizeW = input->size[3];
+    int32_t sizeB  = input->size[0];
+    int32_t sizeD  = input->size[1];
+    int32_t isizeH = input->size[2];
+    int32_t isizeW = input->size[3];
 
-    int64_t osizeH = gradOutput->size[2];
-    int64_t osizeW = gradOutput->size[3];
+    int32_t osizeH = gradOutput->size[2];
+    int32_t osizeW = gradOutput->size[3];
 
     //bool atomic = //(isizeW%osizeW != 0) || (isizeH%osizeH != 0);
 
diff --git a/src/THCUNN/generic/SpatialAdaptiveMaxPooling.cu b/src/THCUNN/generic/SpatialAdaptiveMaxPooling.cu
index 99ef1daca..eba08b41d 100644
--- a/src/THCUNN/generic/SpatialAdaptiveMaxPooling.cu
+++ b/src/THCUNN/generic/SpatialAdaptiveMaxPooling.cu
@@ -49,7 +49,8 @@ void THNN_(SpatialAdaptiveMaxPooling_updateOutput)(
     // run maxpool kernel
     adaptivemaxpool <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (input_data, output_data,
                                    indices_data,
-                                   isizeH, isizeW, osizeH, osizeW,
+                                   static_cast<int>(isizeH), static_cast<int>(isizeW), 
+                                   static_cast<int>(osizeH), static_cast<int>(osizeW),
                                    istrideD, istrideH, istrideW);
     THCudaCheck(cudaGetLastError());
 
@@ -81,7 +82,8 @@ void THNN_(SpatialAdaptiveMaxPooling_updateOutput)(
     // run maxpool kernel
     adaptivemaxpool <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (input_data, output_data,
                                    indices_data,
-                                   isizeH, isizeW, osizeH, osizeW,
+                                   static_cast<int>(isizeH), static_cast<int>(isizeW), 
+                                   static_cast<int>(osizeH), static_cast<int>(osizeW),
                                    istrideD, istrideH, istrideW);
     THCudaCheck(cudaGetLastError());
     // clean
@@ -134,14 +136,16 @@ void THNN_(SpatialAdaptiveMaxPooling_updateGradInput)(
       // run updateGradInput kernel, accumulate gradients atomically
       atomicadaptivemaxgradinput <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (gradInput_data, gradOutput_data,
                                           indices_data,
-                                          isizeH, isizeW, osizeH, osizeW);
+                                          static_cast<int>(isizeH), static_cast<int>(isizeW), 
+                                          static_cast<int>(osizeH), static_cast<int>(osizeW));
     }
     else
     {
       // run updateGradInput kernel
       atomicadaptivemaxgradinput <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (gradInput_data, gradOutput_data,
                                           indices_data,
-                                          isizeH, isizeW, osizeH, osizeW);
+                                          static_cast<int>(isizeH), static_cast<int>(isizeW), 
+                                          static_cast<int>(osizeH), static_cast<int>(osizeW));
     }
     THCudaCheck(cudaGetLastError());
   } else {
@@ -173,14 +177,16 @@ void THNN_(SpatialAdaptiveMaxPooling_updateGradInput)(
       // run updateGradInput kernel, accumulate gradients atomically
       atomicadaptivemaxgradinput <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (gradInput_data, gradOutput_data,
                                           indices_data,
-                                          isizeH, isizeW, osizeH, osizeW);
+                                          static_cast<int>(isizeH), static_cast<int>(isizeW), 
+                                          static_cast<int>(osizeH), static_cast<int>(osizeW));
     }
     else
     {
       // run updateGradInput kernel, accumulate gradients atomically
       adaptivemaxgradinput <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (gradInput_data, gradOutput_data,
                                           indices_data,
-                                          isizeH, isizeW, osizeH, osizeW);
+                                          static_cast<int>(isizeH), static_cast<int>(isizeW), 
+                                          static_cast<int>(osizeH), static_cast<int>(osizeW));
     }
     THCudaCheck(cudaGetLastError());
   }
diff --git a/src/THCUNN/generic/SpatialAveragePooling.cu b/src/THCUNN/generic/SpatialAveragePooling.cu
index 500796b14..0b51fbfdc 100644
--- a/src/THCUNN/generic/SpatialAveragePooling.cu
+++ b/src/THCUNN/generic/SpatialAveragePooling.cu
@@ -132,13 +132,15 @@ void THNN_(SpatialAveragePooling_updateOutput)(
     AvePoolForward<real, accreal, true>
       <<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state) >>>(
         count, input_data,
-        batchSize, nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
+        static_cast<int>(batchSize), static_cast<int>(nInputPlane), static_cast<int>(nInputRows), 
+        static_cast<int>(nInputCols), static_cast<int>(nOutputRows), static_cast<int>(nOutputCols),
         kH, kW, dH, dW, padH, padW, output_data);
   else
     AvePoolForward<real, accreal, false>
       <<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state) >>>(
         count, input_data,
-        batchSize, nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
+        static_cast<int>(batchSize), static_cast<int>(nInputPlane), static_cast<int>(nInputRows), 
+        static_cast<int>(nInputCols), static_cast<int>(nOutputRows), static_cast<int>(nOutputCols),
         kH, kW, dH, dW, padH, padW, output_data);
   THCudaCheck(cudaGetLastError());
 
@@ -216,7 +218,8 @@ void THNN_(SpatialAveragePooling_updateGradInput)(
       <<< GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state) >>>
         (count,
         THCTensor_(data)(state, gradOutput),
-        batchSize, nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
+        static_cast<int>(batchSize), static_cast<int>(nInputPlane), static_cast<int>(nInputRows), 
+        static_cast<int>(nInputCols), static_cast<int>(nOutputRows), static_cast<int>(nOutputCols),
         kH, kW, dH, dW, padH, padW,
         THCTensor_(data)(state, gradInput));
   else
@@ -224,7 +227,8 @@ void THNN_(SpatialAveragePooling_updateGradInput)(
       <<< GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state) >>>
         (count,
         THCTensor_(data)(state, gradOutput),
-        batchSize, nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
+        static_cast<int>(batchSize), static_cast<int>(nInputPlane), static_cast<int>(nInputRows), 
+        static_cast<int>(nInputCols), static_cast<int>(nOutputRows), static_cast<int>(nOutputCols),
         kH, kW, dH, dW, padH, padW,
         THCTensor_(data)(state, gradInput));
   THCudaCheck(cudaGetLastError());
diff --git a/src/THCUNN/generic/SpatialClassNLLCriterion.cu b/src/THCUNN/generic/SpatialClassNLLCriterion.cu
index 0f97809aa..c7b3c3326 100644
--- a/src/THCUNN/generic/SpatialClassNLLCriterion.cu
+++ b/src/THCUNN/generic/SpatialClassNLLCriterion.cu
@@ -121,10 +121,10 @@ void THNN_(SpatialClassNLLCriterion_updateOutput)(
       input_data,
       target_data,
       weights_data,
-      sizeAverage,
-      THCTensor_(size)(state, input, 0),
-      THCTensor_(size)(state, input, 1),
-      THCTensor_(size)(state, input, 2) * THCTensor_(size)(state, input, 3),
+      static_cast<int>(sizeAverage),
+      static_cast<int>(THCTensor_(size)(state, input, 0)),
+      static_cast<int>(THCTensor_(size)(state, input, 1)),
+      static_cast<int>(THCTensor_(size)(state, input, 2) * THCTensor_(size)(state, input, 3)),
       blocks_per_sample,
       ignore_index
   );
@@ -219,10 +219,10 @@ void THNN_(SpatialClassNLLCriterion_updateGradInput)(
       target_data,
       weights_data,
       total_weight_data,
-      sizeAverage,
-      THCTensor_(size)(state, input, 0),
-      THCTensor_(size)(state, input, 1),
-      THCTensor_(size)(state, input, 2) *THCTensor_(size)(state, input, 3),
+      static_cast<int>(sizeAverage),
+      static_cast<int>(THCTensor_(size)(state, input, 0)),
+      static_cast<int>(THCTensor_(size)(state, input, 1)),
+      static_cast<int>(THCTensor_(size)(state, input, 2) *THCTensor_(size)(state, input, 3)),
       blocks_per_sample,
       ignore_index
   );
diff --git a/src/THCUNN/generic/SpatialCrossMapLRN.cu b/src/THCUNN/generic/SpatialCrossMapLRN.cu
index 61a1c7046..fddb8e51c 100644
--- a/src/THCUNN/generic/SpatialCrossMapLRN.cu
+++ b/src/THCUNN/generic/SpatialCrossMapLRN.cu
@@ -40,7 +40,7 @@ void LRNforward(THCState* state, THCTensor* input, THCTensor* output,
   n_threads *= nInputPlane;
   THCudaCheck(cudaGetLastError());
   LRNComputeOutput<<<GET_BLOCKS(n_threads), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(
-    n_threads, THCTensor_(data)(state, input), THCTensor_(data)(state, scale), -beta, THCTensor_(data)(state, output));
+    n_threads, static_cast<const real*>(THCTensor_(data)(state, input)), static_cast<const real*>(THCTensor_(data)(state, scale)), -beta, THCTensor_(data)(state, output));
   THCudaCheck(cudaGetLastError());
 
   THCTensor_(free)(state, input);
@@ -80,12 +80,14 @@ void LRNbackward(THCState* state, THCTensor* input, THCTensor* output,
   gradOutput = THCTensor_(newContiguous)(state, gradOutput);
 
   int n_threads = batchSize * imsize_h * imsize_w;
+#if defined(__NVCC__)
   LRNComputeDiff<real, accreal> <<<GET_BLOCKS(n_threads), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(
       n_threads, THCTensor_(data)(state, input), THCTensor_(data)(state, output),
       THCTensor_(data)(state, scale), THCTensor_(data)(state, gradOutput), batchSize, nInputPlane, imsize_h, imsize_w,
       local_size, -beta, ScalarConvert<int, real>::to(2) * alpha * beta / local_size,
       THCTensor_(data)(state, gradInput));
   THCudaCheck(cudaGetLastError());
+#endif
 
   THCTensor_(free)(state, input);
   THCTensor_(free)(state, gradOutput);
diff --git a/src/THCUNN/generic/SpatialDilatedMaxPooling.cu b/src/THCUNN/generic/SpatialDilatedMaxPooling.cu
index 9d026b872..ed71cedb3 100644
--- a/src/THCUNN/generic/SpatialDilatedMaxPooling.cu
+++ b/src/THCUNN/generic/SpatialDilatedMaxPooling.cu
@@ -147,7 +147,7 @@ void THNN_(SpatialDilatedMaxPooling_updateOutput)(
 
   MaxPoolForward<real, accreal> <<< GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state) >>>
       (count, input_data,
-      batchSize, nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
+      static_cast<int>(batchSize), static_cast<int>(nInputPlane), static_cast<int>(nInputRows), static_cast<int>(nInputCols), static_cast<int>(nOutputRows), static_cast<int>(nOutputCols),
       kH, kW, dH, dW, padH, padW, dilationH, dilationW, output_data, indices_data);
   THCudaCheck(cudaGetLastError());
 
@@ -231,7 +231,7 @@ void THNN_(SpatialDilatedMaxPooling_updateGradInput)(
       (count,
       THCTensor_(data)(state, gradOutput),
       THCIndexTensor_(data)(state, indices),
-      batchSize, nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
+      static_cast<int>(batchSize), static_cast<int>(nInputPlane), static_cast<int>(nInputRows), static_cast<int>(nInputCols), static_cast<int>(nOutputRows), static_cast<int>(nOutputCols),
       kH, kW, dH, dW, padH, padW, dilationH, dilationW,
       THCTensor_(data)(state, gradInput));
   THCudaCheck(cudaGetLastError());
diff --git a/src/THCUNN/generic/SpatialFractionalMaxPooling.cu b/src/THCUNN/generic/SpatialFractionalMaxPooling.cu
index fd8835b27..ed6c421f3 100644
--- a/src/THCUNN/generic/SpatialFractionalMaxPooling.cu
+++ b/src/THCUNN/generic/SpatialFractionalMaxPooling.cu
@@ -72,10 +72,18 @@ void THNN_(SpatialFractionalMaxPooling_updateOutput)(
             devInput.getSize(0));
   dim3 block(outputPlaneSize > 128 ? 128 : outputPlaneSize);
 
-#define SFMP_UPDATE_OUTPUT(POOL_W)                                      \
-  SpatialFractionalMaxPooling_updateOutput<POOL_W, real, accreal>       \
-    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \
-      devInput, devOutput, devIndices, devSamples, poolSizeW, poolSizeH);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define SFMP_UPDATE_OUTPUT(POOL_W)                                      \
+    hipLaunchKernelGGL(                                                   \
+    (SpatialFractionalMaxPooling_updateOutput<POOL_W, real, accreal>),    \
+        grid, block, 0, THCState_getCurrentStream(state),                 \
+        devInput, devOutput, devIndices, devSamples, poolSizeW, poolSizeH);
+#else
+  #define SFMP_UPDATE_OUTPUT(POOL_W)                                      \
+    SpatialFractionalMaxPooling_updateOutput<POOL_W, real, accreal>       \
+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \
+        devInput, devOutput, devIndices, devSamples, poolSizeW, poolSizeH);
+#endif
 
 #define SFMP_UPDATE_OUTPUT_CASE(POOL_W)                 \
   case POOL_W: SFMP_UPDATE_OUTPUT(POOL_W); break
diff --git a/src/THCUNN/generic/SpatialFullConvolution.cu b/src/THCUNN/generic/SpatialFullConvolution.cu
index 6f9fa9844..af9a4736f 100644
--- a/src/THCUNN/generic/SpatialFullConvolution.cu
+++ b/src/THCUNN/generic/SpatialFullConvolution.cu
@@ -58,4 +58,4 @@ void THNN_(SpatialFullConvolution_accGradParameters)(
       kW, kH, dW, dH, padW, padH, 1, 1, adjW, adjH, scale_);
 }
 
-#endif
\ No newline at end of file
+#endif
diff --git a/src/THCUNN/generic/SpatialMaxUnpooling.cu b/src/THCUNN/generic/SpatialMaxUnpooling.cu
index cb08b0a9f..143a83ff0 100644
--- a/src/THCUNN/generic/SpatialMaxUnpooling.cu
+++ b/src/THCUNN/generic/SpatialMaxUnpooling.cu
@@ -38,8 +38,8 @@ void THNN_(SpatialMaxUnpooling_updateOutput)(
   int count = THCTensor_(nElement)(state, input);
 
   MaxUnpoolForward <<< GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state) >>>
-      (count, THCTensor_(data)(state, input), THCIndexTensor_(data)(state, indices),
-      batchSize, nInputPlane, nInputRows, nInputCols, oheight, owidth, THCTensor_(data)(state, output));
+      (count, static_cast<const real*>(THCTensor_(data)(state, input)), static_cast<const long*>(THCIndexTensor_(data)(state, indices)),
+      static_cast<int>(batchSize), static_cast<int>(nInputPlane), static_cast<int>(nInputRows), static_cast<int>(nInputCols), static_cast<int>(oheight), static_cast<int>(owidth), THCTensor_(data)(state, output));
   THCudaCheck(cudaGetLastError());
 
   if(input->nDimension == 3)
@@ -91,8 +91,8 @@ void THNN_(SpatialMaxUnpooling_updateGradInput)(
   int count = THCTensor_(nElement)(state, input);
 
   MaxUnpoolBackward <<< GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state) >>>
-      (count, THCTensor_(data)(state, gradOutput), THCIndexTensor_(data)(state, indices),
-      batchSize, nInputPlane, nInputRows, nInputCols, oheight, owidth, THCTensor_(data)(state, gradInput));
+      (count, static_cast<const real*>(THCTensor_(data)(state, gradOutput)), static_cast<const long*>(THCIndexTensor_(data)(state, indices)),
+      static_cast<int>(batchSize), static_cast<int>(nInputPlane), static_cast<int>(nInputRows), static_cast<int>(nInputCols), static_cast<int>(oheight), static_cast<int>(owidth), THCTensor_(data)(state, gradInput));
   THCudaCheck(cudaGetLastError());
 
   // clean
diff --git a/src/THCUNN/generic/SpatialSubSampling.cu b/src/THCUNN/generic/SpatialSubSampling.cu
index a95d0be85..5af7099cf 100644
--- a/src/THCUNN/generic/SpatialSubSampling.cu
+++ b/src/THCUNN/generic/SpatialSubSampling.cu
@@ -51,10 +51,10 @@ void THNN_(SpatialSubSampling_updateOutput)(
   THNN_(SpatialSubSampling_shapeCheck)(state, input, NULL, weight, kW, kH);
 
   if (input->nDimension == 3) {
-    int64_t nInputCols = input->size[2];
-    int64_t nInputRows = input->size[1];
-    int64_t nOutputCols = (nInputCols - kW) / dW + 1;
-    int64_t nOutputRows = (nInputRows - kH) / dH + 1;
+    int32_t nInputCols = input->size[2];
+    int32_t nInputRows = input->size[1];
+    int32_t nOutputCols = (nInputCols - kW) / dW + 1;
+    int32_t nOutputRows = (nInputRows - kH) / dH + 1;
 
     input = THCTensor_(newContiguous)(state, input);
     input_data = THCTensor_(data)(state, input);
@@ -119,8 +119,8 @@ void THNN_(SpatialSubSampling_updateGradInput)(
   int nInputPlane = THCTensor_(size)(state, weight, 0);
 
   if (input->nDimension == 3) {
-    int64_t nInputCols = input->size[2];
-    int64_t nInputRows = input->size[1];
+    int32_t nInputCols = input->size[2];
+    int32_t nInputRows = input->size[1];
 
     real *weight_data = THCTensor_(data)(state, weight);
     gradOutput = THCTensor_(newContiguous)(state, gradOutput);
@@ -149,9 +149,9 @@ void THNN_(SpatialSubSampling_updateGradInput)(
     }
     THCudaCheck(cudaGetLastError());
   } else {
-    int64_t nInputCols = input->size[3];
-    int64_t nInputRows = input->size[2];
-    int64_t nbatch = input->size[0];
+    int32_t nInputCols = input->size[3];
+    int32_t nInputRows = input->size[2];
+    int32_t nbatch = input->size[0];
 
     real *weight_data = THCTensor_(data)(state, weight);
     gradOutput = THCTensor_(newContiguous)(state, gradOutput);
@@ -199,8 +199,8 @@ void THNN_(SpatialSubSampling_accGradParameters)(
   int nInputPlane = THCTensor_(size)(state, gradWeight, 0);
 
   if (input->nDimension == 3) {
-    int64_t nInputCols = input->size[2];
-    int64_t nInputRows = input->size[1];
+    int32_t nInputCols = input->size[2];
+    int32_t nInputRows = input->size[1];
 
     real *gradWeight_data = THCTensor_(data)(state, gradWeight);
     real *gradBias_data = THCTensor_(data)(state, gradBias);
@@ -221,9 +221,9 @@ void THNN_(SpatialSubSampling_accGradParameters)(
       nInputPlane, nInputRows, nInputCols, kH, kW, dH, dW, scale);
     THCudaCheck(cudaGetLastError());
   } else {
-    int64_t nInputCols = input->size[3];
-    int64_t nInputRows = input->size[2];
-    int64_t nbatch = input->size[0];
+    int32_t nInputCols = input->size[3];
+    int32_t nInputRows = input->size[2];
+    int32_t nbatch = input->size[0];
 
     real *gradWeight_data = THCTensor_(data)(state, gradWeight);
     real *gradBias_data = THCTensor_(data)(state, gradBias);
diff --git a/src/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu b/src/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu
index 218100b12..3ddf92441 100644
--- a/src/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu
+++ b/src/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu
@@ -23,7 +23,7 @@ void THNN_(VolumetricAdaptiveAveragePooling_updateOutput)(
   real *output_data;
   real *input_data;
 
-  int64_t sizeD, isizeT, isizeH, isizeW;
+  int32_t sizeD, isizeT, isizeH, isizeW;
   int64_t istrideD, istrideT, istrideH, istrideW;
   int64_t totalZ;
 
@@ -102,8 +102,8 @@ void THNN_(VolumetricAdaptiveAveragePooling_updateGradInput)(
   real *gradInput_data;
   real *gradOutput_data;
 
-  int64_t sizeD, isizeT, isizeH, isizeW;
-  int64_t osizeT, osizeH, osizeW;
+  int32_t sizeD, isizeT, isizeH, isizeW;
+  int32_t osizeT, osizeH, osizeW;
   int64_t totalZ;
 
   if (input->nDimension == 4) {
diff --git a/src/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu b/src/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu
index b4253d42b..35f71a215 100644
--- a/src/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu
+++ b/src/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu
@@ -75,8 +75,10 @@ void THNN_(VolumetricAdaptiveMaxPooling_updateOutput)(
     dim3 blocks(totalZ > 65535 ? 65535 : totalZ, blocksH);
     cunn_VolumetricAdaptiveMaxPooling_updateOutput_kernel
       <<<blocks, threads, 0, THCState_getCurrentStream(state)>>>(
-        input_data, output_data, indices_data, isizeT, isizeH, isizeW,
-        osizeT, osizeH, osizeW, istrideD, istrideT, istrideH, istrideW, offsetZ
+        input_data, output_data, indices_data, 
+        static_cast<int>(isizeT), static_cast<int>(isizeH), static_cast<int>(isizeW),
+        static_cast<int>(osizeT), static_cast<int>(osizeH), static_cast<int>(osizeW), 
+        istrideD, istrideT, istrideH, istrideW, offsetZ
       );
 
     totalZ -= 65535;
@@ -157,13 +159,17 @@ void THNN_(VolumetricAdaptiveMaxPooling_updateGradInput)(
       cunn_atomic_VolumetricAdaptiveMaxPooling_updateGradInput_kernel
         <<<blocks, threads, 0, THCState_getCurrentStream(state)>>>(
           gradInput_data, gradOutput_data, indices_data,
-          isizeT, isizeH, isizeW, osizeT, osizeH, osizeW, offsetZ
+          static_cast<int>(isizeT), static_cast<int>(isizeH), static_cast<int>(isizeW), 
+          static_cast<int>(osizeT), static_cast<int>(osizeH), static_cast<int>(osizeW), 
+          offsetZ
         );
     } else {
       cunn_VolumetricAdaptiveMaxPooling_updateGradInput_kernel
         <<<blocks, threads, 0, THCState_getCurrentStream(state)>>>(
           gradInput_data, gradOutput_data, indices_data,
-          isizeT, isizeH, isizeW, osizeT, osizeH, osizeW, offsetZ
+          static_cast<int>(isizeT), static_cast<int>(isizeH), static_cast<int>(isizeW), 
+          static_cast<int>(osizeT), static_cast<int>(osizeH), static_cast<int>(osizeW), 
+          offsetZ
         );
     }
 
diff --git a/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu b/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu
index 3bc98dbe1..7bf87ab8b 100644
--- a/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu
+++ b/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu
@@ -2,6 +2,16 @@
 #define THC_GENERIC_FILE "generic/VolumetricDilatedMaxPooling.cu"
 #else
 
+#if defined(__HIP_PLATFORM_HCC__)
+#define UPDATE_OUTPUT_KERNEL_WIDTH(KW) case KW:                         \
+  hipLaunchKernelGGL(                                                   \
+  (cuda_VolumetricDilatedMaxPooling_updateOutput<KW>), grid, block,     \
+    0, THCState_getCurrentStream(state),                             \
+    inputData, inputTime, inputHeight, inputWidth, \
+    cudaIndices, cudaOutput, kT, kH, dT, dH, dW, padT, padH, padW,\
+    dilationT, dilationH, dilationW, offsetZ); \
+    break
+#else
 #define UPDATE_OUTPUT_KERNEL_WIDTH(KW) case KW:                         \
   cuda_VolumetricDilatedMaxPooling_updateOutput<KW><<<grid, block,             \
     0, THCState_getCurrentStream(state)>>>(                             \
@@ -9,6 +19,7 @@
     cudaIndices, cudaOutput, kT, kH, dT, dH, dW, padT, padH, padW,\
     dilationT, dilationH, dilationW, offsetZ); \
     break
+#endif
 
 static inline void THNN_(VolumetricDilatedMaxPooling_shapeCheck)(
                          THCState *state,
@@ -166,7 +177,7 @@ void THNN_(VolumetricDilatedMaxPooling_updateOutput)(
     inputHeight = THCTensor_(size)(state, input, 2);
     inputWidth  = THCTensor_(size)(state, input, 3);
   }
-  else if (fiveDimensionalInput)
+  else if(fiveDimensionalInput)
   {
     /* sizes */
     batchSize   = THCTensor_(size)(state, input, 0);
@@ -175,10 +186,6 @@ void THNN_(VolumetricDilatedMaxPooling_updateOutput)(
     inputHeight = THCTensor_(size)(state, input, 3);
     inputWidth  = THCTensor_(size)(state, input, 4);
   }
-  else
-  {
-    THArgError(2, "4D or 5D tensor expected, got %d", THCTensor_(nDimension)(state, input));
-  }
 
   if (ceilMode)
   {
@@ -225,21 +232,22 @@ void THNN_(VolumetricDilatedMaxPooling_updateOutput)(
 
   input = THCTensor_(newContiguous)(state, input);
   if (fiveDimensionalInput) {
-    // Collapse batch and feature dimensions
-    output = THCTensor_(newFoldBatchDim)(state, output);
+      // Collapse batch and feature dimensions
+      output = THCTensor_(newFoldBatchDim)(state, output);
 
-    THCTensor *old_input = input;
-    input = THCTensor_(newFoldBatchDim)(state, input);
-    THCTensor_(free)(state, old_input);
+      THCTensor *old_input = input;
+      input = THCTensor_(newFoldBatchDim)(state, input);
+      THCTensor_(free)(state, old_input);
   } else {
-    THCTensor_(retain)(state, output);
+      THCTensor_(retain)(state, output);
   }
-  
+
   real* inputData = THCTensor_(data)(state, input);
 
   THCDeviceTensor<real, 4> cudaOutput;
   cudaOutput = toDeviceTensor<real, 4>(state, output);
 
+
   THLongStorage *indicesSize = THLongStorage_newWithSize(4);
   int64_t indicesSizeRaw[4] = { batchSize * inputSlices,
                             outputTime, outputHeight, outputWidth };
diff --git a/src/THCUNN/generic/VolumetricFractionalMaxPooling.cu b/src/THCUNN/generic/VolumetricFractionalMaxPooling.cu
index a3619f609..610fa348f 100644
--- a/src/THCUNN/generic/VolumetricFractionalMaxPooling.cu
+++ b/src/THCUNN/generic/VolumetricFractionalMaxPooling.cu
@@ -78,10 +78,18 @@ void THNN_(VolumetricFractionalMaxPooling_updateOutput)(
             devInput.getSize(0));
   dim3 block(outputPlaneSize > 128 ? 128 : outputPlaneSize);
 
-#define SFMP_UPDATE_OUTPUT(POOL_W)                                      \
-  VolumetricFractionalMaxPooling_updateOutput<POOL_W, real, accreal>       \
-    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \
-      devInput, devOutput, devIndices, devSamples, poolSizeT, poolSizeW, poolSizeH);
+#if defined(__HIP_PLATFORM_HCC__)
+  #define SFMP_UPDATE_OUTPUT(POOL_W)                                      \
+    hipLaunchKernelGGL(                                                   \
+    (VolumetricFractionalMaxPooling_updateOutput<POOL_W, real, accreal>), \
+        grid, block, 0, THCState_getCurrentStream(state),                 \
+        devInput, devOutput, devIndices, devSamples, poolSizeT, poolSizeW, poolSizeH);
+#else
+  #define SFMP_UPDATE_OUTPUT(POOL_W)                                      \
+    VolumetricFractionalMaxPooling_updateOutput<POOL_W, real, accreal>       \
+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \
+        devInput, devOutput, devIndices, devSamples, poolSizeT, poolSizeW, poolSizeH);
+#endif
 
 #define SFMP_UPDATE_OUTPUT_CASE(POOL_W)                 \
   case POOL_W: SFMP_UPDATE_OUTPUT(POOL_W); break
diff --git a/src/THCUNN/im2col.h b/src/THCUNN/im2col.h
index 168bace30..64d524e2f 100644
--- a/src/THCUNN/im2col.h
+++ b/src/THCUNN/im2col.h
@@ -48,6 +48,15 @@ void im2col(cudaStream_t stream, const Dtype* data_im, const int channels,
   // kernel responsible for copying a single-channel grid.
   int num_kernels = channels * height_col * width_col;
   // Launch
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL((im2col_kernel), dim3(GET_BLOCKS(num_kernels)), dim3(CUDA_NUM_THREADS), 0, stream,
+      num_kernels, data_im, height, width, ksize_h, ksize_w,
+      pad_h, pad_w, stride_h, stride_w,
+      dilation_h, dilation_w,
+      height_col, width_col, data_col
+  );
+  THCudaCheck(hipGetLastError());
+#else
   im2col_kernel <<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS, 0, stream>>> (
       num_kernels, data_im, height, width, ksize_h, ksize_w,
       pad_h, pad_w, stride_h, stride_w,
@@ -55,6 +64,7 @@ void im2col(cudaStream_t stream, const Dtype* data_im, const int channels,
       height_col, width_col, data_col
   );
   THCudaCheck(cudaGetLastError());
+#endif
 }
 
 template <typename Dtype, typename Acctype>
@@ -116,6 +126,15 @@ void col2im(cudaStream_t stream, const Dtype* data_col, const int channels,
   int num_kernels = channels * height * width;
   // To avoid involving atomic operations, we will launch one kernel per
   // bottom dimension, and then in the kernel add up the top dimensions.
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL((col2im_kernel<Dtype, Acctype>), dim3(GET_BLOCKS(num_kernels)), dim3(CUDA_NUM_THREADS), 0, stream,
+      num_kernels, data_col, height, width, channels,
+      patch_h, patch_w, pad_h, pad_w, stride_h, stride_w,
+      dilation_h, dilation_w,
+      output_height, output_width, data_im
+  );
+  THCudaCheck(hipGetLastError());
+#else
   col2im_kernel<Dtype, Acctype> <<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS, 0, stream>>> (
       num_kernels, data_col, height, width, channels,
       patch_h, patch_w, pad_h, pad_w, stride_h, stride_w,
@@ -123,6 +142,7 @@ void col2im(cudaStream_t stream, const Dtype* data_col, const int channels,
       output_height, output_width, data_im
   );
   THCudaCheck(cudaGetLastError());
+#endif
 }
 
 #endif
diff --git a/src/THCUNN/row2col.h b/src/THCUNN/row2col.h
index 04765dd01..1a287c993 100644
--- a/src/THCUNN/row2col.h
+++ b/src/THCUNN/row2col.h
@@ -37,10 +37,17 @@ void row2col(cudaStream_t stream, const Dtype *data_row, const int channels,
       (width + 2 * pad_w - (dilation_w * (ksize_w - 1) + 1)) / stride_w + 1;
   int num_kernels = channels * width_col;
   // Launch
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL((row2col_kernel), dim3(GET_BLOCKS(num_kernels)), dim3(CUDA_NUM_THREADS), 0, stream,
+      num_kernels, data_row, width, ksize_w, pad_w, stride_w, 1, width_col,
+      data_col);
+  THCudaCheck(hipGetLastError());
+#else
   row2col_kernel<<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS, 0, stream>>>(
       num_kernels, data_row, width, ksize_w, pad_w, stride_w, 1, width_col,
       data_col);
   THCudaCheck(cudaGetLastError());
+#endif
 }
 
 template <typename Dtype, typename Acctype>
@@ -80,11 +87,19 @@ void col2row(cudaStream_t stream, const Dtype *data_col, const int channels,
   int num_kernels = channels * width;
   // To avoid involving atomic operations, we will launch one kernel per
   // bottom dimension, and then in the kernel add up the top dimensions.
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL((col2row_kernel<
+      Dtype, Acctype>), GET_BLOCKS(num_kernels), CUDA_NUM_THREADS, 0, stream,
+      num_kernels, data_col, width, channels, patch_w, pad_w, stride_w,
+      dilation_w, width_col, data_row);
+
+  THCudaCheck(hipGetLastError());
+#else
   col2row_kernel<
       Dtype, Acctype><<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS, 0, stream>>>(
       num_kernels, data_col, width, channels, patch_w, pad_w, stride_w,
       dilation_w, width_col, data_row);
-
   THCudaCheck(cudaGetLastError());
+#endif
 }
 #endif
diff --git a/src/THCUNN/vol2col.h b/src/THCUNN/vol2col.h
index 223248f50..083f93137 100644
--- a/src/THCUNN/vol2col.h
+++ b/src/THCUNN/vol2col.h
@@ -55,6 +55,15 @@ void vol2col(cudaStream_t stream, const Dtype* data_vol, const int channels,
   // kernel responsible for copying a single-channel grid.
   int num_kernels = channels * depth_col * height_col * width_col;
   // Launch
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL((vol2col_kernel), dim3(GET_BLOCKS(num_kernels)), dim3(CUDA_NUM_THREADS), 0, stream,
+      num_kernels, data_vol, depth, height, width, ksize_t, ksize_h, ksize_w,
+      pad_t, pad_h, pad_w, stride_t, stride_h, stride_w,
+      dilation_t, dilation_h, dilation_w,
+      depth_col, height_col, width_col, data_col
+  );
+  THCudaCheck(hipGetLastError());
+#else
   vol2col_kernel <<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS, 0, stream>>> (
       num_kernels, data_vol, depth, height, width, ksize_t, ksize_h, ksize_w,
       pad_t, pad_h, pad_w, stride_t, stride_h, stride_w,
@@ -62,6 +71,7 @@ void vol2col(cudaStream_t stream, const Dtype* data_vol, const int channels,
       depth_col, height_col, width_col, data_col
   );
   THCudaCheck(cudaGetLastError());
+#endif
 }
 
 template <typename Dtype, typename Acctype>
@@ -127,6 +137,15 @@ void col2vol(cudaStream_t stream, const Dtype* data_col, const int channels,
   int num_kernels = channels * depth * height * width;
   // To avoid involving atomic operations, we will launch one kernel per
   // bottom dimension, and then in the kernel add up the top dimensions.
+#if defined(__HIP_PLATFORM_HCC__)
+  hipLaunchKernelGGL((vol2im_kernel<Dtype, Acctype>), dim3(GET_BLOCKS(num_kernels)), dim3(CUDA_NUM_THREADS), 0, stream,
+      num_kernels, data_col, depth, height, width, channels,
+      patch_t, patch_h, patch_w, pad_t, pad_h, pad_w, stride_t, stride_h, stride_w,
+      dilation_t, dilation_h, dilation_w,
+      output_depth, output_height, output_width, data_vol
+  );
+  THCudaCheck(hipGetLastError());
+#else
   vol2im_kernel<Dtype, Acctype> <<<GET_BLOCKS(num_kernels), CUDA_NUM_THREADS, 0, stream>>> (
       num_kernels, data_col, depth, height, width, channels,
       patch_t, patch_h, patch_w, pad_t, pad_h, pad_w, stride_t, stride_h, stride_w,
@@ -134,6 +153,7 @@ void col2vol(cudaStream_t stream, const Dtype* data_col, const int channels,
       output_depth, output_height, output_width, data_vol
   );
   THCudaCheck(cudaGetLastError());
+#endif
 }
 
 #endif
